{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kjNxUX51Lh0k",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "63ef10fc-5dbf-4604-a9a5-da3e4a65eb8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "#@title **Imports**\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from itertools import product as cartesian_prod\n",
        "#save numpy array as npz file\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "# load numpy array from npz file\n",
        "from numpy import load\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDx4xoSOFzR2"
      },
      "source": [
        "**Variable parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KLhoBZzFxNb"
      },
      "outputs": [],
      "source": [
        "num_train_data=3000\n",
        "num_vali_data=3000\n",
        "num_test_data=3000\n",
        "dim=2\n",
        "num_modes=2*dim\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "model_type = \"Regression\" #\"Classification\" #for four mode classification data use Classification and for square decision tree data use Regression\n",
        "seed=0\n",
        "num_hidden_layers=3\n",
        "input_dim=dim\n",
        "output_dim=1\n",
        "num_hidden_nodes=[5,5,5]\n",
        "beta=[100,10000] #When freeze is true then the layer corr to unfreeze node beta value is 20 rest 10k, for freeze false every layer beta is 20.\n",
        "no_of_batches=1 #[1,10,100]\n",
        "modep='pwc' #piece wise constant means all 1's will be given as input to NPV.\n",
        "num_epoch=5000\n",
        "lr=0.0001\n",
        "weight_decay=0.0\n",
        "\n",
        "\n",
        "\n",
        "test_on_train_data = False #Default False, set True when generate node hyperplanes on training data\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "freeze = False #True - when freeze weights of all nodes except one. False- when all nodes weights are varying\n",
        "layer_num = 3 #Layer number whose node is only varying and rest are freezed\n",
        "node_num = 3 #Node number of the above layer which is only varying rest are freeze\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "NPF_freeze = False #Default False, set True - when freeze the whole NPF\n",
        "NPV_freeze = False #Default False, set True - when freeze the whole NPV\n",
        "NPF_pretrained_freezed = True #Default False, set True - when freeze the whole NPF after training it to max and NPV is trained from reinit\n",
        "NPV_pretrained_freezed = False #Default False, set True - when freeze the whole NPV after training it to max and NPF is trained from reinit\n",
        "\n",
        "\n",
        "train_model = False #Set to true if training a model and False if only infer a trained model\n",
        "infer_model = True #Set to true if infer a pretrained model or infer after training\n",
        "\n",
        "\n",
        "\n",
        "print_all_paths = False #Set to true if wants to print all the path values of init,mid,last epoch\n",
        "show_all_epoch_node_hyperplane = False #Set to true if wants to show and store all node hyperplanes of all epochs\n",
        "best_epoch_show = False #Set to true if wants to show the node hyperplanes of the best epoch\n",
        "save_img = False\n",
        "\n",
        "file_path = \"/content/drive/MyDrive/Research/Saved_models/\"\n",
        "# file_name_load = file_path+\"R_NPF_pretrained_Fixed_1L_3L5N_Beta_20_seed_365.npz\" #change the model name to infer a pretrained model\n",
        "# file_name_load = file_path+\"R_NPF_Fixed_1L_3L5N_Beta_100_seed_0.npz\" #change the model name to infer a pretrained model\n",
        "file_name_load = file_path+\"R_NPF_pretrained_Fixed_1L_3L5N_Beta_100_seed_0.npz\" #change the model name to infer a pretrained model\n",
        "\n",
        "# file_name_load_lr = file_path +\"Path_LR_model_NPF_pretrained.npz\"\n",
        "# file_name_load_lr = file_path +\"Path_LR_model_NPF_Fixed_Beta_100_sigmoid.npz\"\n",
        "file_name_load_lr = file_path +\"Path_LR_model_NPF_pretrained_Beta_100_weinit_0_sigmoid.npz\"\n",
        "\n",
        "enter_path = (0,1,2) #enter the path whose path_value is to be determined in list form as (1st_layer_node_num,2nd_layer_node_num,3rd_layer_node_num)\n",
        "enter_epoch = 0 #enter the epoch no (perfect sq number) whose node hyperplane is to be visualised\n",
        "plot_epoch = 5000 #epoch till which want to plot\n",
        "lr_path = 0.01\n",
        "\n",
        "\n",
        "Path_LR_hypPlane = False #True-- Calculate Path_LR using hyper plane method, False--> calculating using sigmoid\n",
        "train_lr = False\n",
        "infer_path_lr = False\n",
        "\n",
        "train_dlgn_lr = True\n",
        "infer_dlgn_lr = True\n",
        "\n",
        "max_no_of_nodes=max(num_hidden_nodes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iXNxcu4L6kT"
      },
      "outputs": [],
      "source": [
        "#@title **Synthetic data**\n",
        "def set_npseed(seed):\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "def set_torchseed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  \n",
        "#Four mode classification data\n",
        "def data_gen1(num_train_data,num_vali_data,num_test_data,dim=2,seed=0):\n",
        "  # np.random.seed(6790)\n",
        "  set_npseed(seed = seed)\n",
        "  d=dim\n",
        "  num_modes=2*d\n",
        "  centers = np.concatenate((np.eye(d), -1*np.eye(d)), axis=0)\n",
        "  np.random.shuffle(centers)\n",
        "  # np.random.seed(475)\n",
        "  hard=num_modes//2\n",
        "\n",
        "  a=[4]* hard + [1]*(num_modes - hard)\n",
        "  mode_frac = np.array([1./num_modes]*num_modes)\n",
        "  b=1.\n",
        "  num_train_data = num_train_data \n",
        "  num_vali_data=num_vali_data\n",
        "  num_test_data=num_test_data\n",
        "  num_data = num_train_data + num_vali_data + num_test_data\n",
        "\n",
        "  num_data_per_mode = np.int32(num_data*mode_frac)\n",
        "  num_data_per_mode = np.concatenate((num_data_per_mode,[np.sum(num_data_per_mode)]))\n",
        "\n",
        "  landmarks=[-1]*num_modes\n",
        "  labels=[-1]*num_modes\n",
        "  for i in range(num_modes):\n",
        "    landmarks[i] = 0.05*np.random.randn(a[i],d)\n",
        "    landmarks[i]+=centers[i]\n",
        "    labels[i] = (i - np.arange(len(landmarks[i])))%2\n",
        "      \n",
        "  data=[-1]*num_modes\n",
        "  data_labels=[-1]*num_modes\n",
        "\n",
        "  train_data=[-1]*num_modes\n",
        "  train_data_labels=[-1]*num_modes\n",
        "\n",
        "  test_data=[-1]*num_modes\n",
        "  test_data_labels=[-1]*num_modes\n",
        "\n",
        "  vali_data=[-1]*num_modes\n",
        "  vali_data_labels=[-1]*num_modes\n",
        "\n",
        "\n",
        "  modes_data=[]\n",
        "  # np.random.seed(12345)\n",
        "\n",
        "  for i in range(num_modes):\n",
        "    data[i] = 0.1*np.random.randn(num_data_per_mode[i],d)\n",
        "    data[i] += centers[i]\n",
        "    data_labels[i] = np.zeros(num_data_per_mode[i])\n",
        "    for j in range(len(data_labels[i])):\n",
        "      dists = pairwise_distances(data[i][j:j+1,:],landmarks[i])                                   \n",
        "      j_star = np.argmin(dists[0])\n",
        "      data_labels[i][j]=labels[i][j_star]\n",
        "      \n",
        "      train_data[i] = np.array(data[i][:int(mode_frac[i]*num_train_data)])\n",
        "      train_data_labels[i] = np.array(data_labels[i][:int(mode_frac[i]*num_train_data)])\n",
        "\n",
        "      vali_data[i] = np.array(data[i][int(mode_frac[i]*num_train_data): \\\n",
        "                                      int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "      vali_data_labels[i] = np.array(data_labels[i][int(mode_frac[i]*num_train_data): \\\n",
        "                                      int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "\n",
        "      test_data[i] = np.array(data[i][-int(mode_frac[i]*num_test_data):])\n",
        "      test_data_labels[i] = np.array(data_labels[i][-int(mode_frac[i]*num_test_data):])\n",
        "      \n",
        "  return data,data_labels,train_data,train_data_labels,test_data,test_data_labels,vali_data,vali_data_labels,landmarks,labels,centers,mode_frac\n",
        "\n",
        "def uniform_data_gen(num_data,dim=2):\n",
        "  n=num_data\n",
        "  d=dim\n",
        "  x0x1_min = [-1.0, -1.0]\n",
        "  x0x1_max = [1.0, 1.0]\n",
        "  data = np.random.uniform(low=x0x1_min, high=x0x1_max, size=(n,d))\n",
        "  y=np.zeros(n,dtype=np.float32)\n",
        "  for i,val in enumerate(data):\n",
        "    if(-1<=val[0]<=0 and 0<=val[1]<=1):\n",
        "      y[i]=1\n",
        "    elif (0<=val[0]<=1 and 0.5<=val[1]<=1):\n",
        "      y[i]=0.2\n",
        "    elif(0<=val[0]<=1 and -0.5<=val[1]<=0.5):\n",
        "      y[i]=0.5\n",
        "    else:\n",
        "      y[i]=-1\n",
        "  return data,y\n",
        "\n",
        "#Square decision tree data\n",
        "def data_gen2(num_train_data,num_vali_data,num_test_data,dim=2,seed=0):\n",
        "  set_npseed(seed = seed)\n",
        "  train_data,train_data_labels=uniform_data_gen(num_train_data,dim=dim)\n",
        "  vali_data,vali_data_labels=uniform_data_gen(num_vali_data,dim=dim)\n",
        "  test_data,test_data_labels=uniform_data_gen(num_test_data,dim=dim)\n",
        "  data = np.concatenate((train_data,vali_data),axis=0)\n",
        "  data = np.concatenate((data,test_data),axis=0)\n",
        "  data_labels = np.concatenate((train_data_labels,vali_data_labels),axis=0)\n",
        "  data_labels = np.concatenate((data_labels,test_data_labels),axis=0)\n",
        "  return data,data_labels,train_data,train_data_labels,test_data,test_data_labels,vali_data,vali_data_labels,None,None,None,None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtzRiptOL_Ug"
      },
      "outputs": [],
      "source": [
        "#@title **DLGN_FC**\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "class DLGN_FC(nn.Module):\n",
        "  def __init__(self, to_copy=None,NPF_pretrained=True,NPV_pretrained=True, num_hidden_layers=0, input_dim=2, output_dim=1, num_hidden_nodes=[], beta=20, mode='pwc'):\n",
        "    super(DLGN_FC, self).__init__()\n",
        "    if to_copy==None:\n",
        "      self.gating_layers=[]\n",
        "      self.value_layers=[]\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "      self.beta=beta  # Soft gating parameter\n",
        "      self.mode = mode\n",
        "      self.num_nodes=[input_dim]+num_hidden_nodes+[output_dim]\n",
        "      for i in range(num_hidden_layers+1):\n",
        "        self.gating_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1]))\n",
        "        self.value_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=False))\n",
        "    else:\n",
        "      self.gating_layers=[]\n",
        "      self.value_layers=[]\n",
        "      self.num_hidden_layers = to_copy.num_hidden_layers\n",
        "      self.beta=to_copy.beta  # Soft gating parameter\n",
        "      self.mode = to_copy.mode\n",
        "      self.num_nodes=list(to_copy.num_nodes)\n",
        "      for i in range(self.num_hidden_layers+1):\n",
        "        self.gating_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1]))\n",
        "        self.value_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=False))\n",
        "        if NPF_pretrained:\n",
        "          self.gating_layers[i].weight.data =  torch.Tensor(np.array(to_copy.gating_layers[i].weight.detach().numpy()))\n",
        "          self.gating_layers[i].bias.data = torch.Tensor(np.array(to_copy.gating_layers[i].bias.detach().numpy()))\n",
        "        if NPV_pretrained:\n",
        "          self.value_layers[i].weight.data = torch.Tensor(np.array(to_copy.value_layers[i].weight.detach().numpy()))\n",
        "\n",
        "                \n",
        "\n",
        "  def return_gating_functions(self):\n",
        "    effective_weights = []\n",
        "    effective_biases =[]\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      curr_weight = self.gating_layers[i].weight\n",
        "      curr_bias = self.gating_layers[i].bias\n",
        "      if i==0:\n",
        "        effective_weights.append(curr_weight)\n",
        "        effective_biases.append(curr_bias)\n",
        "      else:\n",
        "        effective_biases.append(torch.matmul(curr_weight,effective_biases[-1])+curr_bias)\n",
        "        effective_weights.append(torch.matmul(curr_weight,effective_weights[-1]))\n",
        "    return effective_weights, effective_biases\n",
        "    # effective_weights (and effective biases) is a list of size num_hidden_layers\n",
        "              \n",
        "\n",
        "  def forward(self, x):\n",
        "    gate_scores=[x]\n",
        "    if self.mode=='pwc':\n",
        "      values=[torch.ones(x.shape)]\n",
        "    else:\n",
        "      values=[x]\n",
        "    \n",
        "    for i in range(self.num_hidden_layers):\n",
        "      beta_used = beta[0]\n",
        "      if ((i!=layer_num-1) and freeze):\n",
        "        beta_used = beta[1]\n",
        "      gate_scores.append(self.gating_layers[i](gate_scores[-1]))\n",
        "      curr_gate_on_off = torch.sigmoid(beta_used*gate_scores[-1])\n",
        "      values.append(self.value_layers[i](values[-1])*curr_gate_on_off)\n",
        "    values.append(self.value_layers[self.num_hidden_layers](values[-1]))\n",
        "    # Values is a list of size 1+num_hidden_layers+1\n",
        "    #gate_scores is a list of size 1+num_hidden_layers\n",
        "    return values,gate_scores"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DLGN_LR_FC(nn.Module):\n",
        "  def __init__(self, to_copy=None,num_path_features=125, num_hidden_layers=0, input_dim=2, output_dim=1, num_hidden_nodes=[], beta=20):\n",
        "    super(DLGN_LR_FC, self).__init__()\n",
        "    if to_copy==None:\n",
        "      self.gating_layers=[]\n",
        "      self.path_layer=[]\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "      self.beta=beta  # Soft gating parameter\n",
        "      self.num_nodes=[input_dim]+num_hidden_nodes+[output_dim]\n",
        "      for i in range(num_hidden_layers+1):\n",
        "        self.gating_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1]))\n",
        "\n",
        "\n",
        "      self.path_layer.append(nn.Linear(num_path_features, output_dim, bias=False))\n",
        "      self.path_layer[0].weight.data.fill_(0.0)\n",
        "\n",
        "    else:\n",
        "\n",
        "      self.gating_layers=[]\n",
        "      self.num_hidden_layers = to_copy.num_hidden_layers\n",
        "      self.beta=to_copy.beta  # Soft gating parameter\n",
        "      self.num_nodes=list(to_copy.num_nodes)\n",
        "      for i in range(self.num_hidden_layers+1):\n",
        "        self.gating_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1]))\n",
        "        self.gating_layers[i].weight.data =  torch.Tensor(np.array(to_copy.gating_layers[i].weight.detach().numpy()))\n",
        "        self.gating_layers[i].bias.data = torch.Tensor(np.array(to_copy.gating_layers[i].bias.detach().numpy()))\n",
        "      self.path_layer=[]\n",
        "      self.path_layer.append(nn.Linear(num_path_features, output_dim, bias=False))\n",
        "      self.path_layer[0].weight.data = torch.Tensor(np.array(to_copy.path_layer[0].weight.detach().numpy()))\n",
        "                \n",
        "\n",
        "  def return_gating_functions(self):\n",
        "    effective_weights = []\n",
        "    effective_biases =[]\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      curr_weight = self.gating_layers[i].weight\n",
        "      curr_bias = self.gating_layers[i].bias\n",
        "      if i==0:\n",
        "        effective_weights.append(curr_weight)\n",
        "        effective_biases.append(curr_bias)\n",
        "      else:\n",
        "        effective_biases.append(torch.matmul(curr_weight,effective_biases[-1])+curr_bias)\n",
        "        effective_weights.append(torch.matmul(curr_weight,effective_weights[-1]))\n",
        "    return effective_weights, effective_biases\n",
        "    # effective_weights (and effective biases) is a list of size num_hidden_layers\n",
        "              \n",
        "  def forward(self, x):\n",
        "    gate_scores=[x]\n",
        "    values=[]\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      beta_used = beta[0]\n",
        "      gate_scores.append(self.gating_layers[i](gate_scores[-1]))\n",
        "      curr_gate_on_off = torch.sigmoid(beta_used*gate_scores[-1])\n",
        "      values.append(curr_gate_on_off)\n",
        "    values = torch.stack(values,dim=1)\n",
        "\n",
        "# Working\n",
        "    \n",
        "    # values1 = (values[:,0,:].unsqueeze(dim=1)*values[:,1,:].unsqueeze(dim=2)).view(values.shape[0],-1)\n",
        "\n",
        "    # # print(\"Flag 1\",values1.shape)\n",
        "\n",
        "    # values2 = (values1.unsqueeze(dim=1)*values[:,2,:].unsqueeze(dim=2)).view(values.shape[0],-1)\n",
        "\n",
        "\n",
        "    values1 = (values[:,0,:].unsqueeze(dim=2)*values[:,1,:].unsqueeze(dim=1)).view(values.shape[0],-1)\n",
        "\n",
        "    # print(\"Flag 1\",values1.shape)\n",
        "\n",
        "    values2 = (values1.unsqueeze(dim=2)*values[:,2,:].unsqueeze(dim=1)).view(values.shape[0],-1)\n",
        "\n",
        "    # values1 = []\n",
        "    # for i in range(len(values)):\n",
        "    #   values1.append(torch.outer(values[i,0,:],values[i,1,:]).ravel())\n",
        "    # values1 = torch.stack(values1,dim=0)\n",
        "    \n",
        "    # values2 = []\n",
        "    # for i in range(len(values1)):\n",
        "    #   values2.append(torch.outer(values1[i,:],values[i,2,:]).ravel())\n",
        "    # values2 = torch.stack(values2,dim=0)\n",
        "   \n",
        "\n",
        "    \n",
        "    # x1=values[0].data.numpy()\n",
        "    # y1=values[1].data.numpy()\n",
        "    # z1=values[2].data.numpy()\n",
        "    # y1=(data_curr@effective_weights[1].data.numpy().T)+effective_biases[1].data.numpy()\n",
        "\n",
        "    # xy=np.zeros((x1.shape[0],x1.shape[1]*y1.shape[1]))\n",
        "    # for i in range(x1.shape[0]):\n",
        "    #   xy[i]=np.multiply.outer(x1[i], y1[i]).ravel()\n",
        "\n",
        "\n",
        "    # xyz=np.zeros((x1.shape[0],xy.shape[1]*z1.shape[1]))\n",
        "    # for i in range(x1.shape[0]):\n",
        "    #   xyz[i]=np.multiply.outer(xy[i], z1[i]).ravel()\n",
        "\n",
        "    # xyz=torch.Tensor(xyz)\n",
        "    # print(values2.shape)\n",
        "\n",
        "    path_values=self.path_layer[0](values2)\n",
        "    return path_values,gate_scores"
      ],
      "metadata": {
        "id": "oeCcSZfqdJHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw-19NMnNfFv"
      },
      "outputs": [],
      "source": [
        "#@title **Path_LR**\n",
        "class Path_LR(nn.Module):\n",
        "  def __init__(self, to_copy=None,num_path_features=125, output_dim=1):\n",
        "    super(Path_LR, self).__init__()\n",
        "    if to_copy==None:\n",
        "      self.path_layer=[]\n",
        "      self.path_layer.append(nn.Linear(num_path_features, output_dim, bias=False))\n",
        "      self.path_layer[0].weight.data.fill_(0.0)\n",
        "    else:\n",
        "      self.path_layer=[]\n",
        "      self.path_layer.append(nn.Linear(num_path_features, output_dim, bias=False))\n",
        "      self.path_layer[0].weight.data = torch.Tensor(np.array(to_copy.path_layer[0].weight.detach().numpy()))\n",
        "\n",
        "  def forward(self, x):\n",
        "    path_values=self.path_layer[0](x)\n",
        "    return path_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGAfARdaUD3r"
      },
      "outputs": [],
      "source": [
        "#@title **Check perfect sq**\n",
        "def perfectSq(N) :\n",
        "\tsq_root = round(N**(1/2));\n",
        "\tif sq_root * sq_root == N :\n",
        "\t\treturn True;\n",
        "\telse :\n",
        "\t\treturn False;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncr5k6koMbD_"
      },
      "outputs": [],
      "source": [
        "#@title **Train DLGN model**\n",
        "if train_model:\n",
        "  def train_dlgn(seed,num_hidden_layers,input_dim,output_dim,num_hidden_nodes,beta,no_of_batches,\n",
        "                mode,layer_num,node_num,train_data_curr,vali_data_curr,test_data_curr,train_labels_curr,test_labels_curr,vali_labels_curr,num_epoch,freeze,DLGN_obj_pretrained=None,NPF_pretrained = True,NPV_pretrained = True):\n",
        "    # torch.random.manual_seed(1222)\n",
        "    # torch.random.manual_seed(2311)\n",
        "    layer_num=layer_num-1\n",
        "    node_num=node_num-1\n",
        "    set_torchseed(seed)\n",
        "    if freeze:\n",
        "      DLGN_obj_initial = None\n",
        "    DLGN_obj = None\n",
        "    DLGN_obj_return = None\n",
        "    if DLGN_obj_pretrained == None:\n",
        "      DLGN_obj = DLGN_FC(num_hidden_layers=num_hidden_layers, input_dim=input_dim, output_dim=output_dim, \n",
        "                        num_hidden_nodes=num_hidden_nodes, beta=beta, mode=mode)\n",
        "    else:\n",
        "      DLGN_obj = DLGN_FC(to_copy=DLGN_obj_pretrained,NPF_pretrained=NPF_pretrained,NPV_pretrained=NPV_pretrained)\n",
        "\n",
        "    if freeze:\n",
        "      DLGN_obj_initial = DLGN_FC(to_copy=DLGN_obj)\n",
        "    \n",
        "    DLGN_obj_return = DLGN_FC(to_copy=DLGN_obj)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "    if model_type == \"Regression\":\n",
        "      criterion = nn.MSELoss()\n",
        "    \n",
        "    \n",
        "\n",
        "    DLGN_params = []\n",
        "    DLGN_params += [item.weight for item in DLGN_obj.gating_layers]\n",
        "    DLGN_params += [item.bias for item in DLGN_obj.gating_layers]\n",
        "    DLGN_params += [item.weight for item in DLGN_obj.value_layers]\n",
        "  #     DLGN_params += [item.bias for item in DLGN_obj.value_layers]\n",
        "\n",
        "    # print(\"H0\")\n",
        "    # print(DLGN_params)\n",
        "\n",
        "    if NPF_freeze:\n",
        "      for index,param in enumerate(DLGN_params):\n",
        "        if(index<2*len(DLGN_obj.gating_layers)):\n",
        "          param.requires_grad=False\n",
        "    if NPV_freeze:\n",
        "      for index,param in enumerate(DLGN_params):\n",
        "        if(index>=2*len(DLGN_obj.gating_layers)):\n",
        "          param.requires_grad=False\n",
        "    if freeze:\n",
        "      for index,param in enumerate(DLGN_params):\n",
        "        if((index != layer_num)):\n",
        "          param.requires_grad=False\n",
        "        if((index==len(DLGN_obj.gating_layers)+layer_num)):\n",
        "          param.requires_grad=True\n",
        "\n",
        "    # print(\"H1\")\n",
        "    # print(DLGN_params)\n",
        "\n",
        "    optimizer = optim.Adam(DLGN_params, lr=lr, weight_decay=weight_decay)\n",
        "    \n",
        "\n",
        "    # print(\"H2\")\n",
        "    # print(DLGN_params)\n",
        "    \n",
        "    train_data_torch = torch.Tensor(train_data_curr)\n",
        "    vali_data_torch = torch.Tensor(vali_data_curr)\n",
        "    test_data_torch = torch.Tensor(test_data_curr)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr, dtype=torch.int64)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr, dtype=torch.int64)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr, dtype=torch.int64)\n",
        "    if model_type == \"Regression\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr)\n",
        "\n",
        "    num_batches = no_of_batches\n",
        "    batch_size = len(train_data_curr)//num_batches\n",
        "    losses=[]\n",
        "    DLGN_obj_store = []\n",
        "    best_vali_error = len(vali_labels_curr)\n",
        "\n",
        "    # print(\"H3\")\n",
        "    # print(DLGN_params)\n",
        "\n",
        "    for epoch in tqdm(range(num_epoch)):  # loop over the dataset multiple times\n",
        "      running_loss = 0.0\n",
        "      if perfectSq(epoch):\n",
        "        DLGN_obj_store.append(DLGN_FC(to_copy=DLGN_obj))\n",
        "      \n",
        "      for batch_start in range(0,len(train_data_curr),batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        values,gate_scores = DLGN_obj(train_data_torch[batch_start:batch_start+batch_size])\n",
        "        \n",
        "        if model_type == \"Classification\":\n",
        "          outputs = torch.cat((-1*values[-1], values[-1]), dim=1)\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size])\n",
        "        if model_type == \"Regression\":\n",
        "          outputs = values[-1]\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size].reshape(outputs.shape))\n",
        "\n",
        "        # print(\"H4\")\n",
        "        # print(DLGN_params)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print(\"H5\")\n",
        "        # print(DLGN_params)\n",
        "        if freeze:\n",
        "          for node in range(num_hidden_nodes[layer_num]):\n",
        "            if(node!=node_num):\n",
        "              # self.gating_layers[i].weight.data =  torch.Tensor(np.array(to_copy.gating_layers[i].weight.detach().numpy()))\n",
        "              DLGN_params[layer_num].data[node]= DLGN_obj_initial.gating_layers[layer_num].weight.data[node]\n",
        "              DLGN_params[len(DLGN_obj.gating_layers)+layer_num].data[node]= DLGN_obj_initial.gating_layers[layer_num].bias.data[node]\n",
        "\n",
        "        # print(\"H6\")\n",
        "        # print(DLGN_params)\n",
        "\n",
        "        running_loss += loss.item()    \n",
        "      losses.append(running_loss/num_batches)\n",
        "\n",
        "      values,gate_scores =DLGN_obj(vali_data_torch)\n",
        "      if model_type == \"Classification\":\n",
        "        vali_preds = torch.cat((-1*values[-1], values[-1]), dim=1)\n",
        "        vali_preds = torch.argmax(vali_preds, dim=1).numpy()\n",
        "        vali_error= np.sum(vali_labels_torch.numpy()!=vali_preds)\n",
        "      if model_type == \"Regression\":\n",
        "        vali_preds = values[-1]\n",
        "        vali_error = criterion(vali_preds, vali_labels_torch.reshape(vali_preds.shape)).item()\n",
        "        vali_preds = vali_preds.detach().numpy()\n",
        "      if vali_error < best_vali_error:\n",
        "        DLGN_obj_return = DLGN_FC(to_copy=DLGN_obj)\n",
        "        best_vali_error = vali_error\n",
        "      \n",
        "    values,gate_scores =DLGN_obj_return(test_data_torch)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds = torch.cat((-1*values[-1], values[-1]), dim=1)\n",
        "      test_preds = torch.argmax(test_preds, dim=1).numpy()\n",
        "      test_error= np.sum(test_labels_torch.numpy()!=test_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds = values[-1]\n",
        "      test_error = criterion(test_preds, test_labels_torch.reshape(test_preds.shape)).item()\n",
        "      test_preds = test_preds.detach().numpy()\n",
        "    values,gate_scores=DLGN_obj_return(train_data_torch)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      train_preds = torch.cat((-1*values[-1], values[-1]), dim=1)\n",
        "      train_preds = torch.argmax(train_preds, dim=1).numpy()\n",
        "      train_error= np.sum(train_labels_torch.numpy()!=train_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      train_preds = values[-1]\n",
        "      train_error = criterion(train_preds, train_labels_torch.reshape(train_preds.shape)).item()\n",
        "      train_preds = train_preds.detach().numpy()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"DLGN loss vs epoch\")\n",
        "    plt.plot(losses)\n",
        "    return losses,test_error, train_error, test_preds, DLGN_obj_return, DLGN_obj_store"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Train DLGN_LR model**\n",
        "\n",
        "if train_dlgn_lr:\n",
        "  def train_dlgn_lr(seed,num_path_features,num_hidden_layers,input_dim,output_dim,num_hidden_nodes):\n",
        "    set_torchseed(seed)\n",
        "    DLGN_LR_obj = None\n",
        "    DLGN_LR_return = None \n",
        "    DLGN_LR_obj = DLGN_LR_FC(num_path_features=num_path_features,num_hidden_layers=num_hidden_layers, input_dim=input_dim, output_dim=output_dim, \n",
        "                        num_hidden_nodes=num_hidden_nodes,beta=beta)\n",
        "    \n",
        "\n",
        "    print(DLGN_LR_obj)\n",
        "    DLGN_LR_return = DLGN_LR_FC(to_copy=DLGN_LR_obj)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "    if model_type == \"Regression\":\n",
        "      criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "    DLGN_LR_params = []\n",
        "    DLGN_LR_params += [item.weight for item in DLGN_LR_obj.gating_layers]\n",
        "    DLGN_LR_params += [item.bias for item in DLGN_LR_obj.gating_layers]\n",
        "    DLGN_LR_params += [item.weight for item in DLGN_LR_obj.path_layer]\n",
        "    #print(DLGN_LR_params)\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(DLGN_LR_params, lr=lr_path, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "    train_data_torch = torch.Tensor(train_data_curr)\n",
        "    vali_data_torch = torch.Tensor(vali_data_curr)\n",
        "    test_data_torch = torch.Tensor(test_data_curr)\n",
        "\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr, dtype=torch.int64)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr, dtype=torch.int64)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr, dtype=torch.int64)\n",
        "    if model_type == \"Regression\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr)\n",
        "\n",
        "\n",
        "\n",
        "    num_batches = no_of_batches\n",
        "    batch_size = len(train_data_curr)//num_batches\n",
        "    losses=[]\n",
        "    DLGN_LR_obj_store = []\n",
        "    best_vali_error = len(vali_labels_curr)\n",
        "\n",
        "\n",
        "    for epoch in tqdm(range(num_epoch)):  # loop over the dataset multiple times\n",
        "      running_loss = 0.0\n",
        "      if perfectSq(epoch):\n",
        "        DLGN_LR_obj_store.append(DLGN_LR_FC(to_copy=DLGN_LR_obj))\n",
        "      \n",
        "      for batch_start in range(0,len(train_data_curr),batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        path_values,_ = DLGN_LR_obj(train_data_torch[batch_start:batch_start+batch_size])\n",
        "        # print(path_values)\n",
        "        if model_type == \"Classification\":\n",
        "          outputs = torch.cat((-1*path_values, path_values), dim=1)\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size])\n",
        "        if model_type == \"Regression\":\n",
        "          outputs = path_values\n",
        "          # print(train_labels_torch[batch_start:batch_start+batch_size].reshape(outputs.shape))\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size].reshape(outputs.shape))\n",
        "          # print(loss)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()    \n",
        "      losses.append(running_loss/num_batches)\n",
        "      path_values,_ =DLGN_LR_obj(vali_data_torch)\n",
        "      if model_type == \"Classification\":\n",
        "        vali_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "        vali_preds = torch.argmax(vali_preds, dim=1).numpy()\n",
        "        vali_error= np.sum(vali_labels_torch.numpy()!=vali_preds)\n",
        "      if model_type == \"Regression\":\n",
        "        vali_preds = path_values\n",
        "        vali_error = criterion(vali_preds, vali_labels_torch.reshape(vali_preds.shape)).item()\n",
        "        vali_preds = vali_preds.detach().numpy()\n",
        "      if vali_error < best_vali_error:\n",
        "        DLGN_LR_return = DLGN_LR_FC(to_copy=DLGN_LR_obj)\n",
        "        best_vali_error = vali_error\n",
        "      \n",
        "    path_values,_ =DLGN_LR_return(test_data_torch)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "      test_preds = torch.argmax(test_preds, dim=1).numpy()\n",
        "      test_error= np.sum(test_labels_torch.numpy()!=test_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds = path_values\n",
        "      test_error = criterion(test_preds, test_labels_torch.reshape(test_preds.shape)).item()\n",
        "      test_preds = test_preds.detach().numpy()\n",
        "    path_values,_=DLGN_LR_return(train_data_torch)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      train_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "      train_preds = torch.argmax(train_preds, dim=1).numpy()\n",
        "      train_error= np.sum(train_labels_torch.numpy()!=train_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      train_preds = path_values\n",
        "      train_error = criterion(train_preds, train_labels_torch.reshape(train_preds.shape)).item()\n",
        "      train_preds = train_preds.detach().numpy()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Path_LR loss vs epoch\")\n",
        "    plt.plot(losses)\n",
        "    return losses,test_error, train_error, test_preds, DLGN_LR_return, DLGN_LR_obj_store"
      ],
      "metadata": {
        "id": "i30BvmvApRNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL-qsOGL1lV1"
      },
      "outputs": [],
      "source": [
        "#@title **Generating data**\n",
        "if model_type == \"Classification\":\n",
        "  data_gen = data_gen1\n",
        "if model_type == \"Regression\":\n",
        "  data_gen = data_gen2\n",
        "  \n",
        "data,data_labels,train_data,train_data_labels,test_data,test_data_labels,vali_data,vali_data_labels, \\\n",
        "landmarks,labels,centers,mode_frac= data_gen(num_train_data,num_vali_data,num_test_data,dim=2,seed=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I8md4woFVrE"
      },
      "outputs": [],
      "source": [
        "#@title **Plotting the data**\n",
        "if model_type == \"Classification\":\n",
        "  fig=plt.scatter(np.concatenate(data)[:,0], np.concatenate(data)[:,1], c=np.concatenate(data_labels), s=10)\n",
        "if model_type == \"Regression\":\n",
        "  fig=plt.scatter(data[:,0], data[:,1], c=data_labels, s=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF7PO4Lc2jzV"
      },
      "outputs": [],
      "source": [
        "#@title **Training a DLGN model**\n",
        "## Convention 0th layer is input layer 1st layer is 1st hidden layer and last layer is output layer\n",
        "## so 3 hidden layers mean 1 input node + 3 hidden layers + 1 output layer\n",
        "\n",
        "if model_type == \"Classification\":\n",
        "  train_data_curr = np.concatenate(train_data[0:num_modes])\n",
        "  train_labels_curr = np.concatenate(train_data_labels[0:num_modes])\n",
        "  vali_data_curr = np.concatenate(vali_data[0:num_modes])\n",
        "  vali_labels_curr = np.concatenate(vali_data_labels[0:num_modes])\n",
        "  test_data_curr = np.concatenate(test_data[0:num_modes])\n",
        "  test_labels_curr = np.concatenate(test_data_labels[0:num_modes]) \n",
        "\n",
        "if model_type == \"Regression\":\n",
        "  train_data_curr = train_data\n",
        "  train_labels_curr = train_data_labels\n",
        "  vali_data_curr = vali_data\n",
        "  vali_labels_curr = vali_data_labels\n",
        "  test_data_curr = test_data\n",
        "  test_labels_curr = test_data_labels\n",
        "\n",
        "# print(\"Num Train = \",len(train_data_curr))\n",
        "# print(\"Num Vali  = \",len(vali_data_curr))\n",
        "# print(\"Num Test  = \",len(test_data_curr))\n",
        "# print(\"==========DLGN===========\") \n",
        "if train_model:\n",
        "  losses,test_error, train_error, predictions_dlgn, DLGN_obj_final, DLGN_obj_store = train_dlgn(seed,num_hidden_layers,input_dim,output_dim,num_hidden_nodes,beta,\n",
        "                no_of_batches,modep,layer_num,node_num,train_data_curr,vali_data_curr,test_data_curr,train_labels_curr,test_labels_curr,vali_labels_curr,num_epoch,freeze)   \n",
        "\n",
        "  #predictions_dlgn -- labels corresponding to the best validation model\n",
        "  #DLGN_obj_final -- model corresponding to epoch with the best validation acc\n",
        "  #DLGN_obj_store -- store models corresponding to epochs at regular intervals\n",
        "\n",
        "  if NPF_pretrained_freezed:\n",
        "    # seed=141334\n",
        "    NPF_freeze = True\n",
        "    losses,test_error, train_error, predictions_dlgn, DLGN_obj_final, DLGN_obj_store = train_dlgn(seed,num_hidden_layers,input_dim,output_dim,num_hidden_nodes,beta,\n",
        "                no_of_batches,modep,layer_num,node_num,train_data_curr,vali_data_curr,test_data_curr,train_labels_curr,test_labels_curr,vali_labels_curr,num_epoch,freeze,DLGN_obj_pretrained=DLGN_obj_store[-1],NPF_pretrained = True,NPV_pretrained = False)   \n",
        "    NPF_freeze = False\n",
        "  if NPV_pretrained_freezed:\n",
        "    NPV_freeze = True\n",
        "    losses,test_error, train_error, predictions_dlgn, DLGN_obj_final, DLGN_obj_store = train_dlgn(seed,num_hidden_layers,input_dim,output_dim,num_hidden_nodes,beta,\n",
        "                no_of_batches,modep,layer_num,node_num,train_data_curr,vali_data_curr,test_data_curr,train_labels_curr,test_labels_curr,vali_labels_curr,num_epoch,freeze,DLGN_obj_pretrained=DLGN_obj_store[-1],NPF_pretrained = False,NPV_pretrained = True)   \n",
        "    NPV_freeze = False"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if train_dlgn_lr:\n",
        "  num_path_features=np.prod(num_hidden_nodes)\n",
        "  losses,test_error, train_error, predictions_dlgn_lr, DLGN_LR_obj_final, DLGN_LR_obj_store = train_dlgn_lr(seed,num_path_features,num_hidden_layers,input_dim,output_dim,num_hidden_nodes)   \n",
        "  file_name = \"dlgn_lr_model\"\n",
        "  file_name=file_path+file_name\n",
        "  savez_compressed(file_name, losses,test_error, train_error, predictions_dlgn_lr, DLGN_LR_obj_final, DLGN_LR_obj_store)\n",
        "  file_name_load = file_name+\".npz\""
      ],
      "metadata": {
        "id": "toD7u30muO7l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DLGN_LR_obj_store[1].gating_layers[0].weight"
      ],
      "metadata": {
        "id": "ekUMZMqxAiLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE3jrTJ6VSDB"
      },
      "outputs": [],
      "source": [
        "#@title **Storing the trained output**\n",
        "if train_model:\n",
        "  Beta_str = \"Beta_\"+str(beta[0])+\"_\"\n",
        "  if model_type == \"Regression\":\n",
        "    a=\"R_\"\n",
        "  if model_type == \"Classification\":\n",
        "    a=\"C_\"\n",
        "  if freeze:\n",
        "    prefix = \"Layer_\"+str(layer_num)+\"_node_\"+str(node_num)+\"_\"\n",
        "    Beta_str = \"Beta_\"+str(beta[0])+\"_\"+str(int(beta[1]/1000))+\"k_\"\n",
        "  elif NPF_freeze:\n",
        "    prefix = \"NPF_Fixed_\"\n",
        "  elif NPV_freeze:\n",
        "    prefix = \"NPV_Fixed_\"\n",
        "  elif NPF_pretrained_freezed:\n",
        "    prefix = \"NPF_pretrained_Fixed_\"\n",
        "  elif NPV_pretrained_freezed:\n",
        "    prefix = \"NPV_pretrained_Fixed_\"\n",
        "  else:\n",
        "    prefix = \"No_node_\"\n",
        "  epoch_str = str(int(num_epoch/100000))+\"L_\"\n",
        "  file_name = a+prefix+epoch_str+str(num_hidden_layers)+\"L\"+str(max_no_of_nodes)+\"N_\"+Beta_str+\"seed_\"+str(seed)\n",
        "  \n",
        "  # save to npy file\n",
        "  file_name=file_path+file_name\n",
        "  savez_compressed(file_name, losses,test_error, train_error, predictions_dlgn, DLGN_obj_final, DLGN_obj_store)\n",
        "  file_name_load = file_name+\".npz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfyL6scfMPL-"
      },
      "source": [
        "**Infer the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hII-DPU_bp4"
      },
      "outputs": [],
      "source": [
        "#@title **Load model**\n",
        "if infer_model:\n",
        "  \n",
        "  # load dict of arrays\n",
        "  dict_data = load(file_name_load,allow_pickle=True)\n",
        "  losses=dict_data['arr_0']\n",
        "  test_error=dict_data['arr_1'].item()\n",
        "  train_error=dict_data['arr_2'].item()\n",
        "  predictions_dlgn=dict_data['arr_3']\n",
        "  DLGN_obj_final=dict_data['arr_4'].item()\n",
        "  DLGN_obj_store=dict_data['arr_5']\n",
        "  plt.figure()\n",
        "  plt.title(\"DLGN loss vs epoch\")\n",
        "  fig=plt.plot(losses[:])\n",
        "  if model_type == \"Classification\":\n",
        "    print(\"test_error\",test_error/len(test_data_curr))\n",
        "    print(\"train_error\",train_error/len(train_data_curr))\n",
        "    print('DLGN acc=',np.sum(predictions_dlgn==test_labels_curr)/len(test_data_curr))\n",
        "  if model_type == \"Regression\":\n",
        "    print(\"test_error\",test_error)\n",
        "    print(\"train_error\",train_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Infer dlgn lr model**"
      ],
      "metadata": {
        "id": "1eB1A72kjkp-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if infer_model:\n",
        "  if train_dlgn_lr:\n",
        "    # load dict of arrays\n",
        "    dict_data = load(file_name_load,allow_pickle=True)\n",
        "    losses=dict_data['arr_0']\n",
        "    test_error=dict_data['arr_1'].item()\n",
        "    train_error=dict_data['arr_2'].item()\n",
        "    predictions_dlgn_lr=dict_data['arr_3']\n",
        "    DLGN_LR_obj_final=dict_data['arr_4'].item()\n",
        "    DLGN_LR_obj_store=dict_data['arr_5']\n",
        "    plt.figure()\n",
        "    plt.title(\"DLGN LR loss vs epoch\")\n",
        "    fig=plt.plot(losses[:])\n",
        "    if model_type == \"Classification\":\n",
        "      print(\"test_error\",test_error/len(test_data_curr))\n",
        "      print(\"train_error\",train_error/len(train_data_curr))\n",
        "      print('DLGN acc=',np.sum(predictions_dlgn_lr==test_labels_curr)/len(test_data_curr))\n",
        "    if model_type == \"Regression\":\n",
        "      print(\"test_error\",test_error)\n",
        "      print(\"train_error\",train_error)\n"
      ],
      "metadata": {
        "id": "X0P5RsVAieYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **Path_LR data preparation sigmoid method**\n",
        "def Path_LR_dataPrep_sigmoid(data_curr):\n",
        "  DLGN_obj = DLGN_obj_store[-1]\n",
        "  effective_weights, effective_biases = DLGN_obj.return_gating_functions()\n",
        "\n",
        "  x=(data_curr@effective_weights[0].data.numpy().T)+effective_biases[0].data.numpy()\n",
        "  y=(data_curr@effective_weights[1].data.numpy().T)+effective_biases[1].data.numpy()\n",
        "  z=(data_curr@effective_weights[2].data.numpy().T)+effective_biases[2].data.numpy()\n",
        "\n",
        "  x1=torch.sigmoid(beta[0]*torch.Tensor(x))\n",
        "  y1=torch.sigmoid(beta[0]*torch.Tensor(y))\n",
        "  z1=torch.sigmoid(beta[0]*torch.Tensor(z))\n",
        "\n",
        "  xy=np.zeros((x1.shape[0],x1.shape[1]*y1.shape[1]))\n",
        "  for i in range(x1.shape[0]):\n",
        "    xy[i]=np.multiply.outer(x1[i], y1[i]).ravel()\n",
        "\n",
        "  xyz=np.zeros((x1.shape[0],xy.shape[1]*z1.shape[1]))\n",
        "  for i in range(x1.shape[0]):\n",
        "    xyz[i]=np.multiply.outer(xy[i], z1[i]).ravel()\n",
        "  return xyz"
      ],
      "metadata": {
        "id": "GjYaeRr4MtWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Of8F6NgSHdl9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45c1ennqpCW-"
      },
      "outputs": [],
      "source": [
        "#@title **Path_LR data preparation hyper plane method**\n",
        "if Path_LR_hypPlane:\n",
        "  num_path_features =np.prod(num_hidden_nodes)\n",
        "  path_fea_train = np.zeros(shape=(len(train_data_curr),1)) #3000x1\n",
        "  path_fea_vali = np.zeros(shape=(len(vali_data_curr),1))\n",
        "  path_fea_test = np.zeros(shape=(len(test_data_curr),1))\n",
        "  DLGN_obj = DLGN_obj_store[-1]\n",
        "  effective_weights, effective_biases = DLGN_obj.return_gating_functions()\n",
        "\n",
        "  for i in range(max_no_of_nodes):\n",
        "    for p in range(max_no_of_nodes):\n",
        "      for k in range(max_no_of_nodes):\n",
        "        enter_path=(i,p,k)\n",
        "        Z_path_train = []\n",
        "        Z_path_vali = []\n",
        "        Z_path_test = []\n",
        "        for j in range(len(enter_path)):\n",
        "          weight = effective_weights[j][enter_path[j]].data.numpy()\n",
        "          bias = effective_biases[j][enter_path[j]].data.numpy()\n",
        "          Z_train = ((train_data_curr@weight.T)+bias)>0\n",
        "          Z_vali = ((vali_data_curr@weight.T)+bias)>0\n",
        "          Z_test = ((test_data_curr@weight.T)+bias)>0\n",
        "\n",
        "          Z_path_train.append(Z_train)\n",
        "          Z_path_vali.append(Z_vali)\n",
        "          Z_path_test.append(Z_test)\n",
        "\n",
        "        Z_result_train = Z_path_train[0]\n",
        "        for index in range(1,len(Z_path_train)):\n",
        "          Z_result_train = Z_result_train & Z_path_train[index]\n",
        "        Z_result_train = Z_result_train.reshape([len(Z_result_train),1])\n",
        "        path_fea_train=np.append(path_fea_train,Z_result_train,axis=1)\n",
        "\n",
        "        Z_result_vali = Z_path_vali[0]\n",
        "        for index in range(1,len(Z_path_vali)):\n",
        "          Z_result_vali = Z_result_vali & Z_path_vali[index]\n",
        "        Z_result_vali = Z_result_vali.reshape([len(Z_result_vali),1])\n",
        "        path_fea_vali=np.append(path_fea_vali,Z_result_vali,axis=1)\n",
        "\n",
        "        Z_result_test = Z_path_test[0]\n",
        "        for index in range(1,len(Z_path_test)):\n",
        "          Z_result_test = Z_result_test & Z_path_test[index]\n",
        "        Z_result_test = Z_result_test.reshape([len(Z_result_test),1])\n",
        "        path_fea_test=np.append(path_fea_test,Z_result_test,axis=1)\n",
        "\n",
        "  path_fea_train=path_fea_train[:,1:]\n",
        "  path_fea_vali=path_fea_vali[:,1:]\n",
        "  path_fea_test=path_fea_test[:,1:]\n",
        "\n",
        "else:\n",
        "  num_path_features =np.prod(num_hidden_nodes)\n",
        "  path_fea_train=Path_LR_dataPrep_sigmoid(train_data_curr)\n",
        "  path_fea_vali=Path_LR_dataPrep_sigmoid(vali_data_curr)\n",
        "  path_fea_test=Path_LR_dataPrep_sigmoid(test_data_curr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD7efbGpTszg"
      },
      "outputs": [],
      "source": [
        "#@title **Train Path_LR model**\n",
        "if train_lr:\n",
        "  \n",
        "  def train_path_lr(seed,num_path_features,output_dim,no_of_batches):\n",
        "    set_torchseed(seed)\n",
        "    Path_LR_obj = None\n",
        "    Path_LR_return = None\n",
        "    Path_LR_obj = Path_LR(num_path_features=num_path_features, output_dim=output_dim)\n",
        "    \n",
        "    Path_LR_return = Path_LR(to_copy=Path_LR_obj)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "    if model_type == \"Regression\":\n",
        "      criterion = nn.MSELoss()\n",
        "    \n",
        "    \n",
        "\n",
        "    Path_LR_params = []\n",
        "    Path_LR_params += [item.weight for item in Path_LR_obj.path_layer]\n",
        "\n",
        "    optimizer = optim.Adam(Path_LR_params, lr=lr_path, weight_decay=weight_decay)\n",
        "    train_data_torch = torch.Tensor(path_fea_train)\n",
        "    vali_data_torch = torch.Tensor(path_fea_vali)\n",
        "    test_data_torch = torch.Tensor(path_fea_test)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr, dtype=torch.int64)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr, dtype=torch.int64)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr, dtype=torch.int64)\n",
        "    if model_type == \"Regression\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr)\n",
        "\n",
        "    num_batches = no_of_batches\n",
        "    batch_size = len(train_data_curr)//num_batches\n",
        "    losses=[]\n",
        "    Path_LR_obj_store = []\n",
        "    best_vali_error = len(vali_labels_curr)\n",
        "\n",
        "\n",
        "    for epoch in tqdm(range(num_epoch)):  # loop over the dataset multiple times\n",
        "      running_loss = 0.0\n",
        "      if perfectSq(epoch):\n",
        "        Path_LR_obj_store.append(Path_LR(to_copy=Path_LR_obj))\n",
        "      \n",
        "      for batch_start in range(0,len(train_data_curr),batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        path_values = Path_LR_obj(train_data_torch[batch_start:batch_start+batch_size])\n",
        "        \n",
        "        if model_type == \"Classification\":\n",
        "          outputs = torch.cat((-1*path_values, path_values), dim=1)\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size])\n",
        "        if model_type == \"Regression\":\n",
        "          outputs = path_values\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size].reshape(outputs.shape))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()    \n",
        "      losses.append(running_loss/num_batches)\n",
        "\n",
        "      path_values =Path_LR_obj(vali_data_torch)\n",
        "      if model_type == \"Classification\":\n",
        "        vali_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "        vali_preds = torch.argmax(vali_preds, dim=1).numpy()\n",
        "        vali_error= np.sum(vali_labels_torch.numpy()!=vali_preds)\n",
        "      if model_type == \"Regression\":\n",
        "        vali_preds = path_values\n",
        "        vali_error = criterion(vali_preds, vali_labels_torch.reshape(vali_preds.shape)).item()\n",
        "        vali_preds = vali_preds.detach().numpy()\n",
        "      if vali_error < best_vali_error:\n",
        "        Path_LR_obj_return = Path_LR(to_copy=Path_LR_obj)\n",
        "        best_vali_error = vali_error\n",
        "      \n",
        "    path_values =Path_LR_obj_return(test_data_torch)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "      test_preds = torch.argmax(test_preds, dim=1).numpy()\n",
        "      test_error= np.sum(test_labels_torch.numpy()!=test_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds = path_values\n",
        "      test_error = criterion(test_preds, test_labels_torch.reshape(test_preds.shape)).item()\n",
        "      test_preds = test_preds.detach().numpy()\n",
        "    path_values=Path_LR_obj_return(train_data_torch)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      train_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "      train_preds = torch.argmax(train_preds, dim=1).numpy()\n",
        "      train_error= np.sum(train_labels_torch.numpy()!=train_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      train_preds = path_values\n",
        "      train_error = criterion(train_preds, train_labels_torch.reshape(train_preds.shape)).item()\n",
        "      train_preds = train_preds.detach().numpy()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Path_LR loss vs epoch\")\n",
        "    plt.plot(losses)\n",
        "    return losses,test_error, train_error, test_preds, Path_LR_obj_return, Path_LR_obj_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrY-2uOT2cvG"
      },
      "outputs": [],
      "source": [
        "#@title **Save Path_LR model**\n",
        "if train_lr:\n",
        "  losses_lr,test_error_lr, train_error_lr, predictions_lr, Path_LR_obj_final, Path_LR_obj_store = train_path_lr(seed,num_path_features,output_dim,no_of_batches)\n",
        "  # save numpy array as npz file\n",
        " \n",
        "  # save to npy file\n",
        "  file_name_lr=file_name_load_lr\n",
        "  savez_compressed(file_name_lr, losses_lr,test_error_lr, train_error_lr, predictions_lr, Path_LR_obj_final, Path_LR_obj_store)\n",
        "  file_name_load_lr = file_name_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5seNSwW24CTS"
      },
      "outputs": [],
      "source": [
        "#@title **Load model Path_LR**\n",
        "if infer_model:\n",
        "  if infer_path_lr:\n",
        "    # load dict of arrays\n",
        "    dict_data = load(file_name_load_lr,allow_pickle=True)\n",
        "    losses_lr=dict_data['arr_0']\n",
        "    test_error_lr=dict_data['arr_1'].item()\n",
        "    train_error_lr=dict_data['arr_2'].item()\n",
        "    predictions_lr=dict_data['arr_3']\n",
        "    Path_LR_obj_final=dict_data['arr_4'].item()\n",
        "    Path_LR_obj_store=dict_data['arr_5']\n",
        "    plt.figure()\n",
        "    plt.title(\"Path_LR loss vs epoch\")\n",
        "    fig=plt.plot(losses_lr[:])\n",
        "    if model_type == \"Classification\":\n",
        "      print(\"test_error\",test_error_lr/len(test_data_curr))\n",
        "      print(\"train_error\",train_error_lr/len(train_data_curr))\n",
        "      print('DLGN acc=',np.sum(predictions_lr==test_labels_curr)/len(test_data_curr))\n",
        "    if model_type == \"Regression\":\n",
        "      print(\"test_error\",test_error_lr)\n",
        "      print(\"train_error\",train_error_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXLAKYGC5cLO"
      },
      "outputs": [],
      "source": [
        "#@title **Data vs prediction scatter plot**\n",
        "def data_pred_scatter_plot(predictions):\n",
        "  plt.figure()\n",
        "  plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
        "  plt.scatter(test_data_curr[:,0], test_data_curr[:,1], c=test_labels_curr,vmin=-1,vmax=1)\n",
        "  plt.colorbar()\n",
        "  plt.title(\"Real labelled test data\")\n",
        "  plt.figure()\n",
        "\n",
        "  plt.subplot(1, 2, 2) # index 2\n",
        "  plt.scatter(test_data_curr[:,0], test_data_curr[:,1], c=predictions,vmin=-1,vmax=1)\n",
        "  plt.title(\"Predicted Labelled test data\")\n",
        "  plt.colorbar()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOt7NkqfN0Zm"
      },
      "outputs": [],
      "source": [
        "#@title **Calculate Path values**\n",
        "if infer_model:\n",
        "  def path_value_cal(Model_obj,weight_model_type):\n",
        "    complete_path_vals = np.zeros(tuple(num_hidden_nodes))\n",
        "    complete_paths = list(cartesian_prod(*[range(x) for x in num_hidden_nodes]))\n",
        "    \n",
        "    for path in complete_paths:\n",
        "      if weight_model_type == \"DLGN\":\n",
        "        temp = np.dot(Model_obj.value_layers[0].weight.data.numpy()[path[0],:], np.ones(input_dim))\n",
        "        for k in range(1,num_hidden_layers):\n",
        "            temp *= Model_obj.value_layers[k].weight.data.numpy()[path[k], path[k-1]]\n",
        "        temp *= Model_obj.value_layers[num_hidden_layers].weight.data.numpy()[0, path[-1]]\n",
        "      if weight_model_type==\"Path_LR\" or weight_model_type ==\"DLGN_LR\":\n",
        "        temp = Model_obj.path_layer[0].weight.data.numpy()[0,path[0]*max_no_of_nodes**2+path[1]*max_no_of_nodes**1+path[2]*max_no_of_nodes**0]\n",
        "      complete_path_vals[path]=temp\n",
        "    return complete_paths,complete_path_vals\n",
        "\n",
        "\n",
        "  if print_all_paths:\n",
        "    print(\"================================BEST EPOCH======================================\")\n",
        "    DLGN_obj = DLGN_obj_final\n",
        "    complete_paths,complete_path_vals = path_value_cal(DLGN_obj)\n",
        "    print(complete_paths)\n",
        "    print(complete_path_vals)\n",
        "    # print(\"Path values of layer {} node {}\".format(layer_num,node_num))\n",
        "    # print(complete_paths[node_num-1::num_hidden_nodes[layer_num-1]])\n",
        "    # print(complete_path_vals[:,:,node_num-1]) #3rd layer nodes\n",
        "    # print(complete_path_vals[:,node_num-1,:])#2nd layer nodes\n",
        "    print(\"=================================0TH EPOCH====================================\")\n",
        "\n",
        "    DLGN_obj = DLGN_obj_store[0]\n",
        "    complete_paths,complete_path_vals = path_value_cal(DLGN_obj)\n",
        "    print(complete_paths)\n",
        "    print(complete_path_vals)\n",
        "    # print(\"Path values of layer {} node {}\".format(layer_num,node_num))\n",
        "    # print(complete_paths[node_num-1::num_hidden_nodes[layer_num-1]])\n",
        "    # print(complete_path_vals[:,:,node_num-1]) #3rd layer nodes\n",
        "    # print(complete_path_vals[:,node_num-1,:])#2nd layer nodes\n",
        "    print(\"===============================MIDDLE EPOCH====================================\")\n",
        "\n",
        "    DLGN_obj = DLGN_obj_store[int(np.sqrt(num_epoch)//2)]\n",
        "    complete_paths,complete_path_vals = path_value_cal(DLGN_obj)\n",
        "    print(complete_paths)\n",
        "    print(complete_path_vals)\n",
        "    # print(\"Path values of layer {} node {}\".format(layer_num,node_num))\n",
        "    # print(complete_paths[node_num-1::num_hidden_nodes[layer_num-1]])\n",
        "    # print(complete_path_vals[:,:,node_num-1]) #3rd layer nodes\n",
        "    # print(complete_path_vals[:,node_num-1,:])#2nd layer nodes\n",
        "    print(\"================================LAST EPOCH==================================\")\n",
        "\n",
        "    DLGN_obj = DLGN_obj_store[int(np.sqrt(num_epoch))-1]\n",
        "    complete_paths,complete_path_vals = path_value_cal(DLGN_obj)\n",
        "    print(complete_paths)\n",
        "    print(complete_path_vals)\n",
        "    # print(\"Path values of layer {} node {}\".format(layer_num,node_num))\n",
        "    # print(complete_paths[node_num-1::num_hidden_nodes[layer_num-1]])\n",
        "    # print(complete_path_vals[:,:,node_num-1]) #3rd layer nodes\n",
        "    # print(complete_path_vals[:,node_num-1,:])#2nd layer nodes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkmhLUXA5Z3s"
      },
      "outputs": [],
      "source": [
        "#@title **Path value of a particular path of a particular epoch**\n",
        "def print_path_value(enter_path,enter_epoch,weight_model_type):\n",
        "  if infer_model:\n",
        "    if weight_model_type==\"DLGN\":\n",
        "      Model_obj = DLGN_obj_store[int(np.sqrt(enter_epoch))]\n",
        "    if weight_model_type == \"Path_LR\":\n",
        "      Model_obj = Path_LR_obj_store[int(np.sqrt(enter_epoch))]\n",
        "    if weight_model_type == \"DLGN_LR\":\n",
        "      Model_obj = DLGN_LR_obj_store[int(np.sqrt(enter_epoch))]\n",
        "    complete_paths,complete_path_vals = path_value_cal(Model_obj,weight_model_type)\n",
        "    # print(f'Value of path {enter_path} of epoch {enter_epoch} is : {complete_path_vals[enter_path]}')\n",
        "    return complete_path_vals[enter_path]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcuRagVm-eSY"
      },
      "outputs": [],
      "source": [
        "#@title **Visualization of node hyperplane for a particular epoch**\n",
        "def show_node_hyp(enter_epoch,weight_model_type):\n",
        "  if infer_model:\n",
        "    DLGN_obj = DLGN_obj_store[int(np.sqrt(enter_epoch))]\n",
        "    effective_weights, effective_biases = DLGN_obj.return_gating_functions()\n",
        "    X=test_data_curr[:,0]\n",
        "    Y=test_data_curr[:,1]\n",
        "    if weight_model_type == \"DLGN\":\n",
        "      test_data_torch = torch.Tensor(test_data_curr) \n",
        "      values,gate_scores =DLGN_obj(test_data_torch)\n",
        "      path_values = values[-1]\n",
        "    if weight_model_type == \"Path_LR\":\n",
        "      test_data_torch = torch.Tensor(path_fea_test) \n",
        "      Path_LR_obj = Path_LR_obj_store[int(np.sqrt(enter_epoch))]\n",
        "      path_values = Path_LR_obj(test_data_torch)\n",
        "\n",
        "    if weight_model_type == \"DLGN_LR\":\n",
        "      DLGN_LR_obj = DLGN_LR_obj_store[int(np.sqrt(enter_epoch))]\n",
        "      effective_weights, effective_biases = DLGN_LR_obj.return_gating_functions()\n",
        "      test_data_torch = torch.Tensor(test_data_curr) \n",
        "      path_values,_ = DLGN_LR_obj(test_data_torch)\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds = torch.cat((-1*path_values,path_values),dim=1)\n",
        "      predictions = torch.argmax(test_preds,dim=1).numpy()\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds = path_values\n",
        "      predictions=test_preds.detach().numpy()\n",
        "\n",
        "    no_rows=num_hidden_layers+1\n",
        "    no_colns = max_no_of_nodes\n",
        "    fig,ax=plt.subplots(no_rows,no_colns,figsize=(5*no_colns,4*no_rows)) \n",
        "    fig.tight_layout(pad=2)\n",
        "    fig.text(0,0,s=\"Epoch \"+str(enter_epoch))\n",
        "    for k in range(no_colns//2):\n",
        "      im_hide = ax[0,k].axis('off')\n",
        "    for k in range(no_colns//2+2,no_colns):\n",
        "      im_hide = ax[0,k].axis('off')\n",
        "    im0=ax[0,no_colns//2].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "    ax[0,no_colns//2].title.set_text(\"Real\")\n",
        "    ax[0,no_colns//2].set_xlim(-2,2)\n",
        "    ax[0,no_colns//2].set_ylim(-4,4)\n",
        "    divider = make_axes_locatable(ax[0,no_colns//2])\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "    im1=ax[0,no_colns//2+1].scatter(X, Y, c=predictions,vmin=-1,vmax=1)\n",
        "    ax[0,no_colns//2+1].title.set_text(\"Predicted\")\n",
        "    ax[0,no_colns//2+1].set_xlim(-2,2)\n",
        "    ax[0,no_colns//2+1].set_ylim(-4,4)\n",
        "    divider = make_axes_locatable(ax[0,no_colns//2+1])\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    fig.colorbar(im1, cax=cax, orientation='vertical')\n",
        "    for j in range(num_hidden_layers):\n",
        "      for i in range( num_hidden_nodes[j]):\n",
        "        weight = effective_weights[j][i].data.numpy()\n",
        "        bias = effective_biases[j][i].data.numpy()\n",
        "        Z = ((test_data_curr@weight.T)+bias)>0\n",
        "        line_eq = (-bias - X*weight[0])/weight[1]\n",
        "        im=ax[j+1,i].scatter(X, Y, c=Z,vmin=-1,vmax=1)\n",
        "        im1=ax[j+1,i].plot(X,line_eq,label=\"(\"+str(round(weight.data[0],4))+\")x+\"+\"(\"+str(round(weight.data[1],4))+\")y+\"+\"(\"+str(round(bias.item(),2))+\")=0\")\n",
        "        ax[j+1,i].set_xlim(-2,2)\n",
        "        ax[j+1,i].set_ylim(-4,4)\n",
        "        ax[j+1,i].title.set_text(\"Layer: \"+str(j+1)+\" Node: \"+str(i+1))\n",
        "        ax[j+1,i].legend()\n",
        "        divider = make_axes_locatable(ax[j+1,i])\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "    path0=\"/content/drive/MyDrive/Research/DLGN_fixed_node/\"\n",
        "    if save_img:\n",
        "      fig.savefig(str(path0)+\"Epoch_\"+str(enter_epoch)+\".pdf\", dpi=300,format=\"pdf\")\t\t\t\t\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OexQWHoBxSo"
      },
      "outputs": [],
      "source": [
        "#@title **Node hyperplane for best model**\n",
        "if infer_model:\n",
        "  if best_epoch_show:\n",
        "    effective_weights, effective_biases = DLGN_obj_final.return_gating_functions()\n",
        "    X=test_data_curr[:,0]\n",
        "    Y=test_data_curr[:,1]\n",
        "    no_rows=num_hidden_layers+1\n",
        "    no_colns = max_no_of_nodes\n",
        "    fig,ax=plt.subplots(no_rows,no_colns,figsize=(5*no_colns,4*no_rows)) \n",
        "    fig.tight_layout(pad=2)\n",
        "    fig.text(0,0,s=\"Best Epoch\")\n",
        "    for k in range(no_colns//2):\n",
        "      im_hide = ax[0,k].axis('off')\n",
        "    for k in range(no_colns//2+2,no_colns):\n",
        "      im_hide = ax[0,k].axis('off')\n",
        "    im0=ax[0,no_colns//2].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "    ax[0,no_colns//2].title.set_text(\"Real\")\n",
        "    ax[0,no_colns//2].set_xlim(-2,2)\n",
        "    ax[0,no_colns//2].set_ylim(-4,4)\n",
        "    divider = make_axes_locatable(ax[0,no_colns//2])\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "    im1=ax[0,no_colns//2+1].scatter(X, Y, c=predictions_dlgn,vmin=-1,vmax=1)\n",
        "    ax[0,no_colns//2+1].title.set_text(\"Predicted\")\n",
        "    ax[0,no_colns//2+1].set_xlim(-2,2)\n",
        "    ax[0,no_colns//2+1].set_ylim(-4,4)\n",
        "    divider = make_axes_locatable(ax[0,no_colns//2+1])\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    fig.colorbar(im1, cax=cax, orientation='vertical')\n",
        "    for j in range(num_hidden_layers):\n",
        "      for i in range( num_hidden_nodes[j]):\n",
        "        weight = effective_weights[j][i].data.numpy()\n",
        "        bias = effective_biases[j][i].data.numpy()\n",
        "        Z = ((test_data_curr@weight.T)+bias)>0\n",
        "        line_eq = (-bias - X*weight[0])/weight[1]\n",
        "        im=ax[j+1,i].scatter(X, Y, c=Z,vmin=-1,vmax=1)\n",
        "        im1=ax[j+1,i].plot(X,line_eq,label=\"(\"+str(round(weight.data[0],4))+\")x+\"+\"(\"+str(round(weight.data[1],4))+\")y+\"+\"(\"+str(round(bias.item(),2))+\")=0\")\n",
        "        ax[j+1,i].set_xlim(-2,2)\n",
        "        ax[j+1,i].set_ylim(-4,4)\n",
        "        ax[j+1,i].title.set_text(\"Layer: \"+str(j+1)+\" Node: \"+str(i+1))\n",
        "        ax[j+1,i].legend()\n",
        "        divider = make_axes_locatable(ax[j+1,i])\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "    # fig.subplots_adjust(right=0.8)\n",
        "    # cbar_ax = fig.add_axes([0.85, 0.12, 0.01, 0.82])\n",
        "    # fig.colorbar(im0, cax=cbar_ax)\n",
        "    path0=\"/content/drive/MyDrive/Research/DLGN_fixed_node/\"\n",
        "    if save_img:\n",
        "      fig.savefig(str(path0)+\"Best_epoch.pdf\", dpi=300,format=\"pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONwdZkXSaKaL"
      },
      "outputs": [],
      "source": [
        "#@title **Change of node hyperplane for each layer**\n",
        "def perfectCube(N) :\n",
        "\tcube_root = round(N**(1/3));\n",
        "\tif cube_root * cube_root * cube_root == N :\n",
        "\t\treturn True;\n",
        "\telse :\n",
        "\t\treturn False;\n",
        "if infer_model:\n",
        "\tif test_on_train_data:\n",
        "\t\ttest_data_curr = train_data_curr\n",
        "\t\ttest_labels_curr = train_labels_curr\n",
        "\tif infer_dlgn_lr:\n",
        "\t\tDLGN_obj_store = DLGN_LR_obj_store\n",
        "\tif show_all_epoch_node_hyperplane:\n",
        "\t\tfor epoch_no in range(len(DLGN_obj_store)):\n",
        "\t\t\tif ((epoch_no%10)==0):\n",
        "\t\t\t\tprint(\"________________________________\")\n",
        "\t\t\t\tprint(\"=======Epoch no: \", epoch_no**2)\n",
        "\t\t\t\tprint(\"________________________________\")\n",
        "\n",
        "\t\t\t\teffective_weights, effective_biases = DLGN_obj_store[epoch_no].return_gating_functions()\n",
        "\t\t\t\tX=test_data_curr[:,0]\n",
        "\t\t\t\tY=test_data_curr[:,1]\n",
        "\t\t\t\ttest_data_torch = torch.Tensor(test_data_curr)\n",
        "\t\t\t\tvalues,gate_scores =DLGN_obj_store[epoch_no](test_data_torch)\n",
        "\n",
        "\t\t\t\tif model_type == \"Classification\":\n",
        "\t\t\t\t\ttest_preds = torch.cat((-1*values[-1],values[-1]),dim=1)\n",
        "\t\t\t\t\tpredictions = torch.argmax(test_preds,dim=1).numpy()\n",
        "\t\t\t\tif model_type == \"Regression\":\n",
        "\t\t\t\t\ttest_preds = values[-1]\n",
        "\t\t\t\t\tpredictions=test_preds.detach().numpy()\n",
        "\n",
        "\t\t\t\tno_rows=num_hidden_layers+1\n",
        "\t\t\t\tno_colns = max_no_of_nodes\n",
        "\t\t\t\tfig,ax=plt.subplots(no_rows,no_colns,figsize=(5*no_colns,4*no_rows)) \n",
        "\t\t\t\tfig.tight_layout(pad=2)\n",
        "\t\t\t\tfig.text(0,0,s=\"Epoch no: \"+ str(epoch_no**2))\n",
        "\t\t\t\tfor k in range(no_colns//2):\n",
        "\t\t\t\t\tim_hide = ax[0,k].axis('off')\n",
        "\t\t\t\tfor k in range(no_colns//2+2,no_colns):\n",
        "\t\t\t\t\tim_hide = ax[0,k].axis('off')\n",
        "\t\t\t\tim0=ax[0,no_colns//2].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "\t\t\t\tax[0,no_colns//2].title.set_text(\"Real\")\n",
        "\t\t\t\tax[0,no_colns//2].set_xlim(-2,2)\n",
        "\t\t\t\tax[0,no_colns//2].set_ylim(-4,4)\n",
        "\t\t\t\tdivider = make_axes_locatable(ax[0,no_colns//2])\n",
        "\t\t\t\tcax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "\t\t\t\tfig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "\n",
        "\t\t\t\tim1=ax[0,no_colns//2+1].scatter(X, Y, c=predictions,vmin=-1,vmax=1)\n",
        "\t\t\t\tax[0,no_colns//2+1].title.set_text(\"Predicted\")\n",
        "\t\t\t\tax[0,no_colns//2+1].set_xlim(-2,2)\n",
        "\t\t\t\tax[0,no_colns//2+1].set_ylim(-4,4)\n",
        "\t\t\t\tdivider = make_axes_locatable(ax[0,no_colns//2+1])\n",
        "\t\t\t\tcax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "\t\t\t\tfig.colorbar(im1, cax=cax, orientation='vertical')\n",
        "\n",
        "\t\t\t\tfor j in range(num_hidden_layers):\n",
        "\t\t\t\t\tfor i in range( num_hidden_nodes[j]):\n",
        "\t\t\t\t\t\tweight = effective_weights[j][i].data.numpy()\n",
        "\t\t\t\t\t\tbias = effective_biases[j][i].data.numpy()\n",
        "\t\t\t\t\t\tZ = ((test_data_curr@weight.T)+bias)>0\n",
        "\t\t\t\t\t\tline_eq = (-bias - X*weight[0])/weight[1]\n",
        "\n",
        "\t\t\t\t\t\tim=ax[j+1,i].scatter(X, Y, c=Z,vmin=-1,vmax=1)\n",
        "\t\t\t\t\t\tim1=ax[j+1,i].plot(X,line_eq,label=\"(\"+str(round(weight.data[0],4))+\")x+\"+\"(\"+str(round(weight.data[1],4))+\")y+\"+\"(\"+str(round(bias.item(),4))+\")=0\")\n",
        "\t\t\t\t\t\tax[j+1,i].set_xlim(-2,2)\n",
        "\t\t\t\t\t\tax[j+1,i].set_ylim(-4,4)\n",
        "\t\t\t\t\t\tax[j+1,i].title.set_text(\"Layer: \"+str(j+1)+\" Node: \"+str(i+1))\n",
        "\t\t\t\t\t\tax[j+1,i].legend()\n",
        "\t\t\t\t\t\tdivider = make_axes_locatable(ax[j+1,i])\n",
        "\t\t\t\t\t\tcax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "\t\t\t\t\t\tfig.colorbar(im, cax=cax, orientation='vertical')\n",
        "\t\t\t\t# fig.subplots_adjust(right=0.8)\n",
        "\t\t\t\t# cbar_ax = fig.add_axes([0.85, 0.12, 0.01, 0.82])\n",
        "\t\t\t\t# fig.colorbar(im0, cax=cbar_ax)\n",
        "\t\t\t\tpath=\"/content/drive/MyDrive/Research/DLGN_fixed_node/Layer_3_Node_2/\"\n",
        "\t\t\t\tif save_img:\n",
        "\t\t\t\t\tfig.savefig(str(path)+\"Epoch_\"+str(epoch_no**2)+\"_.png\", dpi=300)\n",
        "\t\t\t\tplt.show\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4jwe_jHVxdB"
      },
      "outputs": [],
      "source": [
        "#@title **Visualization of a particular path for a particular epoch**\n",
        "def show_path_hyp(enter_path,enter_epoch):\n",
        "  if infer_model:\n",
        "    DLGN_obj = DLGN_obj_store[int(np.sqrt(enter_epoch))]\n",
        "    effective_weights, effective_biases = DLGN_obj.return_gating_functions()\n",
        "    X=test_data_curr[:,0]\n",
        "    Y=test_data_curr[:,1]\n",
        "    fig,ax=plt.subplots(1,2,figsize=(5*2,4)) \n",
        "    fig.tight_layout(pad=2)\n",
        "    fig.text(0,0,s=\"Epoch \"+str(enter_epoch))\n",
        "    im0=ax[0].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "    ax[0].title.set_text(\"Real Data\")\n",
        "    ax[0].set_xlim(-2,2)\n",
        "    ax[0].set_ylim(-4,4)\n",
        "    divider = make_axes_locatable(ax[0])\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "    Z_path = []\n",
        "    for j in range(len(enter_path)):\n",
        "      weight = effective_weights[j][enter_path[j]].data.numpy()\n",
        "      bias = effective_biases[j][enter_path[j]].data.numpy()\n",
        "      Z = ((test_data_curr@weight.T)+bias)>0\n",
        "      Z_path.append(Z)\n",
        "    Z_result = Z_path[0]\n",
        "    for i in range(1,len(Z_path)):\n",
        "      Z_result = Z_result & Z_path[i]\n",
        "\n",
        "    data_frac_by_path = np.count_nonzero(Z_result)/len(test_labels_curr)*100 #Stregth of the path defined as fraction of data points active by that path\n",
        "    Z_res_path=Z_result*test_labels_curr\n",
        "    Z_res_non_zero = Z_res_path[np.nonzero(Z_res_path)]\n",
        "    path_std=np.std(Z_res_non_zero)##Path purity defined as the path variance i.e., variance of the data points active for that path\n",
        "    path_mean=np.mean(Z_res_non_zero)\n",
        "\n",
        "    im1=ax[1].scatter(X, Y, c=Z_result,vmin=-1,vmax=1)\n",
        "    ax[1].text(0.5,.9,\"data_frac_by_path: \"+str(round(data_frac_by_path,2)),horizontalalignment='center', verticalalignment='center', transform=ax[1].transAxes)\n",
        "    ax[1].text(0.5,0.1,\"Path_mean: \"+str(round(path_mean,2))+\" ,std: \"+str(round(path_std,2)),horizontalalignment='center', verticalalignment='center', transform=ax[1].transAxes)\n",
        "\n",
        "    ax[1].title.set_text(str(enter_path)+\" Path visualization\")\n",
        "    ax[1].set_xlim(-2,2)\n",
        "    ax[1].set_ylim(-4,4)\n",
        "    divider = make_axes_locatable(ax[1])\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "    path0=\"/content/drive/MyDrive/Research/DLGN_fixed_node/\"\n",
        "    if save_img:\n",
        "      fig.savefig(str(path0)+\"Path_viz_Epoch_\"+str(enter_epoch)+\".pdf\", dpi=300,format=\"pdf\")\n",
        "\n",
        "\n",
        "def return_path_hyp(enter_path,enter_epoch):\n",
        "  if infer_model:\n",
        "    DLGN_obj = DLGN_obj_store[int(np.sqrt(enter_epoch))]\n",
        "    effective_weights, effective_biases = DLGN_obj.return_gating_functions()\n",
        "    Z_path = []\n",
        "    for j in range(len(enter_path)):\n",
        "      weight = effective_weights[j][enter_path[j]].data.numpy()\n",
        "      bias = effective_biases[j][enter_path[j]].data.numpy()\n",
        "      Z = ((test_data_curr@weight.T)+bias)>0\n",
        "      Z_path.append(Z)\n",
        "    Z_result = Z_path[0]\n",
        "    for i in range(1,len(Z_path)):\n",
        "      Z_result = Z_result & Z_path[i]\n",
        "    data_frac_by_path = np.count_nonzero(Z_result)/len(test_labels_curr)*100 #Stregth of the path defined as fraction of data points active by that path\n",
        "    Z_res_path=Z_result*test_labels_curr\n",
        "    Z_res_non_zero = Z_res_path[np.nonzero(Z_res_path)]\n",
        "    path_std=np.std(Z_res_non_zero)##Path purity defined as the path variance i.e., variance of the data points active for that path\n",
        "    path_mean=np.mean(Z_res_non_zero)\n",
        "    return Z_result,round(data_frac_by_path,2),round(path_mean,2),round(path_std,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwDDB5fCFXFQ"
      },
      "outputs": [],
      "source": [
        "# DLGN_obj = DLGN_obj_store[0]\n",
        "# DLGN_obj.value_layers[-1].weight\n",
        "# DLGN_obj.value_layers[-2].weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPwDvbUYFbfB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taWFUEd0G9p2"
      },
      "outputs": [],
      "source": [
        "#@title **Change of W_&_B for all node**\n",
        "if infer_model:\n",
        "  # layer_num = 3\n",
        "  # node_num = 2\n",
        "  if infer_dlgn_lr:\n",
        "    DLGN_obj_store=DLGN_LR_obj_store\n",
        "    print(\"Hello\")\n",
        "  no_rows=num_hidden_layers\n",
        "  no_colns = max_no_of_nodes\n",
        "  fig,ax=plt.subplots(no_rows,no_colns,figsize=(5*no_colns,4*no_rows)) \n",
        "  fig.tight_layout(pad=2)\n",
        "  fig.text(0,0,s=\"Change of weights\")\n",
        "  for layer_num in range(num_hidden_layers):\n",
        "    for node_num in range(num_hidden_nodes[layer_num]):\n",
        "      weight_0 = []\n",
        "      weight_1 = []\n",
        "      bias = []\n",
        "      epoch_count=range(len(DLGN_obj_store))\n",
        "      for epoch_no in range(len(DLGN_obj_store)):\n",
        "        effective_weights, effective_biases = DLGN_obj_store[epoch_no].return_gating_functions()\n",
        "        weight_0.append(effective_weights[layer_num][node_num].data.numpy()[0])\n",
        "        weight_1.append(effective_weights[layer_num][node_num].data.numpy()[1])\n",
        "        bias.append(effective_biases[layer_num][node_num].data.numpy())\n",
        "      im1=ax[layer_num,node_num].plot(epoch_count,weight_0,label=\"Weight_0\")\n",
        "      im2=ax[layer_num,node_num].plot(epoch_count,weight_1,label=\"Weight_1\")\n",
        "      im3=ax[layer_num,node_num].plot(epoch_count,bias,label=\"bias\")\n",
        "      ax[layer_num,node_num].title.set_text(\"Layer: \"+str(layer_num+1)+\" Node: \"+str(node_num+1))\n",
        "      ax[layer_num,node_num].legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_YkavzBLPWi"
      },
      "outputs": [],
      "source": [
        "#@title **Plot Path Value variation**\n",
        "def plot_path_value_variation(enter_path,plot_epoch,weight_model_type): #plot_epoch is till which epoch want to plot\n",
        "  path_value=[]\n",
        "  path_epoch_count = []\n",
        "  for i in range(plot_epoch):\n",
        "    if(perfectSq(i)):\n",
        "      path_epoch_count.append(i)\n",
        "      path_value.append(print_path_value(enter_path,i,weight_model_type))\n",
        "  plt.figure()\n",
        "  plt.plot(path_epoch_count,path_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IifSgQPxvkm4"
      },
      "outputs": [],
      "source": [
        "#@title **Sanity check of path decomposition**\n",
        "def check_sanity(weight_model_type,enter_epoch):\n",
        "  if infer_model:\n",
        "    X=test_data_curr[:,0]\n",
        "    Y=test_data_curr[:,1]\n",
        "    Z=np.zeros(X.shape)\n",
        "    for i in range(max_no_of_nodes):\n",
        "      for j in range(max_no_of_nodes):\n",
        "        for k in range(max_no_of_nodes):\n",
        "          enter_path=(i,j,k)\n",
        "          val=print_path_value(enter_path,enter_epoch,weight_model_type)\n",
        "          Z_result,data_frac_by_path,path_mean,path_std=return_path_hyp(enter_path,enter_epoch)\n",
        "          Z+=val*Z_result\n",
        "    plt.scatter(X,Y,c=Z,vmin=-1,vmax=1)\n",
        "    plt.colorbar()\n",
        "    plt.figure()\n",
        "    plt.scatter(X,Y,c=test_labels_curr,vmin=-1,vmax=1)\n",
        "    plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uVsQ5BeN5xW"
      },
      "outputs": [],
      "source": [
        "#@title **Array of dict storing path,path_val,data frac,mean,std**\n",
        "def path_array(plot_epoch,weight_model_type):\n",
        "  path_val_store = []\n",
        "  for i in range(max_no_of_nodes):\n",
        "    for j in range(max_no_of_nodes):\n",
        "      for k in range(max_no_of_nodes):\n",
        "        enter_path=(i,j,k)\n",
        "        path_value=[]\n",
        "        path_epoch_count = []\n",
        "        for pe in range(plot_epoch):\n",
        "          if(perfectSq(pe)):\n",
        "            path_epoch_count.append(pe)\n",
        "            path_value.append(round(print_path_value(enter_path,pe,weight_model_type),3))\n",
        "        Z_result,data_frac_by_path,path_mean,path_std=return_path_hyp(enter_path,5000)\n",
        "        Dict = {}\n",
        "        Dict['val']=(path_epoch_count,path_value)\n",
        "        Dict['path']=enter_path\n",
        "        Dict['data_frac_by_path']=data_frac_by_path\n",
        "        Dict['path_mean']=path_mean\n",
        "        Dict['path_std']=path_std\n",
        "        path_val_store.append(Dict)\n",
        "  return path_val_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDEvZPUjWmVC"
      },
      "outputs": [],
      "source": [
        "#@title **Printing vals for one path**\n",
        "def print_any_path(plot_epoch,weight_model_type,find_path=(0,0,0)):\n",
        "  path_val_store = path_array(plot_epoch,weight_model_type)\n",
        "  from operator import itemgetter\n",
        "  newlist = sorted(path_val_store, key=itemgetter('path')) \n",
        "  return newlist,newlist[find_path[0]*max_no_of_nodes**2+find_path[1]*max_no_of_nodes+find_path[2]]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVex8vM1YnCX"
      },
      "outputs": [],
      "source": [
        "#@title **5x25 path vals**\n",
        "def plot_all_paths(plot_epoch,weight_model_type):\n",
        "  newlist,_ = print_any_path(plot_epoch,weight_model_type)\n",
        "  for index in range(max_no_of_nodes**num_hidden_layers):\n",
        "    if(index%5==0):\n",
        "      plt.figure(figsize=(5,5))\n",
        "    plt.plot(newlist[index]['val'][0],newlist[index]['val'][1],label=\"p: \"+str(newlist[index]['path'])+\" f: \"+str(newlist[index]['data_frac_by_path'])+\" m: \"+str(newlist[index]['path_mean'])+\" s: \"+str(newlist[index]['path_std']))\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylim(-.3, .3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4THRcB_ibYkJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0aqVxE_L1RC"
      },
      "source": [
        "**Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjF8CRVPBHch"
      },
      "outputs": [],
      "source": [
        "if infer_model: #if we have similar kind of paths only one of them vary much rest don't vary that much (2,1/2,0/3) (1,4,0/1/3/4) seed 365\n",
        "  enter_path = (1,4,3) #(2,2,3)#(2,1,0)#(2,1,3)#(1,4,0)#(1,4,3)#(1,4,1)#(1,4,4) #(2,0,3) #(2,2,0)  \n",
        "  plot_epoch = 5000\n",
        "  enter_epoch = 5000\n",
        "  weight_model_type = \"DLGN\"\n",
        "  data_pred_scatter_plot(predictions_dlgn)\n",
        "  check_sanity(weight_model_type,enter_epoch)\n",
        "  _,path_val=print_any_path(plot_epoch,weight_model_type,find_path=enter_path)\n",
        "  plot_all_paths(plot_epoch,weight_model_type)\n",
        "  #**Path visualization of one path**\n",
        "  plot_path_value_variation(enter_path,plot_epoch,weight_model_type)\n",
        "  show_path_hyp(enter_path,enter_epoch)\n",
        "  #**All node visualization**\n",
        "  show_node_hyp(enter_epoch,weight_model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z6RXB3mBHSE"
      },
      "outputs": [],
      "source": [
        "if infer_model: #if we have similar kind of paths only one of them vary much rest don't vary that much (2,1/2,0/3) (1,4,0/1/3/4) #seed 365\n",
        "  enter_path = (1,4,3) #(2,2,3)#(2,1,0)#(2,1,3)#(1,4,0)#(1,4,3)#(1,4,1)#(1,4,4) #(2,0,3) #(2,2,0)  \n",
        "  plot_epoch = 5000\n",
        "  enter_epoch = 5000\n",
        "  weight_model_type = \"Path_LR\"\n",
        "  data_pred_scatter_plot(predictions_lr)\n",
        "  check_sanity(weight_model_type,enter_epoch)\n",
        "  _,path_val=print_any_path(plot_epoch,weight_model_type,find_path=enter_path)\n",
        "  plot_all_paths(plot_epoch,weight_model_type)\n",
        "  #**Path visualization of one path**\n",
        "  plot_path_value_variation(enter_path,plot_epoch,weight_model_type)\n",
        "  show_path_hyp(enter_path,enter_epoch)\n",
        "  #**All node visualization**\n",
        "  show_node_hyp(enter_epoch,weight_model_type)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if infer_model: #if we have similar kind of paths only one of them vary much rest don't vary that much (2,1/2,0/3) (1,4,0/1/3/4) seed 365\n",
        "  if infer_dlgn_lr:\n",
        "    enter_path = (1,4,3) #(2,2,3)#(2,1,0)#(2,1,3)#(1,4,0)#(1,4,3)#(1,4,1)#(1,4,4) #(2,0,3) #(2,2,0)  \n",
        "    plot_epoch = 5000\n",
        "    enter_epoch = 5000\n",
        "    weight_model_type = \"DLGN_LR\"\n",
        "    data_pred_scatter_plot(predictions_dlgn_lr)\n",
        "    check_sanity(weight_model_type,enter_epoch)\n",
        "    _,path_val=print_any_path(plot_epoch,weight_model_type,find_path=enter_path)\n",
        "    plot_all_paths(plot_epoch,weight_model_type)\n",
        "    #**Path visualization of one path**\n",
        "    plot_path_value_variation(enter_path,plot_epoch,weight_model_type)\n",
        "    show_path_hyp(enter_path,enter_epoch)\n",
        "    #**All node visualization**\n",
        "    show_node_hyp(enter_epoch,weight_model_type)\n",
        "    "
      ],
      "metadata": {
        "id": "ocDASUnYgcif"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_sanity(\"DLGN_LR\",5000)"
      ],
      "metadata": {
        "id": "nc4SuH0buVvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJh22cxD3agF"
      },
      "outputs": [],
      "source": [
        "if infer_model:\n",
        "  check_sanity(\"Path_LR\",5000)\n",
        "  if infer_dlgn_lr:\n",
        "    check_sanity(\"DLGN_LR\",5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hr5OW2CZGcp"
      },
      "outputs": [],
      "source": [
        "#@title **Losswise comparision of DLGN and LR models**\n",
        "def find_nearest_idx(array, value):\n",
        "  idx = (np.abs(array - value)).argmin()\n",
        "  return idx\n",
        "def loss_wise_comp():\n",
        "  X=test_data_curr[:,0]\n",
        "  Y=test_data_curr[:,1]\n",
        "  loss_check =[1,0.7,0.6,0.5,0.4,0.3,0.2,0.1,0.05,0.01,0.001,0]\n",
        "\n",
        "  test_data_torch_dlgn = torch.Tensor(test_data_curr)\n",
        "  test_data_torch_lr = torch.Tensor(path_fea_test) \n",
        "\n",
        "\n",
        "  no_rows=len(loss_check)+1\n",
        "  no_colns = 2\n",
        "  fig,ax=plt.subplots(no_rows,no_colns,figsize=(5*no_colns,4*no_rows)) \n",
        "  fig.tight_layout(pad=5)\n",
        "  fig.text(0,0,s=\"DLGN vs LR\")\n",
        "  im_hide = ax[0,0].axis('off')\n",
        "  \n",
        "  im0=ax[0,1].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "  ax[0,1].title.set_text(\"Real\")\n",
        "  ax[0,1].set_xlim(-2,2)\n",
        "  ax[0,1].set_ylim(-2,2)\n",
        "  divider = make_axes_locatable(ax[0,1])\n",
        "  cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "  fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "  \n",
        "\n",
        "  for index,loss_val in enumerate(loss_check):  \n",
        "    enter_epoch_dlgn = find_nearest_idx(losses,loss_val)\n",
        "    enter_epoch_lr = find_nearest_idx(losses_lr,loss_val) \n",
        "\n",
        "    dlgn_index=int(np.sqrt(enter_epoch_dlgn))\n",
        "    DLGN_obj = DLGN_obj_store[dlgn_index]\n",
        "    values,gate_scores =DLGN_obj(test_data_torch_dlgn)\n",
        "    path_values_dlgn = values[-1]\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds_dlgn = torch.cat((-1*path_values_dlgn,path_values_dlgn),dim=1)\n",
        "      predictions_dlgn = torch.argmax(test_preds_dlgn,dim=1).numpy()\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds_dlgn = path_values_dlgn\n",
        "      predictions_dlgn=test_preds_dlgn.detach().numpy()\n",
        "\n",
        "    lr_index=int(np.sqrt(enter_epoch_lr))\n",
        "    Path_LR_obj = Path_LR_obj_store[lr_index]\n",
        "    path_values_lr = Path_LR_obj(test_data_torch_lr)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds_lr = torch.cat((-1*path_values_lr,path_values_lr),dim=1)\n",
        "      predictions_lr = torch.argmax(test_preds_lr,dim=1).numpy()\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds_lr = path_values_lr\n",
        "      predictions_lr=test_preds_lr.detach().numpy()\n",
        "\n",
        "   \n",
        "      im=ax[index+1,0].scatter(X, Y, c=predictions_dlgn,vmin=-1,vmax=1)\n",
        "      ax[index+1,0].set_xlim(-2,2)\n",
        "      ax[index+1,0].set_ylim(-2,2)\n",
        "      # ax[index+1,0].title.set_text(\"Loss : \"+str(round(losses[enter_epoch_dlgn],4))+\" , \"+str(round(losses[dlgn_index*dlgn_index],4))+\" Epoch : \"+str(enter_epoch_dlgn)+\" , \"+str(dlgn_index*dlgn_index)+\" DLGN\")\n",
        "      ax[index+1,0].title.set_text(\"Loss : \"+str(round(losses[dlgn_index*dlgn_index],4))+\" Epoch : \"+str(dlgn_index*dlgn_index)+\" DLGN\")\n",
        "      # ax[index+1,0].legend()\n",
        "      divider = make_axes_locatable(ax[index+1,0])\n",
        "      cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "      fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "\n",
        "      im1=ax[index+1,1].scatter(X, Y, c=predictions_lr,vmin=-1,vmax=1)\n",
        "      ax[index+1,1].set_xlim(-2,2)\n",
        "      ax[index+1,1].set_ylim(-2,2)\n",
        "      # ax[index+1,1].title.set_text(\"Loss : \"+str(round(losses_lr[enter_epoch_lr],4))+\" , \"+str(round(losses_lr[lr_index*lr_index],4))+\" Epoch : \"+str(enter_epoch_lr)+\" , \"+str(lr_index*lr_index)+\" LR\")\n",
        "      ax[index+1,1].title.set_text(\"Loss : \"+str(round(losses_lr[lr_index*lr_index],4))+\" Epoch : \"+str(lr_index*lr_index)+\" LR\")\n",
        "      # ax[index+1,1].legend()\n",
        "      divider = make_axes_locatable(ax[index+1,1])\n",
        "      cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "      fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bQNXN6DhW_I"
      },
      "outputs": [],
      "source": [
        "if infer_model:\n",
        "  loss_wise_comp()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QArlUIYe9RxM"
      },
      "outputs": [],
      "source": [
        "#@title **Rough not needed**\n",
        "\n",
        "# epoch=len(DLGN_obj_store)\n",
        "# value = np.zeros((epoch,num_hidden_layers,max_no_of_nodes))\n",
        "\n",
        "# for index,model in enumerate(DLGN_obj_store):\n",
        "#   for i in range(len(model.gating_layers)-1):\n",
        "#     for j in range(len(model.gating_layers[i].weight)):\n",
        "#       value[index][i][j]=model.gating_layers[i].weight[j].data.detach().numpy().mean(axis=0)\n",
        "# epoch_count=range(epoch)\n",
        "# plt.figure(figsize=(10, 5))\n",
        "# for i in range(value[0].shape[0]):\n",
        "#   for j in range(value[0].shape[1]):\n",
        "#     plt.plot(epoch_count,value[:,i,j],label=str(i+1)+\"_\"+str(j+1))\n",
        "#     plt.legend()\n",
        "# DLGN_obj_store[0]\n",
        "# DLGN_obj_store[0](torch.Tensor([1.,0.0]))\n",
        "# for j in range(4):\n",
        "#   print(DLGN_obj_store[0].value_layers[j].weight)\n",
        "# DLGN_obj_store[-1](torch.Tensor([0,1]))\n",
        "# n = 100000\n",
        "# x0x1_min = [-1.0, -1.0]\n",
        "# x0x1_max = [1.0, 1.0]\n",
        "# data = np.random.uniform(low=x0x1_min, high=x0x1_max, size=(n,2))\n",
        "# y=[]\n",
        "# for i,val in enumerate(data):\n",
        "#   if(-1<=val[0]<=0 and 0<=val[1]<=1):\n",
        "#     y.append(1)\n",
        "#   elif (0<=val[0]<=1 and 0.5<=val[1]<=1):\n",
        "#     y.append(0.2)\n",
        "#   elif(0<=val[0]<=1 and -0.5<=val[1]<=0.5):\n",
        "#     y.append(0.5)\n",
        "#   else:\n",
        "#     y.append(-1)\n",
        "#   # elif (val[0] in [-1,0] and val[1] in [0,1]) or (val[0] in [0,1] and val[1] in [-0.5,-1]):\n",
        "# fig=plt.scatter(data[:,0], data[:,1],c=y,s=10)\n",
        "# # print(data)\n",
        "# def path_value_cal(DLGN_obj):\n",
        "#   complete_path_vals = np.zeros(tuple(num_hidden_nodes))\n",
        "#   # partial_path_vals_list=[]\n",
        "#   # for j in range(num_hidden_layers):\n",
        "#   #     partial_path_vals_list.append(np.zeros(tuple(num_hidden_nodes[j:])))\n",
        "\n",
        "      \n",
        "\n",
        "#   complete_paths = list(cartesian_prod(*[range(x) for x in num_hidden_nodes]))\n",
        "#   # partial_paths_list=[]\n",
        "\n",
        "#   # for j in range(num_hidden_layers):\n",
        "#   #     partial_paths_list.append(list(cartesian_prod(*[list(range(x)) for x in num_hidden_nodes[j:]])))\n",
        "\n",
        "      \n",
        "#   for path in complete_paths:\n",
        "#       temp = np.dot(DLGN_obj.value_layers[0].weight.data.numpy()[path[0],:], np.ones(input_dim))\n",
        "#       for k in range(1,num_hidden_layers):\n",
        "#           temp *= DLGN_obj.value_layers[k].weight.data.numpy()[path[k], path[k-1]]\n",
        "#       temp *= DLGN_obj.value_layers[num_hidden_layers].weight.data.numpy()[0, path[-1]]\n",
        "#       complete_path_vals[path]=temp\n",
        "          \n",
        "\n",
        "  # for j in range(num_hidden_layers):\n",
        "  #     for path in partial_paths_list[j]:\n",
        "  #         temp = DLGN_obj.value_layers[j].bias.data.numpy()[path[0]]\n",
        "  #         for k in range(j+1,num_hidden_layers):\n",
        "  # #             temp *= DLGN_obj.value_layers[k].weight.data.numpy()[path[k], path[k-1]]\n",
        "  #             temp *= DLGN_obj.value_layers[k].weight.data.numpy()[path[k-j], path[k-1-j]]            \n",
        "  #         temp *= DLGN_obj.value_layers[num_hidden_layers].weight.data.numpy()[0, path[-1]]        \n",
        "  #         partial_path_vals_list[j][path]=temp\n",
        "  # return complete_paths,complete_path_vals\n",
        "#return partial_paths_list,partial_path_vals_list,complete_paths,complete_path_vals\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],[4,5,6]])\n",
        "b = torch.tensor([[10,100,1000],[1,2,3]])\n",
        "\n",
        "# for i in range(len(a)):\n",
        "#   print(torch.outer(a[i,:],b[i,:]).ravel())\n",
        "(a[:,None,:]*b[:,:,None]).view(2,-1)"
      ],
      "metadata": {
        "id": "qTfZABIsXIwK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(a[:,:,None]*b[:,None,:]).view(2,-1)"
      ],
      "metadata": {
        "id": "Jt3eP9nufvQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = torch.tensor([[1,2,3],[4,5,6]])\n"
      ],
      "metadata": {
        "id": "yALES5l70oLy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMNWqXVXfU4w",
        "outputId": "812754ff-f465-4b1e-c982-e41f33b3eb56"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 3],\n",
              "        [4, 5, 6]])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "a[:,None,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GXTQZo_Ff0F4",
        "outputId": "51ca75bb-b267-4158-8131-82f92c9d62d2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[1, 2, 3]],\n",
              "\n",
              "        [[4, 5, 6]]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NFNDCIdMf6Ea"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
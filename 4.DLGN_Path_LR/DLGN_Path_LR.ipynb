{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjNxUX51Lh0k"
      },
      "outputs": [],
      "source": [
        "#@title **Imports**\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "import random\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
        "from itertools import product as cartesian_prod\n",
        "#save numpy array as npz file\n",
        "from numpy import asarray\n",
        "from numpy import savez_compressed\n",
        "# load numpy array from npz file\n",
        "from numpy import load\n",
        "from operator import itemgetter \n",
        "# from google.colab import drive #comment for running locally\n",
        "# drive.mount('/content/drive') #comment for running locally"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDx4xoSOFzR2"
      },
      "source": [
        "**Variable parameters**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KLhoBZzFxNb"
      },
      "outputs": [],
      "source": [
        "num_train_data=3000\n",
        "num_vali_data=3000\n",
        "num_test_data=3000\n",
        "dim=2 #no of dimension x=(x1,x2)\n",
        "num_modes=2*dim # used in the 4 mode classification problem\n",
        "\n",
        "model_type = \"Regression\" #\"Classification\" #for four mode classification data use Classification and for square decision tree data use Regression\n",
        "seed=1 #set the seed\n",
        "num_hidden_layers=3\n",
        "input_dim=dim\n",
        "output_dim=1\n",
        "num_hidden_nodes=[5,5,5] #hidden nodes for each layer\n",
        "beta=[20,10000] #When freeze is true then the layer corr to unfreezed node beta value is beta[0]=100 or 20, rest beta is beta[1]=10k, for freeze false every layer beta is beta[0].\n",
        "no_of_batches=1 #[1,10,100]\n",
        "modep='pwc' #piece wise constant means all 1's will be given as input to DLGN NPV.\n",
        "num_epoch=20000\n",
        "lr=0.0001\n",
        "weight_decay=0.0\n",
        "lr_path = 0.00005 #learning rate for the LR model\n",
        "Path_LR_hypPlane = False #True-- Calculate Path_LR model using hyper plane method, False--> calculating using sigmoid method\n",
        "\n",
        "\n",
        "test_on_train_data = False #Default False, set True to generate node hyperplanes images on training data\n",
        "\n",
        "freeze = False #True - when freeze weights of all nodes except one. False- when all nodes weights are varying\n",
        "layer_num = 3 #Layer number whose node is only varying and rest are freezed\n",
        "node_num = 3 #Node number of the above layer which is only varying rest are freeze\n",
        "\n",
        "\n",
        "NPF_freeze = False #Default False, set True - to freeze the whole NPF\n",
        "NPV_freeze = False #Default False, set True - to freeze the whole NPV\n",
        "NPF_pretrained_freezed = False #Default False, set True - to freeze the whole NPF after training it to max and NPV is trained from reinit\n",
        "NPV_pretrained_freezed = False #Default False, set True - to freeze the whole NPV after training it to max and NPF is trained from reinit\n",
        "\n",
        "show_all_epoch_node_hyperplane = False #Set to true if wants to show and store all node hyperplanes of all epochs\n",
        "best_epoch_show = False #Set to true if wants to show the node hyperplanes of the best epoch\n",
        "save_img = False\n",
        "\n",
        "\n",
        "absolute_path = os.path.abspath('')\n",
        "\n",
        "# relative_path = \"/drive/MyDrive/Research/Saved_models/\" #for running in colab\n",
        "relative_path = \"/Saved_models/\" #for local running\n",
        "\n",
        "# file_path = os.path.join(absolute_path, relative_path)\n",
        "file_path = absolute_path+relative_path\n",
        "\n",
        "# file_path = \"/content/drive/MyDrive/Research/Saved_models/\"\n",
        "\n",
        "enter_path = (0,0,0) #enter the path whose path_value is to be determined in list form as (1st_layer_node_num,2nd_layer_node_num,3rd_layer_node_num)\n",
        "enter_epoch = 0 #enter the epoch no (perfect sq number) whose node hyperplane is to be visualised\n",
        "plot_epoch = 5000 #epoch till which we want to plot\n",
        "max_no_of_nodes=max(num_hidden_nodes) #max of all the no of nodes in all layers\n",
        "\n",
        "# file_name_load_dlgnlr= \"dlgn_lr_model.npz\"\n",
        "\n",
        "######################################\n",
        "######### Training/Inference #########\n",
        "################ Flag ################\n",
        "######################################\n",
        "\n",
        "'''\n",
        "For training or inference or both set the flags of training/inference to True for DLGN model\n",
        "and any of the Path_LR or DLGN_LR model accordingly with wich model DLGN loss we want to compare.\n",
        "Mostly loss of DLGN no node fixed is compared with hybrid DLGN_LR loss and DLGN NPF_Fixed and NPF_pretrained_Fixed\n",
        "loss is compared with Path_LR counter part loss.\n",
        "'''\n",
        "## Set Training or inference or both\n",
        "#For the DLGN model\n",
        "train_model = False #Set to true if training a model and False if only infer a trained model\n",
        "infer_model = True #Set to true if infer a pretrained model or infer along with training\n",
        "\n",
        "#For the Path_LR model\n",
        "train_lr = False #Train the Path_LR model\n",
        "infer_path_lr = False #Infer the Path_LR model\n",
        "\n",
        "#For the DLGN_LR hybrid model\n",
        "train_dlgn_lr = False #Train the hybrid DLGN_LR model (DLGN_LR_FC)\n",
        "infer_dlgn_lr = True #Infer the hybrid DLGN_LR model (DLGN_LR_FC)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title **All Pretrained MODELS**\n",
        "\n",
        "\n",
        "##### Any of the below models (file_name_load) can be uncommented to infer the pretrained DLGN (NPF-NPV) model by setting infer_model = True and train_model = False\n",
        "\n",
        "#==========================================================================================================================================\n",
        "# file_name_load = file_path+\"R_NPF_pretrained_Fixed_1L_3L5N_Beta_20_seed_365.npz\" #change the model name to infer a pretrained model\n",
        "# file_name_load = file_path+\"R_NPF_Fixed_1L_3L5N_Beta_100_seed_0.npz\" \n",
        "# file_name_load = file_path+\"R_NPF_pretrained_Fixed_1L_3L5N_Beta_100_seed_0.npz\" \n",
        "# file_name_load = file_path+\"R_NPF_Fixed_0.5L_3L5N_Beta_1000_seed_0.npz\" \n",
        "# file_name_load = file_path+\"R_NPF_pretrained_Fixed_0.5L_3L5N_Beta_1000_seed_0.npz\"\n",
        "\n",
        "##### Any of the below models (file_name_load_lr) can be uncommented to infer the pretrained Path_LR (NPF-gates as feature to LR ) model by setting infer_path_lr = True and train_lr = False\n",
        "\n",
        "\n",
        "#===========================================================================================================================================\n",
        "# file_name_load_lr = file_path +\"Path_LR_model_NPF_pretrained.npz\"\n",
        "# file_name_load_lr = file_path +\"Path_LR_model_NPF_Fixed_Beta_100_sigmoid.npz\"\n",
        "# file_name_load_lr = file_path +\"Path_LR_model_NPF_pretrained_Beta_100_weinit_0_sigmoid.npz\" #change the model name to infer a pretrained Linear Regression model or to store the model with this name\n",
        "# file_name_load_lr = file_path +\"Path_LR_model_NPF_Fixed_0.5L_Beta_1000_lr_0.0001_sigmoid.npz\"\n",
        "\n",
        "\n",
        "'''\n",
        "## To infer the DLGN and Path_LR models both and to compare the loss of two pre-trained models(DLGN vs (Path_LR or DLGN_LR) \n",
        "## set train_model = False, infer_model = True and (train_lr = False, infer_path_lr = True or train_dlgn_lr = False,infer_dlgn_lr = True) and uncomment any of the below pairs(file_name_load for DLGN and (file_name_load_lr for Path_LR or file_name_load_dlgnlr)) with similar training grounds\n",
        "\n",
        "### To understand the models naming convention is as follows: \n",
        "### # file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L5N_Beta_1000_npf_seed_1_npv_seed_1.npz\"\n",
        "### R_ -> Regression square tree shaped data, NPF_Fixed/NPF_pretrained_Fixed/No_node -> npf model fixed at random or pretrained of not fixed while NPV is trained from init (same naming for NPV_Fixed/pretrained)\n",
        "### 0.2L -> no of epochs = 0.2Lakh=20k, 3L5N -> 3 Layer network with 5 nodes in each Layer, Beta_1000 -> beta value 1000, npf_seed 1 npv_seed 1\n",
        "'''\n",
        "#=================================================================================================================================\n",
        "\n",
        "'''\n",
        "#===========================================DLGN NPF Fixed========================================================================\n",
        "'''\n",
        "#=======================================5 Nodes=====================================================================================\n",
        "#=======================================NPF Seed 1=============================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L5N_Beta_1000_npf_seed_1_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L5N_Beta_1000_NPFseed_1_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L5N_Beta_1000_npf_seed_1_npv_seed_100.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L5N_Beta_1000_NPFseed_1_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L5N_Beta_1000_npf_seed_1_npv_seed_365.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L5N_Beta_1000_NPFseed_1_sigmoid.npz\"\n",
        "\n",
        "\n",
        "#=======================================NPF Seed 100=============================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L5N_Beta_1000_npf_seed_100_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L5N_Beta_1000_NPFseed_100_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L5N_Beta_1000_npf_seed_100_npv_seed_100.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L5N_Beta_1000_NPFseed_100_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L5N_Beta_1000_npf_seed_100_npv_seed_365.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L5N_Beta_1000_NPFseed_100_sigmoid.npz\"\n",
        "\n",
        "\n",
        "#=======================================NPF Seed 365=============================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L5N_Beta_1000_npf_seed_365_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L5N_Beta_1000_NPFseed_365_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L5N_Beta_1000_npf_seed_365_npv_seed_100.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L5N_Beta_1000_NPFseed_365_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L5N_Beta_1000_npf_seed_365_npv_seed_365.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L5N_Beta_1000_NPFseed_365_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "\n",
        "#===================================================================================================================\n",
        "#======================================2 Nodes===================================================================================\n",
        "#=======================================NPF Seed 1=============================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L2N_Beta_1000_npf_seed_1_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L2N_Beta_1000_NPFseed_1_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L2N_Beta_1000_npf_seed_1_npv_seed_100.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L2N_Beta_1000_NPFseed_1_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L2N_Beta_1000_npf_seed_1_npv_seed_365.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L2N_Beta_1000_NPFseed_1_sigmoid.npz\"\n",
        "\n",
        "#=======================================NPF Seed 100=============================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L2N_Beta_1000_npf_seed_100_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L2N_Beta_1000_NPFseed_100_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L2N_Beta_1000_npf_seed_100_npv_seed_100.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L2N_Beta_1000_NPFseed_100_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L2N_Beta_1000_npf_seed_100_npv_seed_365.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L2N_Beta_1000_NPFseed_100_sigmoid.npz\"\n",
        "\n",
        "#=======================================NPF Seed 365=============================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L2N_Beta_1000_npf_seed_365_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L2N_Beta_1000_NPFseed_365_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L2N_Beta_1000_npf_seed_365_npv_seed_100.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L2N_Beta_1000_NPFseed_365_sigmoid.npz\"\n",
        "##============================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L2N_Beta_1000_npf_seed_365_npv_seed_365.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L2N_Beta_1000_NPFseed_365_sigmoid.npz\"\n",
        "\n",
        "#===================================================================================================================\n",
        "#======================================20 Nodes===================================================================================\n",
        "#=======================================NPF Seed 1=============================================================================\n",
        "#===================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L20N_Beta_1000_npf_seed_1_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L20N_Beta_1000_NPFseed_1_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L20N_Beta_1000_npf_seed_1_npv_seed_100.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L20N_Beta_1000_NPFseed_1_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L20N_Beta_1000_npf_seed_1_npv_seed_365.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L20N_Beta_1000_NPFseed_1_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "\n",
        "#=======================================NPF Seed 100=============================================================================\n",
        "# file_name_load = file_path + \"R_NPF_Fixed_0.2L_3L20N_Beta_1000_npf_seed_100_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_Fixed_0.2L_3L20N_Beta_1000_NPFseed_100_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#================================DLGN NPF pretrained Fixed==========================================================\n",
        "'''\n",
        "#=====================================5 Nodes=====================================================================\n",
        "#====================================NPF seed 1=======================================================================\n",
        "# file_name_load = file_path + \"R_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_npf_seed_1_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_NPFseed_1_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_npf_seed_1_npv_seed_100.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_NPFseed_1_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_npf_seed_1_npv_seed_365.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_NPFseed_1_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "\n",
        "#====================================NPF seed 100===================================================================\n",
        "# file_name_load = file_path + \"R_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_npf_seed_100_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_NPFseed_100_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_npf_seed_100_npv_seed_100.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_NPFseed_100_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "# file_name_load = file_path + \"R_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_npf_seed_100_npv_seed_365.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_pretrained_Fixed_0.2L_3L5N_Beta_20_NPFseed_100_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "\n",
        "\n",
        "#=====================================2 Nodes=======================================================================\n",
        "#=====================================Seed 1========================================================================\n",
        "# file_name_load = file_path + \"R_NPF_pretrained_Fixed_0.2L_3L2N_Beta_20_npf_seed_1_npv_seed_1.npz\"\n",
        "# file_name_load_lr = file_path+\"Path_LR_model_NPF_pretrained_Fixed_0.2L_3L2N_Beta_20_NPFseed_1_sigmoid.npz\"\n",
        "#===================================================================================================================\n",
        "\n",
        "\n",
        "\n",
        "'''\n",
        "#==================================DLGN No Node Fixed ==============================================================\n",
        "'''\n",
        "\n",
        "#=====================================5 Nodes=======================================================================\n",
        "#=====================================Seed 1========================================================================\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L5N_Beta_20_npf_seed_1_npv_seed_1.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L5N_Beta_20_npf_seed_1.npz\"\n",
        "#==================================================================================================================\n",
        "\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L5N_Beta_20_npf_seed_1_npv_seed_100.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L5N_Beta_20_npf_seed_1.npz\"\n",
        "#==================================================================================================================\n",
        "\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L5N_Beta_20_npf_seed_1_npv_seed_365.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L5N_Beta_20_npf_seed_1.npz\"\n",
        "#=====================================================================================================================\n",
        "#======================================Seed 100=======================================================================\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L5N_Beta_20_npf_seed_100_npv_seed_1.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L5N_Beta_20_npf_seed_100.npz\"\n",
        "\n",
        "#==================================================================================================================\n",
        "file_name_load = file_path + \"R_No_node_0.2L_3L5N_Beta_20_npf_seed_100_npv_seed_100.npz\"\n",
        "file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L5N_Beta_20_npf_seed_100.npz\"\n",
        "\n",
        "#=======================================Seed 365=====================================================================\n",
        "#==================================================================================================================\n",
        "\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L5N_Beta_20_npf_seed_365_npv_seed_1.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L5N_Beta_20_npf_seed_365.npz\"\n",
        "#==================================================================================================================\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L5N_Beta_20_npf_seed_365_npv_seed_365.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L5N_Beta_20_npf_seed_365.npz\"\n",
        "\n",
        "\n",
        "#======================================2 Nodes========================================================================\n",
        "#======================================Seed 1=====================================================================\n",
        "\n",
        "#==================================================================================================================\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L2N_Beta_20_npf_seed_1_npv_seed_1.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L2N_Beta_20_npf_seed_1.npz\"\n",
        "#==================================================================================================================\n",
        "\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L2N_Beta_20_npf_seed_1_npv_seed_100.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L2N_Beta_20_npf_seed_1.npz\"\n",
        "#==================================================================================================================\n",
        "\n",
        "#===================================Seed 100=====================================================================\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L2N_Beta_20_npf_seed_100_npv_seed_1.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L2N_Beta_20_npf_seed_100.npz\"\n",
        "#==================================================================================================================\n",
        "\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L2N_Beta_20_npf_seed_100_npv_seed_100.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L2N_Beta_20_npf_seed_100.npz\"\n",
        "#==================================================================================================================\n",
        "#=====================================Seed 365=====================================================================\n",
        "\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L2N_Beta_20_npf_seed_365_npv_seed_1.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L2N_Beta_20_npf_seed_365.npz\"\n",
        "\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L2N_Beta_20_npf_seed_365_npv_seed_365.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L2N_Beta_20_npf_seed_365.npz\"\n",
        "\n",
        "#=========================================Node 20=============================================================\n",
        "# file_name_load = file_path + \"R_No_node_0.2L_3L20N_Beta_20_npf_seed_1_npv_seed_1.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.2L_3L20N_Beta_20_npf_seed_1.npz\"\n",
        "# ==================================================================================================================\n",
        "\n",
        "# file_name_load = file_path + \"R_No_node_0.1L_3L20N_Beta_20_npf_seed_100_npv_seed_1.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.1L_3L20N_Beta_20_npf_seed_100.npz\"\n",
        "#==================================================================================================================\n",
        "\n",
        "# file_name_load = file_path + \"R_No_node_0.1L_3L20N_Beta_20_npf_seed_365_npv_seed_1.npz\"\n",
        "# file_name_load_dlgnlr = file_path +\"DLGN_LR_No_node_0.1L_3L20N_Beta_20_npf_seed_365.npz\"\n"
      ],
      "metadata": {
        "id": "UQB9kims7lU8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iXNxcu4L6kT"
      },
      "outputs": [],
      "source": [
        "#@title **Synthetic data**\n",
        "def set_npseed(seed):\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "\n",
        "def set_torchseed(seed):\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  torch.backends.cudnn.deterministic = True\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  \n",
        "#Four mode classification data (synthetic dataset-1)\n",
        "def data_gen1(num_train_data,num_vali_data,num_test_data,dim=2,seed=0):\n",
        "  set_npseed(seed = seed)\n",
        "  d=dim\n",
        "  num_modes=2*d\n",
        "  centers = np.concatenate((np.eye(d), -1*np.eye(d)), axis=0)\n",
        "  np.random.shuffle(centers)\n",
        "  hard=num_modes//2 #half are hard modes means having multi label data, easy modes have only one label\n",
        "\n",
        "  a=[4]* hard + [1]*(num_modes - hard)\n",
        "  mode_frac = np.array([1./num_modes]*num_modes)\n",
        "  b=1.\n",
        "  num_train_data = num_train_data \n",
        "  num_vali_data=num_vali_data\n",
        "  num_test_data=num_test_data\n",
        "  num_data = num_train_data + num_vali_data + num_test_data\n",
        "\n",
        "  num_data_per_mode = np.int32(num_data*mode_frac)\n",
        "  num_data_per_mode = np.concatenate((num_data_per_mode,[np.sum(num_data_per_mode)]))\n",
        "\n",
        "  landmarks=[-1]*num_modes\n",
        "  labels=[-1]*num_modes\n",
        "  for i in range(num_modes):\n",
        "    landmarks[i] = 0.05*np.random.randn(a[i],d)\n",
        "    landmarks[i]+=centers[i]\n",
        "    labels[i] = (i - np.arange(len(landmarks[i])))%2\n",
        "      \n",
        "  data=[-1]*num_modes\n",
        "  data_labels=[-1]*num_modes\n",
        "\n",
        "  train_data=[-1]*num_modes\n",
        "  train_data_labels=[-1]*num_modes\n",
        "\n",
        "  test_data=[-1]*num_modes\n",
        "  test_data_labels=[-1]*num_modes\n",
        "\n",
        "  vali_data=[-1]*num_modes\n",
        "  vali_data_labels=[-1]*num_modes\n",
        "\n",
        "\n",
        "  modes_data=[]\n",
        "\n",
        "  for i in range(num_modes):\n",
        "    data[i] = 0.1*np.random.randn(num_data_per_mode[i],d)\n",
        "    data[i] += centers[i]\n",
        "    data_labels[i] = np.zeros(num_data_per_mode[i])\n",
        "    for j in range(len(data_labels[i])):\n",
        "      dists = pairwise_distances(data[i][j:j+1,:],landmarks[i])                                   \n",
        "      j_star = np.argmin(dists[0])\n",
        "      data_labels[i][j]=labels[i][j_star]\n",
        "      \n",
        "      train_data[i] = np.array(data[i][:int(mode_frac[i]*num_train_data)])\n",
        "      train_data_labels[i] = np.array(data_labels[i][:int(mode_frac[i]*num_train_data)])\n",
        "\n",
        "      vali_data[i] = np.array(data[i][int(mode_frac[i]*num_train_data): \\\n",
        "                                      int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "      vali_data_labels[i] = np.array(data_labels[i][int(mode_frac[i]*num_train_data): \\\n",
        "                                      int(mode_frac[i]*num_train_data)+int(mode_frac[i]*num_vali_data)])\n",
        "\n",
        "      test_data[i] = np.array(data[i][-int(mode_frac[i]*num_test_data):])\n",
        "      test_data_labels[i] = np.array(data_labels[i][-int(mode_frac[i]*num_test_data):])\n",
        "      \n",
        "  return data,data_labels,train_data,train_data_labels,test_data,test_data_labels,vali_data,vali_data_labels,landmarks,labels,centers,mode_frac\n",
        "\n",
        "def uniform_data_gen(num_data,dim=2): #used by data_gen2 function for uniform data generations\n",
        "  n=num_data\n",
        "  d=dim\n",
        "  x0x1_min = [-1.0, -1.0]\n",
        "  x0x1_max = [1.0, 1.0]\n",
        "  data = np.random.uniform(low=x0x1_min, high=x0x1_max, size=(n,d))\n",
        "  y=np.zeros(n,dtype=np.float32)\n",
        "  for i,val in enumerate(data):\n",
        "    if(-1<=val[0]<=0 and 0<=val[1]<=1):\n",
        "      y[i]=1\n",
        "    elif (0<=val[0]<=1 and 0.5<=val[1]<=1):\n",
        "      y[i]=0.2\n",
        "    elif(0<=val[0]<=1 and -0.5<=val[1]<=0.5):\n",
        "      y[i]=0.5\n",
        "    else:\n",
        "      y[i]=-1\n",
        "  return data,y\n",
        "\n",
        "#Square decision tree data\n",
        "def data_gen2(num_train_data,num_vali_data,num_test_data,dim=2,seed=0): #using the above uniform_data_gen junction to generate train,test,valid data seperately\n",
        "  set_npseed(seed = seed)\n",
        "  train_data,train_data_labels=uniform_data_gen(num_train_data,dim=dim)\n",
        "  vali_data,vali_data_labels=uniform_data_gen(num_vali_data,dim=dim)\n",
        "  test_data,test_data_labels=uniform_data_gen(num_test_data,dim=dim)\n",
        "  data = np.concatenate((train_data,vali_data),axis=0)\n",
        "  data = np.concatenate((data,test_data),axis=0)\n",
        "  data_labels = np.concatenate((train_data_labels,vali_data_labels),axis=0)\n",
        "  data_labels = np.concatenate((data_labels,test_data_labels),axis=0)\n",
        "  return data,data_labels,train_data,train_data_labels,test_data,test_data_labels,vali_data,vali_data_labels,None,None,None,None #last 4 entries are kept None to match the data_gen1 return format"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xtzRiptOL_Ug"
      },
      "outputs": [],
      "source": [
        "#@title **DLGN_FC**\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "class DLGN_FC(nn.Module):\n",
        "  def __init__(self, to_copy=None,NPF_pretrained=True,NPV_pretrained=True, num_hidden_layers=0, input_dim=2, output_dim=1, num_hidden_nodes=[], beta=20, mode='pwc'):\n",
        "    super(DLGN_FC, self).__init__()\n",
        "    if to_copy==None:\n",
        "      self.gating_layers=[]\n",
        "      self.value_layers=[]\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "      self.beta=beta  # Soft gating parameter\n",
        "      self.mode = mode\n",
        "      self.num_nodes=[input_dim]+num_hidden_nodes+[output_dim]\n",
        "      # set_torchseed(365)\n",
        "      for i in range(num_hidden_layers+1):\n",
        "        self.gating_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1]))\n",
        "      # set_torchseed(1)\n",
        "      for j in range(num_hidden_layers+1):\n",
        "        self.value_layers.append(nn.Linear(self.num_nodes[j], self.num_nodes[j+1], bias=False))\n",
        "    else: #to_copy is used for using parameters from other DLGN epoch (basically to stored DLGN models)\n",
        "      self.gating_layers=[]\n",
        "      self.value_layers=[]\n",
        "      self.num_hidden_layers = to_copy.num_hidden_layers\n",
        "      self.beta=to_copy.beta  # Soft gating parameter\n",
        "      self.mode = to_copy.mode\n",
        "      self.num_nodes=list(to_copy.num_nodes)\n",
        "      for i in range(self.num_hidden_layers+1):\n",
        "        self.gating_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1]))\n",
        "        self.value_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1], bias=False))\n",
        "        if NPF_pretrained: #by default True, False-- don't store the gating layer parameters, set to reinit\n",
        "          self.gating_layers[i].weight.data =  torch.Tensor(np.array(to_copy.gating_layers[i].weight.detach().numpy()))\n",
        "          self.gating_layers[i].bias.data = torch.Tensor(np.array(to_copy.gating_layers[i].bias.detach().numpy()))\n",
        "        if NPV_pretrained: #by default True, False-- don't store the value layer parameters, set to reinit\n",
        "          self.value_layers[i].weight.data = torch.Tensor(np.array(to_copy.value_layers[i].weight.detach().numpy()))\n",
        "\n",
        "                \n",
        "  def return_gating_functions(self): #helps in returning the effective_weights, effective_biases for each node of each layer\n",
        "    effective_weights = []\n",
        "    effective_biases =[]\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      curr_weight = self.gating_layers[i].weight\n",
        "      curr_bias = self.gating_layers[i].bias\n",
        "      if i==0:\n",
        "        effective_weights.append(curr_weight)\n",
        "        effective_biases.append(curr_bias)\n",
        "      else:\n",
        "        effective_biases.append(torch.matmul(curr_weight,effective_biases[-1])+curr_bias) #since the gating network is linear effective w for layer 2 will be prod of w and b of layer 1 and 2\n",
        "        effective_weights.append(torch.matmul(curr_weight,effective_weights[-1]))\n",
        "    return effective_weights, effective_biases\n",
        "    # effective_weights (and effective biases) is a list of size num_hidden_layers\n",
        "              \n",
        "\n",
        "  def forward(self, x):\n",
        "    gate_scores=[x]\n",
        "    if self.mode=='pwc':\n",
        "      values=[torch.ones(x.shape)]\n",
        "    else:\n",
        "      values=[x]\n",
        "    \n",
        "    for i in range(self.num_hidden_layers):\n",
        "      beta_used = beta[0]\n",
        "      if ((i!=layer_num-1) and freeze): #if freeze is True beta_used is beta[1]\n",
        "        beta_used = beta[1]\n",
        "      gate_scores.append(self.gating_layers[i](gate_scores[-1]))\n",
        "      curr_gate_on_off = torch.sigmoid(beta_used*gate_scores[-1]) #calculating the gate musking used for value layer\n",
        "      values.append(self.value_layers[i](values[-1])*curr_gate_on_off)\n",
        "    values.append(self.value_layers[self.num_hidden_layers](values[-1]))\n",
        "    # Values is a list of size 1+num_hidden_layers+1\n",
        "    #gate_scores is a list of size 1+num_hidden_layers\n",
        "    return values,gate_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oeCcSZfqdJHQ"
      },
      "outputs": [],
      "source": [
        "class DLGN_LR_FC(nn.Module): #Hybrid model with Gating network same as NPF whose gates are used in LR network instead of NPV to find 5x5x5 paths, to apply linear regression on them\n",
        "  def __init__(self, to_copy=None,num_path_features=125, num_hidden_layers=0, input_dim=2, output_dim=1, num_hidden_nodes=[], beta=20):\n",
        "    super(DLGN_LR_FC, self).__init__()\n",
        "    if to_copy==None:\n",
        "      self.gating_layers=[]\n",
        "      self.path_layer=[] #Path n/w or LR n/w takes in 5x5x5 paths (on-off status) as input features to a LinearReg model and predicts the output\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "      self.beta=beta  # Soft gating parameter\n",
        "      self.num_nodes=[input_dim]+num_hidden_nodes+[output_dim]\n",
        "      # set_torchseed(365)\n",
        "      for i in range(num_hidden_layers+1):\n",
        "        self.gating_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1]))\n",
        "\n",
        "      self.path_layer.append(nn.Linear(num_path_features, output_dim, bias=False))\n",
        "      self.path_layer[0].weight.data.fill_(0.0) #initializing weights of the path layer to 0\n",
        "\n",
        "    else:\n",
        "\n",
        "      self.gating_layers=[]\n",
        "      self.num_hidden_layers = to_copy.num_hidden_layers\n",
        "      self.beta=to_copy.beta  # Soft gating parameter\n",
        "      self.num_nodes=list(to_copy.num_nodes)\n",
        "      for i in range(self.num_hidden_layers+1):\n",
        "        self.gating_layers.append(nn.Linear(self.num_nodes[i], self.num_nodes[i+1]))\n",
        "        self.gating_layers[i].weight.data =  torch.Tensor(np.array(to_copy.gating_layers[i].weight.detach().numpy()))\n",
        "        self.gating_layers[i].bias.data = torch.Tensor(np.array(to_copy.gating_layers[i].bias.detach().numpy()))\n",
        "      self.path_layer=[]\n",
        "      self.path_layer.append(nn.Linear(num_path_features, output_dim, bias=False))\n",
        "      self.path_layer[0].weight.data = torch.Tensor(np.array(to_copy.path_layer[0].weight.detach().numpy()))\n",
        "                \n",
        "\n",
        "  def return_gating_functions(self):\n",
        "    effective_weights = []\n",
        "    effective_biases =[]\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      curr_weight = self.gating_layers[i].weight\n",
        "      curr_bias = self.gating_layers[i].bias\n",
        "      if i==0:\n",
        "        effective_weights.append(curr_weight)\n",
        "        effective_biases.append(curr_bias)\n",
        "      else:\n",
        "        effective_biases.append(torch.matmul(curr_weight,effective_biases[-1])+curr_bias)\n",
        "        effective_weights.append(torch.matmul(curr_weight,effective_weights[-1]))\n",
        "    return effective_weights, effective_biases\n",
        "    # effective_weights (and effective biases) is a list of size num_hidden_layers\n",
        "              \n",
        "  def forward(self, x):\n",
        "    gate_scores=[x]\n",
        "    values=[]\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      beta_used = beta[0]\n",
        "      gate_scores.append(self.gating_layers[i](gate_scores[-1]))\n",
        "      curr_gate_on_off = torch.sigmoid(beta_used*gate_scores[-1]) #gate values from gating layers\n",
        "      values.append(curr_gate_on_off) #values[0], values[1],values[2] --> 3000x5 (num_data x num_nodes_per_layer) gating_on_off values\n",
        "    values = torch.stack(values,dim=1) #(3000,3,5), values[0] --> values[:,0,:] (dim 3000x5), values[1]-->values[:,1,:] and values[2]-->values[:,2,:]\n",
        "\n",
        "    #a=values[:,0,:].unsqueeze(dim=2) --> (3000x5x1) and b=values[:,1,:].unsqueeze(dim=1) --> (3000x1x5)\n",
        "    #a*b --> (3000x5x5), a*b.view(3000,-1)--> (3000,25)\n",
        "    '''\n",
        "    a = torch.tensor([[1,2,3],[4,5,6]])\n",
        "    b = torch.tensor([[10,100,1000],[1,2,3]])\n",
        "    (a.unsqueeze(dim=2)*b.unsqueeze(dim=1))\n",
        "    tensor([[[  10,  100, 1000],\n",
        "         [  20,  200, 2000],\n",
        "         [  30,  300, 3000]],\n",
        "\n",
        "        [[   4,    8,   12],\n",
        "         [   5,   10,   15],\n",
        "         [   6,   12,   18]]])\n",
        "\n",
        "    (a.unsqueeze(dim=2)*b.unsqueeze(dim=1)).view(2,-1)\n",
        "    tensor([[  10,  100, 1000,   20,  200, 2000,   30,  300, 3000],\n",
        "        [   4,    8,   12,    5,   10,   15,    6,   12,   18]])\n",
        "    '''\n",
        "\n",
        "    values1 = (values[:,0,:].unsqueeze(dim=2)*values[:,1,:].unsqueeze(dim=1)).view(values.shape[0],-1) #(3000,25) \n",
        "    values2 = (values1.unsqueeze(dim=2)*values[:,2,:].unsqueeze(dim=1)).view(values.shape[0],-1) #(3000,125)\n",
        "\n",
        "    path_values=self.path_layer[0](values2) #(3000x125)-->(3000x1)\n",
        "\n",
        "    return path_values,gate_scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xw-19NMnNfFv"
      },
      "outputs": [],
      "source": [
        "#@title **Path_LR**\n",
        "class Path_LR(nn.Module): #Linear Regression with 5x5x5 path on_off status as input and finding the weights of the paths\n",
        "  def __init__(self, to_copy=None,num_path_features=125, output_dim=1):\n",
        "    super(Path_LR, self).__init__()\n",
        "    if to_copy==None:\n",
        "      self.path_layer=[]\n",
        "      self.path_layer.append(nn.Linear(num_path_features, output_dim, bias=False))\n",
        "      self.path_layer[0].weight.data.fill_(0.0)\n",
        "    else:\n",
        "      self.path_layer=[]\n",
        "      self.path_layer.append(nn.Linear(num_path_features, output_dim, bias=False))\n",
        "      self.path_layer[0].weight.data = torch.Tensor(np.array(to_copy.path_layer[0].weight.detach().numpy()))\n",
        "\n",
        "  def forward(self, x):\n",
        "    path_values=self.path_layer[0](x)\n",
        "    return path_values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGAfARdaUD3r"
      },
      "outputs": [],
      "source": [
        "#@title **Check perfect sq**\n",
        "def perfectSq(N) :\n",
        "\tsq_root = round(N**(1/2));\n",
        "\tif sq_root * sq_root == N :\n",
        "\t\treturn True;\n",
        "\telse :\n",
        "\t\treturn False;"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ncr5k6koMbD_"
      },
      "outputs": [],
      "source": [
        "#@title **Train DLGN model**\n",
        "if train_model:\n",
        "  def train_dlgn(seed,num_hidden_layers,input_dim,output_dim,num_hidden_nodes,beta,no_of_batches,\n",
        "                mode,layer_num,node_num,train_data_curr,vali_data_curr,test_data_curr,train_labels_curr,test_labels_curr,vali_labels_curr,num_epoch,freeze,DLGN_obj_pretrained=None,NPF_pretrained = True,NPV_pretrained = True):\n",
        "    \n",
        "    layer_num=layer_num-1 #layer no of node to unfreeze\n",
        "    node_num=node_num-1 #node no of above layer to unfreeze when freeze is True\n",
        "\n",
        "    set_torchseed(seed)\n",
        "\n",
        "    if freeze:\n",
        "      DLGN_obj_initial = None\n",
        "    DLGN_obj = None\n",
        "    DLGN_obj_return = None\n",
        "    if DLGN_obj_pretrained == None:\n",
        "      DLGN_obj = DLGN_FC(num_hidden_layers=num_hidden_layers, input_dim=input_dim, output_dim=output_dim, \n",
        "                        num_hidden_nodes=num_hidden_nodes, beta=beta, mode=mode)\n",
        "    else: # using DLGN_obj_pretrained as already pre trained model received as argument\n",
        "      DLGN_obj = DLGN_FC(to_copy=DLGN_obj_pretrained,NPF_pretrained=NPF_pretrained,NPV_pretrained=NPV_pretrained)\n",
        "\n",
        "    if freeze:\n",
        "      DLGN_obj_initial = DLGN_FC(to_copy=DLGN_obj)\n",
        "    \n",
        "    DLGN_obj_return = DLGN_FC(to_copy=DLGN_obj)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "    if model_type == \"Regression\":\n",
        "      criterion = nn.MSELoss()\n",
        "    \n",
        "    \n",
        "\n",
        "    DLGN_params = []\n",
        "    DLGN_params += [item.weight for item in DLGN_obj.gating_layers]\n",
        "    DLGN_params += [item.bias for item in DLGN_obj.gating_layers]\n",
        "    DLGN_params += [item.weight for item in DLGN_obj.value_layers]\n",
        "\n",
        "    print(DLGN_params)\n",
        "  #     DLGN_params += [item.bias for item in DLGN_obj.value_layers]\n",
        "\n",
        "    if NPF_freeze: #set requires_grad=False for NPF params\n",
        "      for index,param in enumerate(DLGN_params):\n",
        "        if(index<2*len(DLGN_obj.gating_layers)):\n",
        "          param.requires_grad=False\n",
        "    if NPV_freeze: #set requires_grad=False for NPV params\n",
        "      for index,param in enumerate(DLGN_params):\n",
        "        if(index>=2*len(DLGN_obj.gating_layers)):\n",
        "          param.requires_grad=False\n",
        "    if freeze: #set requires_grad=True for only that layer containing the node rest all grads are False\n",
        "      for index,param in enumerate(DLGN_params):\n",
        "        if((index != layer_num)):\n",
        "          param.requires_grad=False\n",
        "        if((index==len(DLGN_obj.gating_layers)+layer_num)):\n",
        "          param.requires_grad=True\n",
        "\n",
        "\n",
        "    optimizer = optim.Adam(DLGN_params, lr=lr, weight_decay=weight_decay)\n",
        "    \n",
        "    \n",
        "    train_data_torch = torch.Tensor(train_data_curr)\n",
        "    vali_data_torch = torch.Tensor(vali_data_curr)\n",
        "    test_data_torch = torch.Tensor(test_data_curr)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr, dtype=torch.int64)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr, dtype=torch.int64)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr, dtype=torch.int64)\n",
        "    if model_type == \"Regression\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr)\n",
        "\n",
        "    num_batches = no_of_batches\n",
        "    batch_size = len(train_data_curr)//num_batches\n",
        "    losses=[]\n",
        "    DLGN_obj_store = []\n",
        "    best_vali_error = len(vali_labels_curr)\n",
        "\n",
        "\n",
        "    for epoch in tqdm(range(num_epoch)):  # loop over the dataset multiple times\n",
        "      running_loss = 0.0\n",
        "      if perfectSq(epoch): #storing DLGN params for all squared no epochs\n",
        "        DLGN_obj_store.append(DLGN_FC(to_copy=DLGN_obj))\n",
        "      \n",
        "      for batch_start in range(0,len(train_data_curr),batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        values,gate_scores = DLGN_obj(train_data_torch[batch_start:batch_start+batch_size])\n",
        "        \n",
        "        if model_type == \"Classification\":\n",
        "          outputs = torch.cat((-1*values[-1], values[-1]), dim=1)\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size])\n",
        "        if model_type == \"Regression\":\n",
        "          outputs = values[-1]\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size].reshape(outputs.shape))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if freeze:\n",
        "          for node in range(num_hidden_nodes[layer_num]):\n",
        "            if(node!=node_num): #setting all node weights to initial node weights except the node to unfreeze thus freezing all nodes except one\n",
        "              DLGN_params[layer_num].data[node]= DLGN_obj_initial.gating_layers[layer_num].weight.data[node]\n",
        "              DLGN_params[len(DLGN_obj.gating_layers)+layer_num].data[node]= DLGN_obj_initial.gating_layers[layer_num].bias.data[node]\n",
        "\n",
        "\n",
        "        running_loss += loss.item()    \n",
        "      losses.append(running_loss/num_batches)\n",
        "\n",
        "      #validation data\n",
        "      values,gate_scores =DLGN_obj(vali_data_torch)\n",
        "      if model_type == \"Classification\":\n",
        "        vali_preds = torch.cat((-1*values[-1], values[-1]), dim=1)\n",
        "        vali_preds = torch.argmax(vali_preds, dim=1).numpy()\n",
        "        vali_error= np.sum(vali_labels_torch.numpy()!=vali_preds)\n",
        "      if model_type == \"Regression\":\n",
        "        vali_preds = values[-1]\n",
        "        vali_error = criterion(vali_preds, vali_labels_torch.reshape(vali_preds.shape)).item()\n",
        "        vali_preds = vali_preds.detach().numpy()\n",
        "      if vali_error < best_vali_error:\n",
        "        DLGN_obj_return = DLGN_FC(to_copy=DLGN_obj)\n",
        "        best_vali_error = vali_error\n",
        "    \n",
        "    #test data\n",
        "    values,gate_scores =DLGN_obj_return(test_data_torch)\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds = torch.cat((-1*values[-1], values[-1]), dim=1)\n",
        "      test_preds = torch.argmax(test_preds, dim=1).numpy()\n",
        "      test_error= np.sum(test_labels_torch.numpy()!=test_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds = values[-1]\n",
        "      test_error = criterion(test_preds, test_labels_torch.reshape(test_preds.shape)).item()\n",
        "      test_preds = test_preds.detach().numpy()\n",
        "\n",
        "    #train data\n",
        "    values,gate_scores=DLGN_obj_return(train_data_torch)\n",
        "    if model_type == \"Classification\":\n",
        "      train_preds = torch.cat((-1*values[-1], values[-1]), dim=1)\n",
        "      train_preds = torch.argmax(train_preds, dim=1).numpy()\n",
        "      train_error= np.sum(train_labels_torch.numpy()!=train_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      train_preds = values[-1]\n",
        "      train_error = criterion(train_preds, train_labels_torch.reshape(train_preds.shape)).item()\n",
        "      train_preds = train_preds.detach().numpy()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"DLGN loss vs epoch\")\n",
        "    plt.plot(losses)\n",
        "    return losses,test_error, train_error, test_preds, DLGN_obj_return, DLGN_obj_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i30BvmvApRNo"
      },
      "outputs": [],
      "source": [
        "#@title **Train DLGN_LR model**\n",
        "#Training the hybrid DLGN_LR_FC model\n",
        "if train_dlgn_lr:\n",
        "  def train_dlgn_lr(seed,num_path_features,num_hidden_layers,input_dim,output_dim,num_hidden_nodes):\n",
        "    set_torchseed(seed)\n",
        "    DLGN_LR_obj = None\n",
        "    DLGN_LR_return = None \n",
        "    DLGN_LR_obj = DLGN_LR_FC(num_path_features=num_path_features,num_hidden_layers=num_hidden_layers, input_dim=input_dim, output_dim=output_dim, \n",
        "                        num_hidden_nodes=num_hidden_nodes,beta=beta)\n",
        "    \n",
        "\n",
        "    DLGN_LR_return = DLGN_LR_FC(to_copy=DLGN_LR_obj)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "    if model_type == \"Regression\":\n",
        "      criterion = nn.MSELoss()\n",
        "\n",
        "\n",
        "    DLGN_LR_params = []\n",
        "    DLGN_LR_params += [item.weight for item in DLGN_LR_obj.gating_layers]\n",
        "    DLGN_LR_params += [item.bias for item in DLGN_LR_obj.gating_layers]\n",
        "    DLGN_LR_params += [item.weight for item in DLGN_LR_obj.path_layer]\n",
        "\n",
        "    print(DLGN_LR_params)\n",
        "\n",
        "    optimizer = optim.Adam(DLGN_LR_params, lr=lr_path, weight_decay=weight_decay)\n",
        "\n",
        "\n",
        "    train_data_torch = torch.Tensor(train_data_curr)\n",
        "    vali_data_torch = torch.Tensor(vali_data_curr)\n",
        "    test_data_torch = torch.Tensor(test_data_curr)\n",
        "\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr, dtype=torch.int64)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr, dtype=torch.int64)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr, dtype=torch.int64)\n",
        "    if model_type == \"Regression\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr)\n",
        "\n",
        "\n",
        "\n",
        "    num_batches = no_of_batches\n",
        "    batch_size = len(train_data_curr)//num_batches\n",
        "    losses=[]\n",
        "    DLGN_LR_obj_store = []\n",
        "    best_vali_error = len(vali_labels_curr)\n",
        "\n",
        "\n",
        "    for epoch in tqdm(range(num_epoch)):  # loop over the dataset multiple times\n",
        "      running_loss = 0.0\n",
        "      if perfectSq(epoch):\n",
        "        DLGN_LR_obj_store.append(DLGN_LR_FC(to_copy=DLGN_LR_obj))\n",
        "      \n",
        "      for batch_start in range(0,len(train_data_curr),batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        path_values,_ = DLGN_LR_obj(train_data_torch[batch_start:batch_start+batch_size])\n",
        "        if model_type == \"Classification\":\n",
        "          outputs = torch.cat((-1*path_values, path_values), dim=1)\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size])\n",
        "        if model_type == \"Regression\":\n",
        "          outputs = path_values\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size].reshape(outputs.shape))\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()    \n",
        "      losses.append(running_loss/num_batches)\n",
        "\n",
        "      path_values,_ =DLGN_LR_obj(vali_data_torch)\n",
        "      if model_type == \"Classification\":\n",
        "        vali_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "        vali_preds = torch.argmax(vali_preds, dim=1).numpy()\n",
        "        vali_error= np.sum(vali_labels_torch.numpy()!=vali_preds)\n",
        "      if model_type == \"Regression\":\n",
        "        vali_preds = path_values\n",
        "        vali_error = criterion(vali_preds, vali_labels_torch.reshape(vali_preds.shape)).item()\n",
        "        vali_preds = vali_preds.detach().numpy()\n",
        "      if vali_error < best_vali_error:\n",
        "        DLGN_LR_return = DLGN_LR_FC(to_copy=DLGN_LR_obj)\n",
        "        best_vali_error = vali_error\n",
        "      \n",
        "    path_values,_ =DLGN_LR_return(test_data_torch)\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "      test_preds = torch.argmax(test_preds, dim=1).numpy()\n",
        "      test_error= np.sum(test_labels_torch.numpy()!=test_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds = path_values\n",
        "      test_error = criterion(test_preds, test_labels_torch.reshape(test_preds.shape)).item()\n",
        "      test_preds = test_preds.detach().numpy()\n",
        "\n",
        "    path_values,_=DLGN_LR_return(train_data_torch)\n",
        "    if model_type == \"Classification\":\n",
        "      train_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "      train_preds = torch.argmax(train_preds, dim=1).numpy()\n",
        "      train_error= np.sum(train_labels_torch.numpy()!=train_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      train_preds = path_values\n",
        "      train_error = criterion(train_preds, train_labels_torch.reshape(train_preds.shape)).item()\n",
        "      train_preds = train_preds.detach().numpy()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"DLGN_LR loss vs epoch\")\n",
        "    plt.plot(losses)\n",
        "    return losses,test_error, train_error, test_preds, DLGN_LR_return, DLGN_LR_obj_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XL-qsOGL1lV1"
      },
      "outputs": [],
      "source": [
        "#@title **Generating data**\n",
        "if model_type == \"Classification\":\n",
        "  data_gen = data_gen1\n",
        "if model_type == \"Regression\":\n",
        "  data_gen = data_gen2\n",
        "  \n",
        "data,data_labels,train_data,train_data_labels,test_data,test_data_labels,vali_data,vali_data_labels, \\\n",
        "landmarks,labels,centers,mode_frac= data_gen(num_train_data,num_vali_data,num_test_data,dim=2,seed=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2I8md4woFVrE"
      },
      "outputs": [],
      "source": [
        "#@title **Plotting the data**\n",
        "if model_type == \"Classification\":\n",
        "  fig=plt.scatter(np.concatenate(data)[:,0], np.concatenate(data)[:,1], c=np.concatenate(data_labels), s=10)\n",
        "if model_type == \"Regression\":\n",
        "  fig=plt.scatter(data[:,0], data[:,1], c=data_labels, s=10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LF7PO4Lc2jzV"
      },
      "outputs": [],
      "source": [
        "#@title **Training a DLGN model**\n",
        "## Convention 0th layer is input layer, 1st layer is 1st hidden layer and last layer is output layer\n",
        "## so 3 hidden layers mean 1 input node + 3 hidden layers + 1 output layer\n",
        "\n",
        "if model_type == \"Classification\":\n",
        "  train_data_curr = np.concatenate(train_data[0:num_modes])\n",
        "  train_labels_curr = np.concatenate(train_data_labels[0:num_modes])\n",
        "  vali_data_curr = np.concatenate(vali_data[0:num_modes])\n",
        "  vali_labels_curr = np.concatenate(vali_data_labels[0:num_modes])\n",
        "  test_data_curr = np.concatenate(test_data[0:num_modes])\n",
        "  test_labels_curr = np.concatenate(test_data_labels[0:num_modes]) \n",
        "\n",
        "if model_type == \"Regression\":\n",
        "  train_data_curr = train_data\n",
        "  train_labels_curr = train_data_labels\n",
        "  vali_data_curr = vali_data\n",
        "  vali_labels_curr = vali_data_labels\n",
        "  test_data_curr = test_data\n",
        "  test_labels_curr = test_data_labels\n",
        "\n",
        "#training the dlgn model\n",
        "if train_model:\n",
        "  losses,test_error, train_error, predictions_dlgn, DLGN_obj_final, DLGN_obj_store = train_dlgn(seed,num_hidden_layers,input_dim,output_dim,num_hidden_nodes,beta,\n",
        "                no_of_batches,modep,layer_num,node_num,train_data_curr,vali_data_curr,test_data_curr,train_labels_curr,test_labels_curr,vali_labels_curr,num_epoch,freeze)   \n",
        "\n",
        "  #predictions_dlgn -- labels corresponding to the best validation model\n",
        "  #DLGN_obj_final -- model corresponding to epoch with the best validation acc\n",
        "  #DLGN_obj_store -- store models corresponding to epochs at regular intervals\n",
        "\n",
        "  if NPF_pretrained_freezed: #NPF is set to pretrained model weights and freezed, NPV is set to reinit, DLGN_obj_pretrained=DLGN_obj_store[-1] using the training model last epoch defined above\n",
        "    NPF_freeze = True\n",
        "    seed=1 #set seed for getting different npv init for pretrained fixed npf\n",
        "    losses,test_error, train_error, predictions_dlgn, DLGN_obj_final, DLGN_obj_store = train_dlgn(seed,num_hidden_layers,input_dim,output_dim,num_hidden_nodes,beta,\n",
        "                no_of_batches,modep,layer_num,node_num,train_data_curr,vali_data_curr,test_data_curr,train_labels_curr,test_labels_curr,vali_labels_curr,num_epoch,freeze,DLGN_obj_pretrained=DLGN_obj_store[-1],NPF_pretrained = True,NPV_pretrained = False)   \n",
        "    NPF_freeze = False #Again set it back to false\n",
        "\n",
        "  if NPV_pretrained_freezed:\n",
        "    NPV_freeze = True\n",
        "    losses,test_error, train_error, predictions_dlgn, DLGN_obj_final, DLGN_obj_store = train_dlgn(seed,num_hidden_layers,input_dim,output_dim,num_hidden_nodes,beta,\n",
        "                no_of_batches,modep,layer_num,node_num,train_data_curr,vali_data_curr,test_data_curr,train_labels_curr,test_labels_curr,vali_labels_curr,num_epoch,freeze,DLGN_obj_pretrained=DLGN_obj_store[-1],NPF_pretrained = False,NPV_pretrained = True)   \n",
        "    NPV_freeze = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toD7u30muO7l"
      },
      "outputs": [],
      "source": [
        "#Training DLGN_LR_FC\n",
        "if train_dlgn_lr:\n",
        "  num_path_features=np.prod(num_hidden_nodes) #5x5x5\n",
        "  losses_dlgnlr,test_error_dlgnlr, train_error_dlgnlr, predictions_dlgn_lr, DLGN_LR_obj_final, DLGN_LR_obj_store = train_dlgn_lr(seed,num_path_features,num_hidden_layers,input_dim,output_dim,num_hidden_nodes)   \n",
        "  savez_compressed(file_name_load_dlgnlr, losses_dlgnlr,test_error_dlgnlr, train_error_dlgnlr, predictions_dlgn_lr, DLGN_LR_obj_final, DLGN_LR_obj_store)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dE3jrTJ6VSDB"
      },
      "outputs": [],
      "source": [
        "#@title **Storing the trained output**\n",
        "#Naming of the DLGN model and storing it\n",
        "if train_model:\n",
        "  Beta_str = \"Beta_\"+str(beta[0])+\"_\"\n",
        "  if model_type == \"Regression\":\n",
        "    a=\"R_\"\n",
        "  if model_type == \"Classification\":\n",
        "    a=\"C_\"\n",
        "  if freeze:\n",
        "    prefix = \"Layer_\"+str(layer_num)+\"_node_\"+str(node_num)+\"_\"\n",
        "    Beta_str = \"Beta_\"+str(beta[0])+\"_\"+str(int(beta[1]/1000))+\"k_\"\n",
        "  elif NPF_freeze:\n",
        "    prefix = \"NPF_Fixed_\"\n",
        "  elif NPV_freeze:\n",
        "    prefix = \"NPV_Fixed_\"\n",
        "  elif NPF_pretrained_freezed:\n",
        "    prefix = \"NPF_pretrained_Fixed_\"\n",
        "  elif NPV_pretrained_freezed:\n",
        "    prefix = \"NPV_pretrained_Fixed_\"\n",
        "  else:\n",
        "    prefix = \"No_node_\"\n",
        "  epoch_str = str(num_epoch/100000)+\"L_\"\n",
        "  file_name = a+prefix+epoch_str+str(num_hidden_layers)+\"L\"+str(max_no_of_nodes)+\"N_\"+Beta_str+\"seed_\"+str(seed)\n",
        "  \n",
        "  # save to npy file\n",
        "  file_name=file_path+file_name\n",
        "  savez_compressed(file_name, losses,test_error, train_error, predictions_dlgn, DLGN_obj_final, DLGN_obj_store)\n",
        "  file_name_load = file_name+\".npz\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfyL6scfMPL-"
      },
      "source": [
        "**Infer the trained model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1hII-DPU_bp4"
      },
      "outputs": [],
      "source": [
        "#@title **Load model**\n",
        "if infer_model:\n",
        "  # load dict of arrays\n",
        "  dict_data = load(file_name_load,allow_pickle=True)\n",
        "  losses=dict_data['arr_0']\n",
        "  test_error=dict_data['arr_1'].item()\n",
        "  train_error=dict_data['arr_2'].item()\n",
        "  predictions_dlgn=dict_data['arr_3']\n",
        "  DLGN_obj_final=dict_data['arr_4'].item()\n",
        "  DLGN_obj_store=dict_data['arr_5']\n",
        "  plt.figure()\n",
        "  plt.title(\"DLGN loss vs epoch\")\n",
        "  fig=plt.plot(losses[:])\n",
        "  if model_type == \"Classification\":\n",
        "    print(\"test_error\",test_error/len(test_data_curr))\n",
        "    print(\"train_error\",train_error/len(train_data_curr))\n",
        "    print('DLGN acc=',np.sum(predictions_dlgn==test_labels_curr)/len(test_data_curr))\n",
        "  if model_type == \"Regression\":\n",
        "    print(\"test_error\",test_error)\n",
        "    print(\"train_error\",train_error)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eB1A72kjkp-"
      },
      "source": [
        "**Infer dlgn lr model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X0P5RsVAieYI"
      },
      "outputs": [],
      "source": [
        "if infer_dlgn_lr:\n",
        "  # load dict of arrays \n",
        "  dict_data_dlgnlr = load(file_name_load_dlgnlr,allow_pickle=True)\n",
        "  losses_dlgnlr=dict_data_dlgnlr['arr_0']\n",
        "  test_error_dlgnlr=dict_data_dlgnlr['arr_1'].item()\n",
        "  train_error_dlgnlr=dict_data_dlgnlr['arr_2'].item()\n",
        "  predictions_dlgn_lr=dict_data_dlgnlr['arr_3']\n",
        "  DLGN_LR_obj_final=dict_data_dlgnlr['arr_4'].item()\n",
        "  DLGN_LR_obj_store=dict_data_dlgnlr['arr_5']\n",
        "  plt.figure()\n",
        "  plt.title(\"DLGN LR loss vs epoch\")\n",
        "  fig=plt.plot(losses_dlgnlr[:])\n",
        "  if model_type == \"Classification\":\n",
        "    print(\"test_error\",test_error_dlgnlr/len(test_data_curr))\n",
        "    print(\"train_error\",train_error_dlgnlr/len(train_data_curr))\n",
        "    print('DLGN acc=',np.sum(predictions_dlgn_lr==test_labels_curr)/len(test_data_curr))\n",
        "  if model_type == \"Regression\":\n",
        "    print(\"test_error\",test_error_dlgnlr)\n",
        "    print(\"train_error\",train_error_dlgnlr)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjYaeRr4MtWT"
      },
      "outputs": [],
      "source": [
        "#@title **Path_LR data preparation sigmoid method**\n",
        "def Path_LR_dataPrep_sigmoid(data_curr):\n",
        "  DLGN_obj = DLGN_obj_store[-1]\n",
        "  effective_weights, effective_biases = DLGN_obj.return_gating_functions()\n",
        "\n",
        "  x=(data_curr@effective_weights[0].data.numpy().T)+effective_biases[0].data.numpy() #(3000x5)--> 1st layer node pre_activation\n",
        "  y=(data_curr@effective_weights[1].data.numpy().T)+effective_biases[1].data.numpy() #(3000x5)--> 2nd layer node pre_activation\n",
        "  z=(data_curr@effective_weights[2].data.numpy().T)+effective_biases[2].data.numpy() #(3000x5)--> 3rd layer node pre_activation\n",
        "\n",
        "  x1=torch.sigmoid(beta[0]*torch.Tensor(x)) #(3000x5)--> 1st layer node gates\n",
        "  y1=torch.sigmoid(beta[0]*torch.Tensor(y)) #(3000x5)--> 2nd layer node gates\n",
        "  z1=torch.sigmoid(beta[0]*torch.Tensor(z)) #(3000x5)--> 3rd layer node gates\n",
        "\n",
        "  xy=np.zeros((x1.shape[0],x1.shape[1]*y1.shape[1]))\n",
        "  for i in range(x1.shape[0]):\n",
        "    xy[i]=np.multiply.outer(x1[i], y1[i]).ravel() #(3000x25)\n",
        "\n",
        "  xyz=np.zeros((x1.shape[0],xy.shape[1]*z1.shape[1]))\n",
        "  for i in range(x1.shape[0]):\n",
        "    xyz[i]=np.multiply.outer(xy[i], z1[i]).ravel() #(3000x125)\n",
        "\n",
        "  # x=(data_curr@effective_weights[0].T)+effective_biases[0] #(3000x5)--> 1st layer node pre_activation\n",
        "  # y=(data_curr@effective_weights[1].T)+effective_biases[1] #(3000x5)--> 2nd layer node pre_activation\n",
        "  # z=(data_curr@effective_weights[2].T)+effective_biases[2] #(3000x5)--> 3rd layer node pre_activation\n",
        "\n",
        "  # values = []\n",
        "  # values.append(torch.sigmoid(beta[0]*x)) #(3000x5)--> 1st layer node gates\n",
        "  # values.append(torch.sigmoid(beta[0]*y)) #(3000x5)--> 2nd layer node gates\n",
        "  # values.append(torch.sigmoid(beta[0]*z)) #(3000x5)--> 3rd layer node gates\n",
        "\n",
        "  # values = torch.stack(values,dim=1) #(3000x3x5)\n",
        "  # values1 = (values[:,0,:].unsqueeze(dim=2)*values[:,1,:].unsqueeze(dim=1)).view(values.shape[0],-1) #(3000,25) \n",
        "  # values2 = (values1.unsqueeze(dim=2)*values[:,2,:].unsqueeze(dim=1)).view(values.shape[0],-1) #(3000,125)\n",
        "\n",
        "  return xyz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "45c1ennqpCW-"
      },
      "outputs": [],
      "source": [
        "#@title **Path_LR data preparation hyper plane method**\n",
        "if Path_LR_hypPlane:\n",
        "  num_path_features =np.prod(num_hidden_nodes)\n",
        "  path_fea_train = np.zeros(shape=(len(train_data_curr),1)) #3000x1\n",
        "  path_fea_vali = np.zeros(shape=(len(vali_data_curr),1))\n",
        "  path_fea_test = np.zeros(shape=(len(test_data_curr),1))\n",
        "  DLGN_obj = DLGN_obj_store[-1]\n",
        "  effective_weights, effective_biases = DLGN_obj.return_gating_functions()\n",
        "\n",
        "  for i in range(max_no_of_nodes):\n",
        "    for p in range(max_no_of_nodes):\n",
        "      for k in range(max_no_of_nodes):\n",
        "        enter_path=(i,p,k)\n",
        "        Z_path_train = []\n",
        "        Z_path_vali = []\n",
        "        Z_path_test = []\n",
        "        for j in range(len(enter_path)):\n",
        "          weight = effective_weights[j][enter_path[j]].data.numpy()\n",
        "          bias = effective_biases[j][enter_path[j]].data.numpy()\n",
        "          Z_train = ((train_data_curr@weight.T)+bias)>0\n",
        "          Z_vali = ((vali_data_curr@weight.T)+bias)>0\n",
        "          Z_test = ((test_data_curr@weight.T)+bias)>0\n",
        "\n",
        "          Z_path_train.append(Z_train)\n",
        "          Z_path_vali.append(Z_vali)\n",
        "          Z_path_test.append(Z_test)\n",
        "\n",
        "        Z_result_train = Z_path_train[0]\n",
        "        for index in range(1,len(Z_path_train)):\n",
        "          Z_result_train = Z_result_train & Z_path_train[index]\n",
        "        Z_result_train = Z_result_train.reshape([len(Z_result_train),1])\n",
        "        path_fea_train=np.append(path_fea_train,Z_result_train,axis=1)\n",
        "\n",
        "        Z_result_vali = Z_path_vali[0]\n",
        "        for index in range(1,len(Z_path_vali)):\n",
        "          Z_result_vali = Z_result_vali & Z_path_vali[index]\n",
        "        Z_result_vali = Z_result_vali.reshape([len(Z_result_vali),1])\n",
        "        path_fea_vali=np.append(path_fea_vali,Z_result_vali,axis=1)\n",
        "\n",
        "        Z_result_test = Z_path_test[0]\n",
        "        for index in range(1,len(Z_path_test)):\n",
        "          Z_result_test = Z_result_test & Z_path_test[index]\n",
        "        Z_result_test = Z_result_test.reshape([len(Z_result_test),1])\n",
        "        path_fea_test=np.append(path_fea_test,Z_result_test,axis=1)\n",
        "\n",
        "  path_fea_train=path_fea_train[:,1:]\n",
        "  path_fea_vali=path_fea_vali[:,1:]\n",
        "  path_fea_test=path_fea_test[:,1:]\n",
        "\n",
        "else:\n",
        "  num_path_features =np.prod(num_hidden_nodes)\n",
        "  path_fea_train=Path_LR_dataPrep_sigmoid(train_data_curr) #(3000x125)\n",
        "  path_fea_vali=Path_LR_dataPrep_sigmoid(vali_data_curr) #(3000x125)\n",
        "  path_fea_test=Path_LR_dataPrep_sigmoid(test_data_curr) #(3000x125)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YD7efbGpTszg"
      },
      "outputs": [],
      "source": [
        "#@title **Train Path_LR model**\n",
        "#Training the Path_LR model\n",
        "if train_lr:  \n",
        "  def train_path_lr(seed,num_path_features,output_dim,no_of_batches):\n",
        "    set_torchseed(seed)\n",
        "    Path_LR_obj = None\n",
        "    Path_LR_return = None\n",
        "    Path_LR_obj = Path_LR(num_path_features=num_path_features, output_dim=output_dim)\n",
        "    \n",
        "    Path_LR_return = Path_LR(to_copy=Path_LR_obj)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      criterion = nn.CrossEntropyLoss()\n",
        "    if model_type == \"Regression\":\n",
        "      criterion = nn.MSELoss()\n",
        "    \n",
        "    \n",
        "\n",
        "    Path_LR_params = []\n",
        "    Path_LR_params += [item.weight for item in Path_LR_obj.path_layer]\n",
        "\n",
        "    print(Path_LR_params)\n",
        "\n",
        "    optimizer = optim.Adam(Path_LR_params, lr=lr_path, weight_decay=weight_decay)\n",
        "    train_data_torch = torch.Tensor(path_fea_train)\n",
        "    vali_data_torch = torch.Tensor(path_fea_vali)\n",
        "    test_data_torch = torch.Tensor(path_fea_test)\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr, dtype=torch.int64)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr, dtype=torch.int64)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr, dtype=torch.int64)\n",
        "    if model_type == \"Regression\":\n",
        "      train_labels_torch = torch.tensor(train_labels_curr)\n",
        "      test_labels_torch = torch.tensor(test_labels_curr)\n",
        "      vali_labels_torch = torch.tensor(vali_labels_curr)\n",
        "\n",
        "    num_batches = no_of_batches\n",
        "    batch_size = len(train_data_curr)//num_batches\n",
        "    losses=[]\n",
        "    Path_LR_obj_store = []\n",
        "    best_vali_error = len(vali_labels_curr)\n",
        "\n",
        "\n",
        "    for epoch in tqdm(range(num_epoch)):  # loop over the dataset multiple times\n",
        "      running_loss = 0.0\n",
        "      if perfectSq(epoch):\n",
        "        Path_LR_obj_store.append(Path_LR(to_copy=Path_LR_obj))\n",
        "      \n",
        "      for batch_start in range(0,len(train_data_curr),batch_size):\n",
        "        optimizer.zero_grad()\n",
        "        path_values = Path_LR_obj(train_data_torch[batch_start:batch_start+batch_size])\n",
        "        \n",
        "        if model_type == \"Classification\":\n",
        "          outputs = torch.cat((-1*path_values, path_values), dim=1)\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size])\n",
        "        if model_type == \"Regression\":\n",
        "          outputs = path_values\n",
        "          loss = criterion(outputs, train_labels_torch[batch_start:batch_start+batch_size].reshape(outputs.shape))\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()    \n",
        "      losses.append(running_loss/num_batches)\n",
        "\n",
        "      path_values =Path_LR_obj(vali_data_torch)\n",
        "      if model_type == \"Classification\":\n",
        "        vali_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "        vali_preds = torch.argmax(vali_preds, dim=1).numpy()\n",
        "        vali_error= np.sum(vali_labels_torch.numpy()!=vali_preds)\n",
        "      if model_type == \"Regression\":\n",
        "        vali_preds = path_values\n",
        "        vali_error = criterion(vali_preds, vali_labels_torch.reshape(vali_preds.shape)).item()\n",
        "        vali_preds = vali_preds.detach().numpy()\n",
        "      if vali_error < best_vali_error:\n",
        "        Path_LR_obj_return = Path_LR(to_copy=Path_LR_obj)\n",
        "        best_vali_error = vali_error\n",
        "      \n",
        "    path_values =Path_LR_obj_return(test_data_torch)\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "      test_preds = torch.argmax(test_preds, dim=1).numpy()\n",
        "      test_error= np.sum(test_labels_torch.numpy()!=test_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds = path_values\n",
        "      test_error = criterion(test_preds, test_labels_torch.reshape(test_preds.shape)).item()\n",
        "      test_preds = test_preds.detach().numpy()\n",
        "\n",
        "    path_values=Path_LR_obj_return(train_data_torch)\n",
        "    if model_type == \"Classification\":\n",
        "      train_preds = torch.cat((-1*path_values, path_values), dim=1)\n",
        "      train_preds = torch.argmax(train_preds, dim=1).numpy()\n",
        "      train_error= np.sum(train_labels_torch.numpy()!=train_preds)\n",
        "    if model_type == \"Regression\":\n",
        "      train_preds = path_values\n",
        "      train_error = criterion(train_preds, train_labels_torch.reshape(train_preds.shape)).item()\n",
        "      train_preds = train_preds.detach().numpy()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.title(\"Path_LR loss vs epoch\")\n",
        "    plt.plot(losses)\n",
        "    return losses,test_error, train_error, test_preds, Path_LR_obj_return, Path_LR_obj_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrY-2uOT2cvG"
      },
      "outputs": [],
      "source": [
        "#@title **Save Path_LR model**\n",
        "if train_lr:\n",
        "  losses_lr,test_error_lr, train_error_lr, predictions_lr, Path_LR_obj_final, Path_LR_obj_store = train_path_lr(seed,num_path_features,output_dim,no_of_batches)\n",
        "  # save numpy array as npz file\n",
        " \n",
        "  # save to npy file\n",
        "  file_name_lr=file_name_load_lr\n",
        "  savez_compressed(file_name_lr, losses_lr,test_error_lr, train_error_lr, predictions_lr, Path_LR_obj_final, Path_LR_obj_store)\n",
        "  file_name_load_lr = file_name_lr"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5seNSwW24CTS"
      },
      "outputs": [],
      "source": [
        "#@title **Load model Path_LR**\n",
        "if infer_model:\n",
        "  if infer_path_lr:\n",
        "    # load dict of arrays\n",
        "    dict_data = load(file_name_load_lr,allow_pickle=True)\n",
        "    losses_lr=dict_data['arr_0']\n",
        "    test_error_lr=dict_data['arr_1'].item()\n",
        "    train_error_lr=dict_data['arr_2'].item()\n",
        "    predictions_lr=dict_data['arr_3']\n",
        "    Path_LR_obj_final=dict_data['arr_4'].item()\n",
        "    Path_LR_obj_store=dict_data['arr_5']\n",
        "    plt.figure()\n",
        "    plt.title(\"Path_LR loss vs epoch\")\n",
        "    fig=plt.plot(losses_lr[:])\n",
        "    if model_type == \"Classification\":\n",
        "      print(\"test_error\",test_error_lr/len(test_data_curr))\n",
        "      print(\"train_error\",train_error_lr/len(train_data_curr))\n",
        "      print('DLGN acc=',np.sum(predictions_lr==test_labels_curr)/len(test_data_curr))\n",
        "    if model_type == \"Regression\":\n",
        "      print(\"test_error\",test_error_lr)\n",
        "      print(\"train_error\",train_error_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WXLAKYGC5cLO"
      },
      "outputs": [],
      "source": [
        "#@title **Data vs prediction scatter plot**\n",
        "def data_pred_scatter_plot(predictions):\n",
        "  plt.figure()\n",
        "  plt.subplot(1, 2, 1) # row 1, col 2 index 1\n",
        "  plt.scatter(test_data_curr[:,0], test_data_curr[:,1], c=test_labels_curr,vmin=-1,vmax=1)\n",
        "  plt.colorbar()\n",
        "  plt.title(\"Real labelled test data\")\n",
        "  plt.figure()\n",
        "\n",
        "  plt.subplot(1, 2, 2) # index 2\n",
        "  plt.scatter(test_data_curr[:,0], test_data_curr[:,1], c=predictions,vmin=-1,vmax=1)\n",
        "  plt.title(\"Predicted Labelled test data\")\n",
        "  plt.colorbar()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LOt7NkqfN0Zm"
      },
      "outputs": [],
      "source": [
        "#@title **Calculate Path values**\n",
        "if infer_model or infer_dlgn_lr:\n",
        "  def path_value_cal(Model_obj,weight_model_type):\n",
        "    complete_path_vals = np.zeros(tuple(num_hidden_nodes)) #(5,5,5)\n",
        "    complete_paths = list(cartesian_prod(*[range(x) for x in num_hidden_nodes])) #(0,0,0)...(5,5,5)\n",
        "    \n",
        "    for path in complete_paths: #eg: path: (1,2,3)\n",
        "      if weight_model_type == \"DLGN\":\n",
        "        temp = np.dot(Model_obj.value_layers[0].weight.data.numpy()[path[0],:], np.ones(input_dim)) #weights of 1st layer 1 node (2x1) dot input(3000x2) = (3000x1) \n",
        "        for k in range(1,num_hidden_layers):\n",
        "            temp *= Model_obj.value_layers[k].weight.data.numpy()[path[k], path[k-1]] #all intermediate node to node weights eg: layer 1 node 1 to layer 2 node 2 for above eg\n",
        "        temp *= Model_obj.value_layers[num_hidden_layers].weight.data.numpy()[0, path[-1]] #last layer to output \n",
        "      if weight_model_type==\"Path_LR\" or weight_model_type ==\"DLGN_LR\":\n",
        "        temp = Model_obj.path_layer[0].weight.data.numpy()[0,path[0]*max_no_of_nodes**2+path[1]*max_no_of_nodes**1+path[2]*max_no_of_nodes**0] #for LR weight is 125 dimension so for path (1,2,3) corresponding entry is 1x5^2+2x5^1+3x5^0\n",
        "      complete_path_vals[path]=temp\n",
        "    return complete_paths,complete_path_vals #stores all path values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BkmhLUXA5Z3s"
      },
      "outputs": [],
      "source": [
        "#@title **Path value of a particular path of a particular epoch**\n",
        "def print_path_value(enter_path,enter_epoch,weight_model_type): #print value of a particular path for a particular epoch\n",
        "  if infer_model or infer_dlgn_lr:\n",
        "    if weight_model_type==\"DLGN\":\n",
        "      Model_obj = DLGN_obj_store[int(np.sqrt(enter_epoch))]\n",
        "    if weight_model_type == \"Path_LR\":\n",
        "      Model_obj = Path_LR_obj_store[int(np.sqrt(enter_epoch))]\n",
        "    if weight_model_type == \"DLGN_LR\":\n",
        "      Model_obj = DLGN_LR_obj_store[int(np.sqrt(enter_epoch))]\n",
        "    complete_paths,complete_path_vals = path_value_cal(Model_obj,weight_model_type)\n",
        "    # print(f'Value of path {enter_path} of epoch {enter_epoch} is : {complete_path_vals[enter_path]}')\n",
        "    return complete_path_vals[enter_path]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QcuRagVm-eSY"
      },
      "outputs": [],
      "source": [
        "#@title **Visualization of node hyperplane for a particular epoch**\n",
        "def show_node_hyp(enter_epoch,weight_model_type):\n",
        "  if infer_model or infer_dlgn_lr:\n",
        "    if infer_model:\n",
        "      DLGN_obj = DLGN_obj_store[int(np.sqrt(enter_epoch))]\n",
        "      effective_weights, effective_biases = DLGN_obj.return_gating_functions()\n",
        "    X=test_data_curr[:,0]\n",
        "    Y=test_data_curr[:,1]\n",
        "    if weight_model_type == \"DLGN\":\n",
        "      test_data_torch = torch.Tensor(test_data_curr) \n",
        "      values,gate_scores =DLGN_obj(test_data_torch)\n",
        "      path_values = values[-1]\n",
        "    if weight_model_type == \"Path_LR\":\n",
        "      test_data_torch = torch.Tensor(path_fea_test) \n",
        "      Path_LR_obj = Path_LR_obj_store[int(np.sqrt(enter_epoch))]\n",
        "      path_values = Path_LR_obj(test_data_torch)\n",
        "\n",
        "    if weight_model_type == \"DLGN_LR\":\n",
        "      DLGN_LR_obj = DLGN_LR_obj_store[int(np.sqrt(enter_epoch))]\n",
        "      effective_weights, effective_biases = DLGN_LR_obj.return_gating_functions()\n",
        "      test_data_torch = torch.Tensor(test_data_curr) \n",
        "      path_values,_ = DLGN_LR_obj(test_data_torch)\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds = torch.cat((-1*path_values,path_values),dim=1)\n",
        "      predictions = torch.argmax(test_preds,dim=1).numpy()\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds = path_values\n",
        "      predictions=test_preds.detach().numpy()\n",
        "\n",
        "    no_rows=num_hidden_layers+1\n",
        "    no_colns = max_no_of_nodes\n",
        "    fig,ax=plt.subplots(no_rows,no_colns,figsize=(5*no_colns,4*no_rows)) \n",
        "    fig.tight_layout(pad=2)\n",
        "    fig.text(0,0,s=\"Epoch \"+str(enter_epoch))\n",
        "    if no_colns>2:\n",
        "      for k in range(no_colns//2):\n",
        "        im_hide = ax[0,k].axis('off')\n",
        "      for k in range(no_colns//2+2,no_colns):\n",
        "        im_hide = ax[0,k].axis('off')\n",
        "      im0=ax[0,no_colns//2].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "      ax[0,no_colns//2].title.set_text(\"Real\")\n",
        "      ax[0,no_colns//2].set_xlim(-2,2)\n",
        "      ax[0,no_colns//2].set_ylim(-4,4)\n",
        "      divider = make_axes_locatable(ax[0,no_colns//2])\n",
        "      cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "      fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "      im1=ax[0,no_colns//2+1].scatter(X, Y, c=predictions,vmin=-1,vmax=1)\n",
        "      ax[0,no_colns//2+1].title.set_text(\"Predicted\")\n",
        "      ax[0,no_colns//2+1].set_xlim(-2,2)\n",
        "      ax[0,no_colns//2+1].set_ylim(-4,4)\n",
        "      divider = make_axes_locatable(ax[0,no_colns//2+1])\n",
        "      cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "      fig.colorbar(im1, cax=cax, orientation='vertical')\n",
        "    else:\n",
        "        im0=ax[0,0].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "        ax[0,0].title.set_text(\"Real\")\n",
        "        ax[0,0].set_xlim(-2,2)\n",
        "        ax[0,0].set_ylim(-4,4)\n",
        "        divider = make_axes_locatable(ax[0,0])\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "        fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "        im1=ax[0,1].scatter(X, Y, c=predictions,vmin=-1,vmax=1)\n",
        "        ax[0,1].title.set_text(\"Predicted\")\n",
        "        ax[0,1].set_xlim(-2,2)\n",
        "        ax[0,1].set_ylim(-4,4)\n",
        "        divider = make_axes_locatable(ax[0,1])\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "        fig.colorbar(im1, cax=cax, orientation='vertical')\n",
        "    for j in range(num_hidden_layers):\n",
        "      for i in range( num_hidden_nodes[j]):\n",
        "        weight = effective_weights[j][i].data.numpy()\n",
        "        bias = effective_biases[j][i].data.numpy()\n",
        "        Z = ((test_data_curr@weight.T)+bias)>0\n",
        "        line_eq = (-bias - X*weight[0])/weight[1]\n",
        "        im=ax[j+1,i].scatter(X, Y, c=Z,vmin=-1,vmax=1)\n",
        "        im1=ax[j+1,i].plot(X,line_eq,label=\"(\"+str(round(weight.data[0],4))+\")x+\"+\"(\"+str(round(weight.data[1],4))+\")y+\"+\"(\"+str(round(bias.item(),2))+\")=0\")\n",
        "        ax[j+1,i].set_xlim(-2,2)\n",
        "        ax[j+1,i].set_ylim(-4,4)\n",
        "        ax[j+1,i].title.set_text(\"Layer: \"+str(j+1)+\" Node: \"+str(i+1))\n",
        "        ax[j+1,i].legend()\n",
        "        divider = make_axes_locatable(ax[j+1,i])\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "    path0=\"/content/drive/MyDrive/Research/DLGN_fixed_node/\"\n",
        "    if save_img:\n",
        "      fig.savefig(str(path0)+\"Epoch_\"+str(enter_epoch)+\".pdf\", dpi=300,format=\"pdf\")\t\t\t\t\t"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7OexQWHoBxSo"
      },
      "outputs": [],
      "source": [
        "#@title **Node hyperplane for best model**\n",
        "if infer_model:\n",
        "  if best_epoch_show:\n",
        "    effective_weights, effective_biases = DLGN_obj_final.return_gating_functions()\n",
        "    X=test_data_curr[:,0]\n",
        "    Y=test_data_curr[:,1]\n",
        "    no_rows=num_hidden_layers+1\n",
        "    no_colns = max_no_of_nodes\n",
        "    fig,ax=plt.subplots(no_rows,no_colns,figsize=(5*no_colns,4*no_rows)) \n",
        "    fig.tight_layout(pad=2)\n",
        "    fig.text(0,0,s=\"Best Epoch\")\n",
        "    for k in range(no_colns//2):\n",
        "      im_hide = ax[0,k].axis('off')\n",
        "    for k in range(no_colns//2+2,no_colns):\n",
        "      im_hide = ax[0,k].axis('off')\n",
        "    im0=ax[0,no_colns//2].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "    ax[0,no_colns//2].title.set_text(\"Real\")\n",
        "    ax[0,no_colns//2].set_xlim(-2,2)\n",
        "    ax[0,no_colns//2].set_ylim(-4,4)\n",
        "    divider = make_axes_locatable(ax[0,no_colns//2])\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "    im1=ax[0,no_colns//2+1].scatter(X, Y, c=predictions_dlgn,vmin=-1,vmax=1)\n",
        "    ax[0,no_colns//2+1].title.set_text(\"Predicted\")\n",
        "    ax[0,no_colns//2+1].set_xlim(-2,2)\n",
        "    ax[0,no_colns//2+1].set_ylim(-4,4)\n",
        "    divider = make_axes_locatable(ax[0,no_colns//2+1])\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    fig.colorbar(im1, cax=cax, orientation='vertical')\n",
        "    for j in range(num_hidden_layers):\n",
        "      for i in range( num_hidden_nodes[j]):\n",
        "        weight = effective_weights[j][i].data.numpy()\n",
        "        bias = effective_biases[j][i].data.numpy()\n",
        "        Z = ((test_data_curr@weight.T)+bias)>0\n",
        "        line_eq = (-bias - X*weight[0])/weight[1]\n",
        "        im=ax[j+1,i].scatter(X, Y, c=Z,vmin=-1,vmax=1)\n",
        "        im1=ax[j+1,i].plot(X,line_eq,label=\"(\"+str(round(weight.data[0],4))+\")x+\"+\"(\"+str(round(weight.data[1],4))+\")y+\"+\"(\"+str(round(bias.item(),2))+\")=0\")\n",
        "        ax[j+1,i].set_xlim(-2,2)\n",
        "        ax[j+1,i].set_ylim(-4,4)\n",
        "        ax[j+1,i].title.set_text(\"Layer: \"+str(j+1)+\" Node: \"+str(i+1))\n",
        "        ax[j+1,i].legend()\n",
        "        divider = make_axes_locatable(ax[j+1,i])\n",
        "        cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "        fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "    # fig.subplots_adjust(right=0.8)\n",
        "    # cbar_ax = fig.add_axes([0.85, 0.12, 0.01, 0.82])\n",
        "    # fig.colorbar(im0, cax=cbar_ax)\n",
        "    path0=\"/content/drive/MyDrive/Research/DLGN_fixed_node/\"\n",
        "    if save_img:\n",
        "      fig.savefig(str(path0)+\"Best_epoch.pdf\", dpi=300,format=\"pdf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONwdZkXSaKaL"
      },
      "outputs": [],
      "source": [
        "#@title **Change of node hyperplane for each layer**\n",
        "def perfectCube(N) :\n",
        "\tcube_root = round(N**(1/3));\n",
        "\tif cube_root * cube_root * cube_root == N :\n",
        "\t\treturn True;\n",
        "\telse :\n",
        "\t\treturn False;\n",
        "\n",
        "if infer_model or infer_dlgn_lr:\n",
        "\tdef show_node_hyp_eachepoch(weight_model_type):\n",
        "\t\tif test_on_train_data:\n",
        "\t\t\ttest_data_curr = train_data_curr\n",
        "\t\t\ttest_labels_curr = train_labels_curr\n",
        "\t\tif (weight_model_type == \"DLGN_LR\"):\n",
        "\t\t\tDLGN_obj_store_model = DLGN_LR_obj_store\n",
        "\t\telse:\n",
        "\t\t\tDLGN_obj_store_model = DLGN_obj_store\n",
        "\t\tfor epoch_no in range(len(DLGN_obj_store_model)):\n",
        "\t\t\tif ((epoch_no%10)==0):\n",
        "\t\t\t\tprint(\"________________________________\")\n",
        "\t\t\t\tprint(\"=======Epoch no: \", epoch_no**2)\n",
        "\t\t\t\tprint(\"________________________________\")\n",
        "\n",
        "\t\t\t\teffective_weights, effective_biases = DLGN_obj_store_model[epoch_no].return_gating_functions()\n",
        "\t\t\t\tX=test_data_curr[:,0]\n",
        "\t\t\t\tY=test_data_curr[:,1]\n",
        "\t\t\t\ttest_data_torch = torch.Tensor(test_data_curr)\n",
        "\t\t\t\tvalues,gate_scores =DLGN_obj_store_model[epoch_no](test_data_torch)\n",
        "\t\t\t\tif weight_model_type != \"DLGN_LR\":\n",
        "\t\t\t\t\tvalues = values[-1]\n",
        "\t\t\t\tif model_type == \"Classification\":\n",
        "\t\t\t\t\ttest_preds = torch.cat((-1*values,values),dim=1)\n",
        "\t\t\t\t\tpredictions = torch.argmax(test_preds,dim=1).numpy()\n",
        "\t\t\t\tif model_type == \"Regression\":\n",
        "\t\t\t\t\ttest_preds = values\n",
        "\t\t\t\t\tpredictions=test_preds.detach().numpy()\n",
        "\n",
        "\t\t\t\tno_rows=num_hidden_layers+1\n",
        "\t\t\t\tno_colns = max_no_of_nodes\n",
        "\t\t\t\tfig,ax=plt.subplots(no_rows,no_colns,figsize=(5*no_colns,4*no_rows)) \n",
        "\t\t\t\tfig.tight_layout(pad=2)\n",
        "\t\t\t\tfig.text(0,0,s=\"Epoch no: \"+ str(epoch_no**2))\n",
        "\t\t\t\tfor k in range(no_colns//2):\n",
        "\t\t\t\t\tim_hide = ax[0,k].axis('off')\n",
        "\t\t\t\tfor k in range(no_colns//2+2,no_colns):\n",
        "\t\t\t\t\tim_hide = ax[0,k].axis('off')\n",
        "\t\t\t\tim0=ax[0,no_colns//2].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "\t\t\t\tax[0,no_colns//2].title.set_text(\"Real\")\n",
        "\t\t\t\tax[0,no_colns//2].set_xlim(-2,2)\n",
        "\t\t\t\tax[0,no_colns//2].set_ylim(-4,4)\n",
        "\t\t\t\tdivider = make_axes_locatable(ax[0,no_colns//2])\n",
        "\t\t\t\tcax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "\t\t\t\tfig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "\n",
        "\t\t\t\tim1=ax[0,no_colns//2+1].scatter(X, Y, c=predictions,vmin=-1,vmax=1)\n",
        "\t\t\t\tax[0,no_colns//2+1].title.set_text(\"Predicted\")\n",
        "\t\t\t\tax[0,no_colns//2+1].set_xlim(-2,2)\n",
        "\t\t\t\tax[0,no_colns//2+1].set_ylim(-4,4)\n",
        "\t\t\t\tdivider = make_axes_locatable(ax[0,no_colns//2+1])\n",
        "\t\t\t\tcax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "\t\t\t\tfig.colorbar(im1, cax=cax, orientation='vertical')\n",
        "\n",
        "\t\t\t\tfor j in range(num_hidden_layers):\n",
        "\t\t\t\t\tfor i in range( num_hidden_nodes[j]):\n",
        "\t\t\t\t\t\tweight = effective_weights[j][i].data.numpy()\n",
        "\t\t\t\t\t\tbias = effective_biases[j][i].data.numpy()\n",
        "\t\t\t\t\t\tZ = ((test_data_curr@weight.T)+bias)>0\n",
        "\t\t\t\t\t\tline_eq = (-bias - X*weight[0])/weight[1]\n",
        "\n",
        "\t\t\t\t\t\tim=ax[j+1,i].scatter(X, Y, c=Z,vmin=-1,vmax=1)\n",
        "\t\t\t\t\t\tim1=ax[j+1,i].plot(X,line_eq,label=\"(\"+str(round(weight.data[0],4))+\")x+\"+\"(\"+str(round(weight.data[1],4))+\")y+\"+\"(\"+str(round(bias.item(),4))+\")=0\")\n",
        "\t\t\t\t\t\tax[j+1,i].set_xlim(-2,2)\n",
        "\t\t\t\t\t\tax[j+1,i].set_ylim(-4,4)\n",
        "\t\t\t\t\t\tax[j+1,i].title.set_text(\"Layer: \"+str(j+1)+\" Node: \"+str(i+1))\n",
        "\t\t\t\t\t\tax[j+1,i].legend()\n",
        "\t\t\t\t\t\tdivider = make_axes_locatable(ax[j+1,i])\n",
        "\t\t\t\t\t\tcax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "\t\t\t\t\t\tfig.colorbar(im, cax=cax, orientation='vertical')\n",
        "\t\t\t\t# fig.subplots_adjust(right=0.8)\n",
        "\t\t\t\t# cbar_ax = fig.add_axes([0.85, 0.12, 0.01, 0.82])\n",
        "\t\t\t\t# fig.colorbar(im0, cax=cbar_ax)\n",
        "\t\t\t\tpath=\"/content/drive/MyDrive/Research/DLGN_fixed_node/Layer_3_Node_2/\"\n",
        "\t\t\t\tif save_img:\n",
        "\t\t\t\t\tfig.savefig(str(path)+\"Epoch_\"+str(epoch_no**2)+\"_.png\", dpi=300)\n",
        "\t\t\t\tplt.show\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4jwe_jHVxdB"
      },
      "outputs": [],
      "source": [
        "#@title **Visualization of a particular path hyp for a particular epoch**\n",
        "def show_path_hyp(enter_path,enter_epoch,weight_model_type):\n",
        "  if (weight_model_type == \"DLGN_LR\"):\n",
        "    DLGN_obj_store_model = DLGN_LR_obj_store\n",
        "  else:\n",
        "    DLGN_obj_store_model = DLGN_obj_store\n",
        "\n",
        "\n",
        "  DLGN_obj = DLGN_obj_store_model[int(np.sqrt(enter_epoch))]\n",
        "  effective_weights, effective_biases = DLGN_obj.return_gating_functions()\n",
        "  X=test_data_curr[:,0]\n",
        "  Y=test_data_curr[:,1]\n",
        "  fig,ax=plt.subplots(1,2,figsize=(5*2,4)) \n",
        "  fig.tight_layout(pad=2)\n",
        "  fig.text(0,0,s=\"Epoch \"+str(enter_epoch))\n",
        "  im0=ax[0].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "  ax[0].title.set_text(\"Real Data\")\n",
        "  ax[0].set_xlim(-2,2)\n",
        "  ax[0].set_ylim(-4,4)\n",
        "  divider = make_axes_locatable(ax[0])\n",
        "  cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "  fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "  Z_path = []\n",
        "  for j in range(len(enter_path)):\n",
        "    weight = effective_weights[j][enter_path[j]].data.numpy()\n",
        "    bias = effective_biases[j][enter_path[j]].data.numpy()\n",
        "    Z = ((test_data_curr@weight.T)+bias)>0\n",
        "    Z_path.append(Z)\n",
        "  Z_result = Z_path[0]\n",
        "  for i in range(1,len(Z_path)):\n",
        "    Z_result = Z_result & Z_path[i]\n",
        "\n",
        "  data_frac_by_path = np.count_nonzero(Z_result)/len(test_labels_curr)*100 #Stregth of the path defined as fraction of data points active by that path\n",
        "  Z_res_path=Z_result*test_labels_curr\n",
        "  Z_res_non_zero = Z_res_path[np.nonzero(Z_res_path)]\n",
        "  path_std=np.std(Z_res_non_zero)##Path purity defined as the path variance i.e., variance of the data points active for that path\n",
        "  path_mean=np.mean(Z_res_non_zero)\n",
        "\n",
        "  im1=ax[1].scatter(X, Y, c=Z_result,vmin=-1,vmax=1)\n",
        "  ax[1].text(0.5,.9,\"data_frac_by_path: \"+str(round(data_frac_by_path,2)),horizontalalignment='center', verticalalignment='center', transform=ax[1].transAxes)\n",
        "  ax[1].text(0.5,0.1,\"Path_mean: \"+str(round(path_mean,2))+\" ,std: \"+str(round(path_std,2)),horizontalalignment='center', verticalalignment='center', transform=ax[1].transAxes)\n",
        "\n",
        "  ax[1].title.set_text(str(enter_path)+\" Path visualization\")\n",
        "  ax[1].set_xlim(-2,2)\n",
        "  ax[1].set_ylim(-4,4)\n",
        "  divider = make_axes_locatable(ax[1])\n",
        "  cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "  fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "  path0=\"/content/drive/MyDrive/Research/DLGN_fixed_node/\"\n",
        "  if save_img:\n",
        "    fig.savefig(str(path0)+\"Path_viz_Epoch_\"+str(enter_epoch)+\".pdf\", dpi=300,format=\"pdf\")\n",
        "\n",
        "\n",
        "def return_path_hyp(enter_path,enter_epoch,weight_model_type):\n",
        "  if (weight_model_type == \"DLGN_LR\"):\n",
        "    DLGN_obj_store_model = DLGN_LR_obj_store\n",
        "  else:\n",
        "    DLGN_obj_store_model = DLGN_obj_store\n",
        "\n",
        "  DLGN_obj = DLGN_obj_store_model[int(np.sqrt(enter_epoch))]\n",
        "  effective_weights, effective_biases = DLGN_obj.return_gating_functions()\n",
        "  Z_path = []\n",
        "  for j in range(len(enter_path)):\n",
        "    weight = effective_weights[j][enter_path[j]].data.numpy()\n",
        "    bias = effective_biases[j][enter_path[j]].data.numpy()\n",
        "    Z = ((test_data_curr@weight.T)+bias)>0\n",
        "    Z_path.append(Z)\n",
        "  Z_result = Z_path[0]\n",
        "  for i in range(1,len(Z_path)):\n",
        "    Z_result = Z_result & Z_path[i]\n",
        "  data_frac_by_path = np.count_nonzero(Z_result)/len(test_labels_curr)*100 #Stregth of the path defined as fraction of data points active by that path\n",
        "  Z_res_path=Z_result*test_labels_curr\n",
        "  Z_res_non_zero = Z_res_path[np.nonzero(Z_res_path)]\n",
        "  path_std=np.std(Z_res_non_zero)##Path purity defined as the path variance i.e., variance of the data points active for that path\n",
        "  path_mean=np.mean(Z_res_non_zero)\n",
        "  return Z_result,round(data_frac_by_path,2),round(path_mean,2),round(path_std,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HwDDB5fCFXFQ"
      },
      "outputs": [],
      "source": [
        "# DLGN_obj = DLGN_obj_store[0]\n",
        "# DLGN_obj.value_layers[-1].weight\n",
        "# DLGN_obj.value_layers[-2].weight"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPwDvbUYFbfB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "taWFUEd0G9p2"
      },
      "outputs": [],
      "source": [
        "#@title **Change of W_&_B for all node**\n",
        "def change_wb(weight_model_type):\n",
        "  if infer_model or infer_dlgn_lr:\n",
        "    # layer_num = 3\n",
        "    # node_num = 2\n",
        "    if (weight_model_type == \"DLGN_LR\"):\n",
        "      DLGN_obj_store_model = DLGN_LR_obj_store\n",
        "    else:\n",
        "      DLGN_obj_store_model = DLGN_obj_store\n",
        "    no_rows=num_hidden_layers\n",
        "    no_colns = max_no_of_nodes\n",
        "    fig,ax=plt.subplots(no_rows,no_colns,figsize=(5*no_colns,4*no_rows)) \n",
        "    fig.tight_layout(pad=2)\n",
        "    fig.text(0,0,s=\"Change of weights\")\n",
        "    for layer_num in range(num_hidden_layers):\n",
        "      for node_num in range(num_hidden_nodes[layer_num]):\n",
        "        weight_0 = []\n",
        "        weight_1 = []\n",
        "        bias = []\n",
        "        epoch_count=range(len(DLGN_obj_store_model))\n",
        "        for epoch_no in range(len(DLGN_obj_store_model)):\n",
        "          effective_weights, effective_biases = DLGN_obj_store_model[epoch_no].return_gating_functions()\n",
        "          weight_0.append(effective_weights[layer_num][node_num].data.numpy()[0])\n",
        "          weight_1.append(effective_weights[layer_num][node_num].data.numpy()[1])\n",
        "          bias.append(effective_biases[layer_num][node_num].data.numpy())\n",
        "        im1=ax[layer_num,node_num].plot(epoch_count,weight_0,label=\"Weight_0\")\n",
        "        im2=ax[layer_num,node_num].plot(epoch_count,weight_1,label=\"Weight_1\")\n",
        "        im3=ax[layer_num,node_num].plot(epoch_count,bias,label=\"bias\")\n",
        "        ax[layer_num,node_num].title.set_text(\"Layer: \"+str(layer_num+1)+\" Node: \"+str(node_num+1))\n",
        "        ax[layer_num,node_num].legend()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_YkavzBLPWi"
      },
      "outputs": [],
      "source": [
        "#@title **Plot Path Value variation**\n",
        "def plot_path_value_variation(enter_path,plot_epoch,weight_model_type): #plot_epoch is till which epoch want to plot\n",
        "  path_value=[]\n",
        "  path_epoch_count = []\n",
        "  for i in range(plot_epoch):\n",
        "    if(perfectSq(i)):\n",
        "      path_epoch_count.append(i)\n",
        "      path_value.append(print_path_value(enter_path,i,weight_model_type))\n",
        "  plt.figure()\n",
        "  plt.plot(path_epoch_count,path_value)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IifSgQPxvkm4"
      },
      "outputs": [],
      "source": [
        "#@title **Sanity check of path decomposition**\n",
        "def check_sanity(weight_model_type,enter_epoch):\n",
        "  X=test_data_curr[:,0]\n",
        "  Y=test_data_curr[:,1]\n",
        "  Z=np.zeros(X.shape)\n",
        "  for i in range(max_no_of_nodes):\n",
        "    for j in range(max_no_of_nodes):\n",
        "      for k in range(max_no_of_nodes):\n",
        "        enter_path=(i,j,k)\n",
        "        val=print_path_value(enter_path,enter_epoch,weight_model_type)\n",
        "        Z_result,data_frac_by_path,path_mean,path_std=return_path_hyp(enter_path,enter_epoch,weight_model_type)\n",
        "        Z+=val*Z_result\n",
        "  plt.scatter(X,Y,c=Z,vmin=-1,vmax=1)\n",
        "  plt.colorbar()\n",
        "  plt.figure()\n",
        "  plt.scatter(X,Y,c=test_labels_curr,vmin=-1,vmax=1)\n",
        "  plt.colorbar()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uVsQ5BeN5xW"
      },
      "outputs": [],
      "source": [
        "#@title **Array of dict storing path,path_val,data frac,mean,std**\n",
        "def path_array(plot_epoch,enter_epoch,weight_model_type):\n",
        "  path_val_store = []\n",
        "  for i in range(max_no_of_nodes):\n",
        "    for j in range(max_no_of_nodes):\n",
        "      for k in range(max_no_of_nodes):\n",
        "        enter_path=(i,j,k)\n",
        "        path_value=[]\n",
        "        path_epoch_count = []\n",
        "        for pe in range(plot_epoch):\n",
        "          if(perfectSq(pe)):\n",
        "            path_epoch_count.append(pe)\n",
        "            path_value.append(round(print_path_value(enter_path,pe,weight_model_type),3))\n",
        "        Z_result,data_frac_by_path,path_mean,path_std=return_path_hyp(enter_path,enter_epoch,weight_model_type)\n",
        "        Dict = {}\n",
        "        Dict['val']=(path_epoch_count,path_value)\n",
        "        Dict['path']=enter_path\n",
        "        Dict['data_frac_by_path']=data_frac_by_path\n",
        "        Dict['path_mean']=path_mean\n",
        "        Dict['path_std']=path_std\n",
        "        path_val_store.append(Dict)\n",
        "  return path_val_store"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDEvZPUjWmVC"
      },
      "outputs": [],
      "source": [
        "#@title **Printing vals for one path**\n",
        "def print_any_path(plot_epoch,enter_epoch,weight_model_type,find_path=(0,0,0)):\n",
        "  path_val_store = path_array(plot_epoch,enter_epoch,weight_model_type)\n",
        "  newlist = sorted(path_val_store, key=itemgetter('path')) \n",
        "  return newlist,newlist[find_path[0]*max_no_of_nodes**2+find_path[1]*max_no_of_nodes+find_path[2]]  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XVex8vM1YnCX"
      },
      "outputs": [],
      "source": [
        "#@title **5x25 path vals**\n",
        "def plot_all_paths(plot_epoch,enter_epoch,weight_model_type):\n",
        "  newlist,_ = print_any_path(plot_epoch,enter_epoch,weight_model_type)\n",
        "  for index in range(max_no_of_nodes**num_hidden_layers):\n",
        "    if(index%max_no_of_nodes==0):\n",
        "      plt.figure(figsize=(5,5))\n",
        "    plt.plot(newlist[index]['val'][0],newlist[index]['val'][1],label=\"p: \"+str(newlist[index]['path'])+\" f: \"+str(newlist[index]['data_frac_by_path'])+\" m: \"+str(newlist[index]['path_mean'])+\" s: \"+str(newlist[index]['path_std']))\n",
        "    plt.legend(loc=\"upper left\")\n",
        "    plt.ylim(-.3, .3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4THRcB_ibYkJ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hr5OW2CZGcp"
      },
      "outputs": [],
      "source": [
        "#@title **Losswise comparision of DLGN and LR or DLGN_LR models**\n",
        "def find_nearest_idx(array, value):\n",
        "  idx = (np.abs(array - value)).argmin()\n",
        "  return idx\n",
        "def loss_wise_comp(weight_model_type):\n",
        "  X=test_data_curr[:,0]\n",
        "  Y=test_data_curr[:,1]\n",
        "  loss_check =[1,0.7,0.65,0.6,0.55,0.5,0.45,0.4,0.3,0.2,0.1,0.05,0.01,0.001,0]\n",
        "\n",
        "  test_data_torch_dlgn = torch.Tensor(test_data_curr)\n",
        "  if weight_model_type==\"Path_LR\":\n",
        "    test_data_torch_lr = torch.Tensor(path_fea_test)\n",
        "  else:\n",
        "    test_data_torch_lr = torch.Tensor(test_data_curr)\n",
        "\n",
        "  no_rows=len(loss_check)+1\n",
        "  no_colns = 2\n",
        "  fig,ax=plt.subplots(no_rows,no_colns,figsize=(5*no_colns,4*no_rows)) \n",
        "  fig.tight_layout(pad=5)\n",
        "  fig.text(0,0,s=\"DLGN vs LR\")\n",
        "  im_hide = ax[0,0].axis('off')\n",
        "  \n",
        "  im0=ax[0,1].scatter(X, Y, c=test_labels_curr,vmin=-1,vmax=1)\n",
        "  ax[0,1].title.set_text(\"Real\")\n",
        "  ax[0,1].set_xlim(-2,2)\n",
        "  ax[0,1].set_ylim(-2,2)\n",
        "  divider = make_axes_locatable(ax[0,1])\n",
        "  cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "  fig.colorbar(im0, cax=cax, orientation='vertical')\n",
        "  \n",
        "\n",
        "  for index,loss_val in enumerate(loss_check):  \n",
        "    enter_epoch_dlgn = find_nearest_idx(losses,loss_val)\n",
        "    if weight_model_type == \"Path_LR\":\n",
        "      losses_lr_m = losses_lr\n",
        "      enter_epoch_lr = find_nearest_idx(losses_lr_m,loss_val) \n",
        "    else:\n",
        "      losses_lr_m = losses_dlgnlr\n",
        "      enter_epoch_lr = find_nearest_idx(losses_lr_m,loss_val) \n",
        "\n",
        "\n",
        "    dlgn_index=int(np.sqrt(enter_epoch_dlgn))\n",
        "    DLGN_obj = DLGN_obj_store[dlgn_index]\n",
        "    values,gate_scores =DLGN_obj(test_data_torch_dlgn)\n",
        "    path_values_dlgn = values[-1]\n",
        "\n",
        "    if model_type == \"Classification\":\n",
        "      test_preds_dlgn = torch.cat((-1*path_values_dlgn,path_values_dlgn),dim=1)\n",
        "      predictions_dlgn = torch.argmax(test_preds_dlgn,dim=1).numpy()\n",
        "    if model_type == \"Regression\":\n",
        "      test_preds_dlgn = path_values_dlgn\n",
        "      predictions_dlgn=test_preds_dlgn.detach().numpy()\n",
        "\n",
        "    if weight_model_type==\"Path_LR\":\n",
        "      lr_index=int(np.sqrt(enter_epoch_lr))\n",
        "      Path_LR_obj = Path_LR_obj_store[lr_index]\n",
        "      path_values_lr = Path_LR_obj(test_data_torch_lr)\n",
        "\n",
        "      if model_type == \"Classification\":\n",
        "        test_preds_lr = torch.cat((-1*path_values_lr,path_values_lr),dim=1)\n",
        "        predictions_lr = torch.argmax(test_preds_lr,dim=1).numpy()\n",
        "      if model_type == \"Regression\":\n",
        "        test_preds_lr = path_values_lr\n",
        "        predictions_lr=test_preds_lr.detach().numpy()\n",
        "    else:\n",
        "      lr_index=int(np.sqrt(enter_epoch_lr))\n",
        "      DLGN_LR_obj = DLGN_LR_obj_store[lr_index]\n",
        "      path_values_lr,_ = DLGN_LR_obj(test_data_torch_lr)\n",
        "\n",
        "      if model_type == \"Classification\":\n",
        "        test_preds_lr = torch.cat((-1*path_values_lr,path_values_lr),dim=1)\n",
        "        predictions_lr = torch.argmax(test_preds_lr,dim=1).numpy()\n",
        "      if model_type == \"Regression\":\n",
        "        test_preds_lr = path_values_lr\n",
        "        predictions_lr=test_preds_lr.detach().numpy()\n",
        "\n",
        "   \n",
        "    im=ax[index+1,0].scatter(X, Y, c=predictions_dlgn,vmin=-1,vmax=1)\n",
        "    ax[index+1,0].set_xlim(-2,2)\n",
        "    ax[index+1,0].set_ylim(-2,2)\n",
        "    # ax[index+1,0].title.set_text(\"Loss : \"+str(round(losses[enter_epoch_dlgn],4))+\" , \"+str(round(losses[dlgn_index*dlgn_index],4))+\" Epoch : \"+str(enter_epoch_dlgn)+\" , \"+str(dlgn_index*dlgn_index)+\" DLGN\")\n",
        "    ax[index+1,0].title.set_text(\"Loss : \"+str(round(losses[dlgn_index*dlgn_index],4))+\" Epoch : \"+str(dlgn_index*dlgn_index)+\" DLGN\")\n",
        "    # ax[index+1,0].legend()\n",
        "    divider = make_axes_locatable(ax[index+1,0])\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "\n",
        "    im1=ax[index+1,1].scatter(X, Y, c=predictions_lr,vmin=-1,vmax=1)\n",
        "    ax[index+1,1].set_xlim(-2,2)\n",
        "    ax[index+1,1].set_ylim(-2,2)\n",
        "    # ax[index+1,1].title.set_text(\"Loss : \"+str(round(losses_lr[enter_epoch_lr],4))+\" , \"+str(round(losses_lr[lr_index*lr_index],4))+\" Epoch : \"+str(enter_epoch_lr)+\" , \"+str(lr_index*lr_index)+\" LR\")\n",
        "    ax[index+1,1].title.set_text(\"Loss : \"+str(round(losses_lr_m[lr_index*lr_index],4))+\" Epoch : \"+str(lr_index*lr_index)+\" LR\")\n",
        "    # ax[index+1,1].legend()\n",
        "    divider = make_axes_locatable(ax[index+1,1])\n",
        "    cax = divider.append_axes('right', size='5%', pad=0.05)\n",
        "    fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P0aqVxE_L1RC"
      },
      "source": [
        "**Results**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CjF8CRVPBHch"
      },
      "outputs": [],
      "source": [
        "if infer_model: #if we have similar kind of paths only one of them vary much rest don't vary that much (2,1/2,0/3) (1,4,0/1/3/4) seed 365\n",
        "  enter_path = (0,0,0) #(2,2,3)#(2,1,0)#(2,1,3)#(1,4,0)#(1,4,3)#(1,4,1)#(1,4,4) #(2,0,3) #(2,2,0)  \n",
        "  plot_epoch = 5000\n",
        "  enter_epoch = 5000\n",
        "  weight_model_type = \"DLGN\"\n",
        "  data_pred_scatter_plot(predictions_dlgn)\n",
        "  check_sanity(weight_model_type,enter_epoch)\n",
        "  change_wb(weight_model_type)\n",
        "  _,path_val=print_any_path(plot_epoch,enter_epoch,weight_model_type,find_path=enter_path)\n",
        "  plot_all_paths(plot_epoch,enter_epoch,weight_model_type)\n",
        "  #**Path visualization of one path**\n",
        "  plot_path_value_variation(enter_path,plot_epoch,weight_model_type)\n",
        "  show_path_hyp(enter_path,enter_epoch,weight_model_type)\n",
        "  #**All node visualization**\n",
        "  show_node_hyp(enter_epoch,weight_model_type)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# show_node_hyp(12000,\"DLGN\")"
      ],
      "metadata": {
        "id": "3ytC4GBdQ2zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z6RXB3mBHSE"
      },
      "outputs": [],
      "source": [
        "if infer_model: #if we have similar kind of paths only one of them vary much rest don't vary that much (2,1/2,0/3) (1,4,0/1/3/4) #seed 365\n",
        "  if infer_path_lr:\n",
        "    enter_path = (0,0,0) #(2,2,3)#(2,1,0)#(2,1,3)#(1,4,0)#(1,4,3)#(1,4,1)#(1,4,4) #(2,0,3) #(2,2,0)  \n",
        "    plot_epoch = 5000\n",
        "    enter_epoch = 5000\n",
        "    weight_model_type = \"Path_LR\"\n",
        "    data_pred_scatter_plot(predictions_lr)\n",
        "    check_sanity(weight_model_type,enter_epoch)\n",
        "    _,path_val=print_any_path(plot_epoch,enter_epoch,weight_model_type,find_path=enter_path)\n",
        "    plot_all_paths(plot_epoch,enter_epoch,weight_model_type)\n",
        "    #**Path visualization of one path**\n",
        "    plot_path_value_variation(enter_path,plot_epoch,weight_model_type)\n",
        "    show_path_hyp(enter_path,enter_epoch,weight_model_type)\n",
        "    #**All node visualization**\n",
        "    show_node_hyp(enter_epoch,weight_model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ocDASUnYgcif"
      },
      "outputs": [],
      "source": [
        "if infer_dlgn_lr:\n",
        "  enter_path = (0,0,0) #(2,2,3)#(2,1,0)#(2,1,3)#(1,4,0)#(1,4,3)#(1,4,1)#(1,4,4) #(2,0,3) #(2,2,0)  \n",
        "  plot_epoch = 5000\n",
        "  enter_epoch = 5000\n",
        "  weight_model_type = \"DLGN_LR\"\n",
        "  data_pred_scatter_plot(predictions_dlgn_lr)\n",
        "  check_sanity(weight_model_type,enter_epoch)\n",
        "  _,path_val=print_any_path(plot_epoch,enter_epoch,weight_model_type,find_path=enter_path)\n",
        "  plot_all_paths(plot_epoch,enter_epoch,weight_model_type)\n",
        "  #**Path visualization of one path**\n",
        "  plot_path_value_variation(enter_path,plot_epoch,weight_model_type)\n",
        "  show_path_hyp(enter_path,enter_epoch,weight_model_type)\n",
        "  #**All node visualization**\n",
        "  show_node_hyp(enter_epoch,weight_model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nc4SuH0buVvU"
      },
      "outputs": [],
      "source": [
        "if infer_dlgn_lr:\n",
        "  check_sanity(\"DLGN_LR\",5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJh22cxD3agF"
      },
      "outputs": [],
      "source": [
        "if infer_path_lr:\n",
        "  check_sanity(\"Path_LR\",5000)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Loss Comparision**"
      ],
      "metadata": {
        "id": "Rdeh2jrsTCBq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1bQNXN6DhW_I"
      },
      "outputs": [],
      "source": [
        "if infer_model:\n",
        "  weight_model_type = \"DLGN_LR\"  #Path_LR --> to compare loss between DLGN and Path_LR else DLGN_LR to compare loss between DLGN and DLGN_LR\n",
        "  loss_wise_comp(weight_model_type)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhRoEKlcwJlG"
      },
      "outputs": [],
      "source": [
        "#@title **Result Visualization** \n",
        "'''\n",
        "These are the functions to be called for different result visualizations.\n",
        "Uncomment the functions want to run with proper variables.\n",
        "'''\n",
        "\n",
        "#variables used\n",
        "enter_path = (0,0,0)  #enter the path\n",
        "plot_epoch = 5000 #enter the epoch no it which result is needed\n",
        "enter_epoch = 5000 #enter the epoch no whose results are needed\n",
        "weight_model_type = \"DLGN\"  #Path_LR --> for Path_LR, DLGN_LR for DLGN_LR and DLGN for DLGN\n",
        "\n",
        "# Plot the test data actual and predicted\n",
        "# data_pred_scatter_plot(predictions_lr) #predictions_lr --> for Path_LR model,predictions_dlgn_lr --> for DLGN_LR model,predictions_dlgn --> for DLGN model \n",
        "\n",
        "#To check if NPF*NPV giving expected output\n",
        "# check_sanity(weight_model_type,enter_epoch)\n",
        "\n",
        "#To print the path values, data praction covered by the path, mean, s.d of the portion covered by the path\n",
        "# _,path_val=print_any_path(plot_epoch,enter_epoch,weight_model_type,find_path=enter_path)\n",
        "\n",
        "#To plot all the path values with epoch, data praction covered by the path, mean, s.d of the portion covered by the path\n",
        "plot_all_paths(plot_epoch,enter_epoch,weight_model_type)\n",
        "\n",
        "# #Path value visualization of one path\n",
        "# plot_path_value_variation(enter_path,plot_epoch,weight_model_type)\n",
        "\n",
        "# To show the portion of the data covered by the path in the dataset\n",
        "# show_path_hyp(enter_path,enter_epoch,weight_model_type)\n",
        "\n",
        "# All node hyperplane visualization\n",
        "# show_node_hyp(enter_epoch,weight_model_type)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weight_model_type = \"DLGN_LR\"  #Path_LR --> to compare loss between DLGN and Path_LR else DLGN_LR to compare loss between DLGN and DLGN_LR\n",
        "# loss_wise_comp(weight_model_type)"
      ],
      "metadata": {
        "id": "5HfRPLrVO6Xe"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}